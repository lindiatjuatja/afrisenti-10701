{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEkdLm_JF84s"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/afrisenti-logo.png\" width=\"30%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXY0zNpmLM1Z"
   },
   "source": [
    "<center>\n",
    "\n",
    "#SemEval 2023 Shared Task 12: AfriSenti (Task A)\n",
    "\n",
    "###Starter Notebook\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3wrnOfUBE7A"
   },
   "source": [
    "Baseline code based on the starter code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4BB4JDx5W8d"
   },
   "source": [
    "#1) Installations and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DObkW3ulM7yg"
   },
   "source": [
    "##a. Mount drive (if you are running on colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-OZxUWIMqtq"
   },
   "source": [
    "##b. Clone or update competition repository\n",
    "\n",
    "After cloning, under MyDrive, you will see afrisenti-semeval-2023 folder with all the the data for the afrisenti shared task (training and dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17192,
     "status": "ok",
     "timestamp": 1666932930576,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "OsuZvweV6jzo",
    "outputId": "601e0364-95f3-4c0e-d3e9-fa63688e3546"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Language to train sentiment classifier for\n",
    "# am dz ha ig ma pcm pt sw yo\n",
    "\n",
    "LANGUAGE_CODE = 'yo'\n",
    "folder = ''\n",
    "\n",
    "colab = False\n",
    "\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    proj_folder = '/content/drive/MyDrive'\n",
    "else:\n",
    "    proj_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6974,
     "status": "ok",
     "timestamp": 1666932941234,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "2392TKadMvqT",
    "outputId": "a5427066-2d8f-4e1f-ccda-46eddb636031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Downloads\\afrisent\n",
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\n"
     ]
    }
   ],
   "source": [
    "%cd {proj_folder}\n",
    "\n",
    "\n",
    "PROJECT_DIR = f'{proj_folder}/afrisent-semeval-2023'\n",
    "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
    "\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  !git clone {PROJECT_GITHUB_URL}\n",
    "else:\n",
    "  %cd {PROJECT_DIR}\n",
    "  pass\n",
    "#   !git pull {PROJECT_GITHUB_URL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb03Gp9fUN8C"
   },
   "source": [
    "##c. Install required libraries\n",
    "\n",
    "- Set the project dire\n",
    "ctory in the cell below, where the requirements file should also be located, and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Cbmi_mQ4k3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\n",
      "Requirement already satisfied: pandas in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 2)) (1.21.5)\n",
      "Requirement already satisfied: transformers in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 3)) (4.23.1)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 4)) (1.12.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 5)) (1.7.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 6)) (3.19.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 7)) (1.0.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 8)) (0.13.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 9)) (0.1.97)\n",
      "Requirement already satisfied: datasets>=1.8.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 10)) (2.6.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 11)) (0.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from pandas->-r starter_kit/requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from pandas->-r starter_kit/requirements.txt (line 1)) (2021.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (2022.3.15)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from torch>=1.3->-r starter_kit/requirements.txt (line 4)) (4.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from scikit-learn->-r starter_kit/requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from scikit-learn->-r starter_kit/requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from accelerate->-r starter_kit/requirements.txt (line 8)) (5.8.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (2022.2.0)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (0.3.5.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (3.1.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (0.18.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (3.8.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (0.70.13)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (9.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers->-r starter_kit/requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->-r starter_kit/requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->transformers->-r starter_kit/requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->transformers->-r starter_kit/requirements.txt (line 3)) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->transformers->-r starter_kit/requirements.txt (line 3)) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->transformers->-r starter_kit/requirements.txt (line 3)) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers->-r starter_kit/requirements.txt (line 3)) (0.4.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "%cd {PROJECT_DIR}\n",
    "\n",
    "if os.path.isdir(PROJECT_DIR):\n",
    "  #The requirements file should be in PROJECT_DIR\n",
    "  if os.path.isfile(os.path.join(PROJECT_DIR, 'starter_kit/requirements.txt')):\n",
    "    !pip install -r starter_kit/requirements.txt\n",
    "  else:\n",
    "    print('requirements.txt file not found')\n",
    "\n",
    "else:\n",
    "  print(\"Project directory not found, please check again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zszKhh2Ufb3"
   },
   "source": [
    "##d. Import libraries\n",
    "\n",
    "Import libraries below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8QIl420aUM1O"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Please don not edit anything here\n",
    "languages = ['am', 'dz', 'ha', 'ig', 'ma', 'pcm', 'pt', 'sw', 'yo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoRyMJMDJ7lF"
   },
   "source": [
    "#2) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wk5vMnrXMSS"
   },
   "source": [
    "##a. Formatting\n",
    "\n",
    "The training dataset that was provided for the competition is in the following format:\n",
    "\n",
    "| ID | text | label |\n",
    "| --- | --- | --- |\n",
    "| twt001 | example text | negative |\n",
    "| twt002 | example text | positive |\n",
    "| ... | ... | ... |\n",
    "\n",
    "However, the code in the starter kit do not expect the \n",
    "ID and require the training (and evaluation) data to be in the following format\n",
    "\n",
    "|text | label |\n",
    "|--- | --- |\n",
    "|example text | negative |\n",
    "|example text | positive |\n",
    "|... | ... |\n",
    "\n",
    "To reformat the data run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1666884271530,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "QzoSbWC678Zm",
    "outputId": "69eacc7f-cad8-4bad-9f0f-71c4c32758ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory found.\n"
     ]
    }
   ],
   "source": [
    "# Training Data Paths\n",
    "\n",
    "TASK = 'SubtaskA'\n",
    "TRAINING_DATA_DIR = os.path.join(PROJECT_DIR, TASK, 'train')\n",
    "FORMATTED_TRAIN_DATA = os.path.join(TRAINING_DATA_DIR, 'formatted-train-data')\n",
    "\n",
    "if os.path.isdir(TRAINING_DATA_DIR):\n",
    "  print('Data directory found.')\n",
    "  if not os.path.isdir(FORMATTED_TRAIN_DATA):\n",
    "    print('Creating directory to store formatted data.')\n",
    "    os.mkdir(FORMATTED_TRAIN_DATA)\n",
    "else:\n",
    "  print(TRAINING_DATA_DIR + ' is not a valid directory or does not exist!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1666884272475,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "ssyIZOUJMrzM",
    "outputId": "7a883beb-a7cd-44ca-8ab8-84147f25295d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\SubtaskA\\train\n",
      "formatted-train-data skipped!\n",
      "README.txt skipped!\n",
      "splitted-train-dev-test skipped!\n"
     ]
    }
   ],
   "source": [
    "%cd {TRAINING_DATA_DIR}\n",
    "\n",
    "training_files = os.listdir()\n",
    "\n",
    "if len(training_files) > 0:\n",
    "  for training_file in training_files:\n",
    "    if training_file.endswith('.tsv'):\n",
    "\n",
    "      data = training_file.split('_')[0]\n",
    "      if not os.path.isdir(os.path.join(FORMATTED_TRAIN_DATA, data)):\n",
    "        print(data, 'Creating directory to store train, dev and test splits.')\n",
    "        os.mkdir(os.path.join(FORMATTED_TRAIN_DATA, data))\n",
    "      \n",
    "      df = pd.read_csv(training_file, sep='\\t', names=['ID', 'text', 'label'], header=0)\n",
    "      df[['text', 'label']].to_csv(os.path.join(FORMATTED_TRAIN_DATA, data, 'train.tsv'), sep='\\t', index=False)\n",
    "    else:\n",
    "      print(training_file + ' skipped!')\n",
    "else:\n",
    "  print('No files are found in this directory!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S2Dup8GHl1Q"
   },
   "source": [
    "After running the code above, a new folder (called formated-train-data) with formated files is created in the \"datasets\" folder in the train sub-folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LgeVN_wXGrq"
   },
   "source": [
    "##b. <font color='red'>`(Optional) Creating Evaluation (Dev and Test) sets from the available training data`</font>\n",
    "\n",
    "You may wish to create train and evaluation (dev and test) sets from the training data provided. If you wish to do so, you can run any of the cells below`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APxVxL06lfux"
   },
   "source": [
    "###i. If you want to create both the Dev and Test sets, run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1371,
     "status": "ok",
     "timestamp": 1666884273844,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "aVq1Blz0YF2b",
    "outputId": "f1397651-bded-49cb-e9ff-e9eac2dfe952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory found.\n",
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\SubtaskA\\train\\formatted-train-data\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(FORMATTED_TRAIN_DATA):\n",
    "  print('Data directory found.')\n",
    "  SPLITTED_DATA = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev-test')\n",
    "  if not os.path.isdir(SPLITTED_DATA):\n",
    "    print('Creating directory to store train, dev and test splits.')\n",
    "    os.mkdir(SPLITTED_DATA)\n",
    "else:\n",
    "  print(FORMATTED_TRAIN_DATA + ' is not a valid directory or does not exist!')\n",
    "\n",
    "%cd {FORMATTED_TRAIN_DATA}\n",
    "formatted_training_files = os.listdir()\n",
    "\n",
    "if len(formatted_training_files) > 0:\n",
    "  for data_name in formatted_training_files:\n",
    "    formatted_training_file = os.path.join(data_name, 'train.tsv')\n",
    "    if os.path.isfile(formatted_training_file):\n",
    "      labeled_tweets = pd.read_csv(formatted_training_file, sep='\\t', names=['text', 'label'], header=0)\n",
    "      train, dev, test = np.split(labeled_tweets.sample(frac=1, random_state=42), [int(.7*len(labeled_tweets)), int(.8*len(labeled_tweets))])\n",
    "\n",
    "      if not os.path.isdir(os.path.join(SPLITTED_DATA, data_name)):\n",
    "        print(data_name, 'Creating directory to store train, dev and test splits.')\n",
    "        os.mkdir(os.path.join(SPLITTED_DATA, data_name))\n",
    "\n",
    "      train.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'train.tsv'), sep='\\t', index=False)\n",
    "      dev.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'dev.tsv'), sep='\\t', index=False)\n",
    "      test.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name,'test.tsv'), sep='\\t', index=False)\n",
    "    else:\n",
    "      print(training_file + ' is not a supported file!')\n",
    "else:\n",
    "  print('No files are found in this directory!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDoyRlje3Rm7"
   },
   "source": [
    "#3) Training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AaXec415s0f"
   },
   "source": [
    "##a. Set project parameters\n",
    "\n",
    "For a list of models that be used for fine-tuning, you can check [HERE](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1666884274396,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "M0TKIFrE5ybV",
    "outputId": "d0b5ee8b-20f7-4fa3-b8b6-65b773c70060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\n",
      "Everything set. You can now start model training.\n"
     ]
    }
   ],
   "source": [
    "%cd {PROJECT_DIR}\n",
    "\n",
    "if LANGUAGE_CODE in languages:\n",
    "  # Model Training Parameters\n",
    "  MODEL_NAME_OR_PATH = 'Davlan/afro-xlmr-mini'\n",
    "  BATCH_SIZE = 32\n",
    "  LEARNING_RATE = 5e-5\n",
    "  NUMBER_OF_TRAINING_EPOCHS = 5\n",
    "  MAXIMUM_SEQUENCE_LENGTH = 128\n",
    "  SAVE_STEPS = -1\n",
    "\n",
    "  print('Everything set. You can now start model training.')\n",
    "\n",
    "else:\n",
    "  print(\"Invalid language code/Dataset not released. Please input any of the following released data\\n\\n\\t- 'am'\\n\\t- 'dz'\\n\\t- 'ha'\\n\\t- 'ig'\\n\\t- 'ma'\\n\\t- 'pcm'\\n\\t- 'pt'\\n\\t- 'sw'\\n\\t- 'yo'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2qcCQnU8dgQ"
   },
   "source": [
    "##b. Train the model\n",
    "\n",
    "In the section below, we provide three options: \n",
    "\n",
    "- 1) training model without any validation; \n",
    "- 2) training model with validation but without testing; \n",
    "- 3) training a model with validation and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fEw-qcEhYnx"
   },
   "source": [
    "###Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sE22Nmnx9lf1"
   },
   "source": [
    "\n",
    "\n",
    "####Starter Code: Datasets, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 19003,
     "status": "ok",
     "timestamp": 1666879952672,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "ERP2sja3i3uQ",
    "outputId": "730084f3-6cde-4ef4-a638-861aed0addea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1710cbdb5f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import warnings\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from datasets import Features, Value, ClassLabel, load_dataset, Dataset\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "\n",
    "\n",
    "np.random.seed(420)\n",
    "torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1666879952673,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "2k6XKSygMnT4",
    "outputId": "debd529a-c939-47d6-ae2c-60dd40430f4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Thomas\\\\Downloads\\\\afrisent/afrisent-semeval-2023\\\\SubtaskA\\\\train'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EJPw829sIRY3"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev-test', LANGUAGE_CODE)\n",
    "EVAL_DIR = os.path.join(PROJECT_DIR, TASK, 'dev')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_DIR, 'models', LANGUAGE_CODE + '_no_eval')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUMBER_OF_TRAINING_EPOCHS,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    overwrite_output_dir=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "60upFv0znHN5"
   },
   "outputs": [],
   "source": [
    "data_args = SimpleNamespace(**{\n",
    "    'max_seq_length': MAXIMUM_SEQUENCE_LENGTH,\n",
    "    'overwrite_cache': False,\n",
    "    'pad_to_max_length': True,\n",
    "    'max_train_samples': None,\n",
    "    'max_eval_samples': None,\n",
    "    'max_predict_samples': None\n",
    "})\n",
    "\n",
    "model_args = SimpleNamespace(**{\n",
    "    'model_name_or_path': MODEL_NAME_OR_PATH,\n",
    "    'config_name': None,\n",
    "    'tokenizer_name': None,\n",
    "    'data_dir': DATA_DIR,\n",
    "    'eval_dir': EVAL_DIR,\n",
    "    'cache_dir': None,\n",
    "    'do_lower_case': None,\n",
    "    'use_fast_tokenizer': True,\n",
    "    'model_revision': 'main',\n",
    "    'use_auth_token': False,\n",
    "    'ignore_mismatched_sizes': False\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2Nx63rcbnHXy"
   },
   "outputs": [],
   "source": [
    "# See all possible arguments in src/transformers/training_args.py\n",
    "# or by passing the --help flag to this script.\n",
    "# We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "# parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "# model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "#send_example_telemetry(\"run_xnli\", model_args)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "log_level = 0 #training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1666879952910,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "yGlFrGztnhVX",
    "outputId": "52cf93f7-869b-4240-dd91-546daaa57a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2022 04:22:41 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1666879952911,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "ZSh2DwJzM_cI",
    "outputId": "326b1c0d-2cd5-4516-a142-7e0342714bd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yo'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGUAGE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1666883839778,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "hZc-mzkXnhdF",
    "outputId": "06e5bc16-fd84-4c40-f720-3558328f4fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'neutral', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
    "# download the dataset.\n",
    "# Downloading and loading xnli dataset from the hub.\n",
    "\n",
    "\n",
    "if training_args.do_train:\n",
    "\n",
    "    #train_dataset = load_dataset('csv', data_files={'train': model_args.data_dir + '/train.csv'}, cache_dir=model_args.cache_dir)\n",
    "    #df = train_dataset[\"train\"].to_pandas()\n",
    "    #label_list = df['label'].unique().tolist()\n",
    "    #label_list = train_dataset.features[\"label\"].names\n",
    "    df = pd.read_csv(model_args.data_dir + '/train.tsv', sep='\\t')\n",
    "    df = df.dropna()\n",
    "    train_dataset = Dataset.from_pandas(df)\n",
    "    label_list = df['label'].unique().tolist()\n",
    "\n",
    "if training_args.do_eval:\n",
    "    #eval_dataset = load_dataset('csv', data_files={'validation': model_args.data_dir + '/dev.csv'}, cache_dir=model_args.cache_dir)\n",
    "\n",
    "    #df = eval_dataset[\"validation\"].to_pandas()\n",
    "    #label_list = df['label'].unique().tolist()\n",
    "    #label_list = eval_dataset.features[\"label\"].names\n",
    "    df = pd.read_csv(model_args.data_dir + '/dev.tsv', sep='\\t')\n",
    "    df = df.dropna()\n",
    "    eval_dataset = Dataset.from_pandas(df)\n",
    "    label_list = df['label'].unique().tolist()\n",
    "\n",
    "if training_args.do_predict:\n",
    "    #predict_dataset = load_dataset('csv', data_files={'test': model_args.data_dir + '/test.csv'}, cache_dir=model_args.cache_dir)\n",
    "\n",
    "    #df = predict_dataset[\"test\"].to_pandas()\n",
    "    #label_list = df['label'].unique().tolist()\n",
    "    #label_list = predict_dataset.features[\"label\"].names\n",
    "    df = pd.read_csv(model_args.data_dir + '/test.tsv', sep='\\t')\n",
    "    df = df.dropna()\n",
    "    predict_dataset = Dataset.from_pandas(df)\n",
    "    label_list = df['label'].unique().tolist()\n",
    "\n",
    "# Labels\n",
    "num_labels = len(label_list)\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9gxi4Ll-HFQ"
   },
   "source": [
    "####Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6173,
     "status": "ok",
     "timestamp": 1666883849304,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "pd_l2PG3nhiY",
    "outputId": "5753544f-0e2b-489c-edfc-1014449d4170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|modeling_utils.py:2596] 2022-10-30 04:22:42,741 >> Some weights of the model checkpoint at Davlan/afro-xlmr-mini were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2608] 2022-10-30 04:22:42,742 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-mini and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "#         finetuning_task=\"xnli\",\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    do_lower_case=model_args.do_lower_case,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    "    ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1666883849305,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "slMaLbYvnHmP",
    "outputId": "7c6e8b47-3511-42b8-a689-317cd74f3190"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def preprocess_function(examples):\\n    # Tokenize the texts\\n    return tokenizer(\\n        examples[\"premise\"],\\n        examples[\"hypothesis\"],\\n        padding=padding,\\n        max_length=data_args.max_seq_length,\\n        truncation=True,\\n    )\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the datasets\n",
    "# Padding strategy\n",
    "if data_args.pad_to_max_length:\n",
    "    padding = \"max_length\"\n",
    "else:\n",
    "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "    padding = False\n",
    "\n",
    "# Some models have set the order of the labels to use, so let's make sure we do use it.\n",
    "label_to_id = None\n",
    "label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "if label_to_id is not None:\n",
    "    model.config.label2id = label_to_id\n",
    "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "'''\n",
    "    def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    return tokenizer(\n",
    "        examples[\"premise\"],\n",
    "        examples[\"hypothesis\"],\n",
    "        padding=padding,\n",
    "        max_length=data_args.max_seq_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 21194,
     "status": "ok",
     "timestamp": 1666883870487,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "gKAQ2leboGN3",
    "outputId": "4d40c5b1-5009-4979-cf98-dfbbd898627e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdd40bf3f254dac801e83233903a0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4816cce7c24e329bd5a1cad9d32b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    #print\n",
    "    texts =(examples['text'],)\n",
    "    result = tokenizer(*texts, padding=padding, max_length=data_args.max_seq_length, truncation=True)\n",
    "    #print(examples['text'])\n",
    "    #result = tokenizer(examples['text'], examples['text'], padding=padding, max_length=data_args.max_seq_length, truncation=True)\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    \n",
    "    result['length'], result[\"tokenized\"] = [], []\n",
    "    for input_ids in result['input_ids']:\n",
    "        toks = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)\n",
    "        result['length'].append(len(toks)+2)\n",
    "        result['tokenized'].append(' '.join(toks))\n",
    "    return result\n",
    "\n",
    "if training_args.do_train:\n",
    "    if data_args.max_train_samples is not None:\n",
    "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "    with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "        train_dataset = train_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )\n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "if training_args.do_eval:\n",
    "    if data_args.max_eval_samples is not None:\n",
    "        max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "        eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "    with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )\n",
    "\n",
    "if training_args.do_predict:\n",
    "    if data_args.max_predict_samples is not None:\n",
    "        max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
    "        predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
    "    with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n",
    "        predict_dataset = predict_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on prediction dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1666883921333,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "6Hi6VwK5O1ve",
    "outputId": "1d30958b-4b3d-409f-e9c2-97f9fb4a07f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask', 'length', 'tokenized'],\n",
       "     num_rows: 5965\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask', 'length', 'tokenized'],\n",
       "     num_rows: 852\n",
       " }))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNWVct2XARap"
   },
   "source": [
    "####LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "U6a2g_l1rZan"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6tSt7NFOezKe"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, emb_dim=300, num_layers=1, dropout=0.5, lstm_dropout=0.0):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(len(tokenizer), emb_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=lstm_dropout)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(2*hidden_dim, 3)\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "\n",
    "        text_emb = self.embedding(text)\n",
    "\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.hidden_dim]\n",
    "        out_reverse = output[:, 0, self.hidden_dim:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        text_fea = self.fc(text_fea)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = text_fea\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "KzLIu8Jo7QJq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193, 4772, 852)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pts = len(train_dataset)\n",
    "shuffled_ids = np.arange(num_pts, dtype=int)\n",
    "np.random.shuffle(shuffled_ids)\n",
    "\n",
    "valid_ids = torch.LongTensor(np.array(train_dataset['input_ids'])[shuffled_ids[:num_pts // 5]])\n",
    "valid_lengths = torch.LongTensor(np.array(train_dataset['length'])[shuffled_ids[:num_pts // 5]]).cpu()\n",
    "valid_labels = torch.LongTensor(np.array(train_dataset['label'])[shuffled_ids[:num_pts // 5]])\n",
    "\n",
    "train_ids = torch.LongTensor(np.array(train_dataset['input_ids'])[shuffled_ids[num_pts // 5:]])\n",
    "train_lengths = torch.LongTensor(np.array(train_dataset['length'])[shuffled_ids[num_pts // 5:]]).cpu()\n",
    "train_labels = torch.LongTensor(np.array(train_dataset['label'])[shuffled_ids[num_pts // 5:]])\n",
    "\n",
    "eval_ids = torch.LongTensor(eval_dataset['input_ids'])\n",
    "eval_lengths = torch.LongTensor(eval_dataset['length']).cpu()\n",
    "eval_labels = torch.LongTensor(eval_dataset['label'])\n",
    "len(valid_ids), len(train_ids), len(eval_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "n_ZIAx_8UZzA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 0.0312 Val Acc: 0.583, Eval Acc: 0.526, Eval Acc @ Best Val 0.526, Eval F1: 0.566: 100%|█| 150/\n",
      "Epoch 2/15, Train Loss: 0.0253 Val Acc: 0.597, Eval Acc: 0.56, Eval Acc @ Best Val 0.56, Eval F1: 0.575: 100%|█| 150/15\n",
      "Epoch 3/15, Train Loss: 0.0192 Val Acc: 0.598, Eval Acc: 0.581, Eval Acc @ Best Val 0.581, Eval F1: 0.585: 100%|█| 150/\n",
      "Epoch 4/15, Train Loss: 0.0128 Val Acc: 0.605, Eval Acc: 0.575, Eval Acc @ Best Val 0.575, Eval F1: 0.574: 100%|█| 150/\n",
      "Epoch 5/15, Train Loss: 0.00816 Val Acc: 0.593, Eval Acc: 0.574, Eval Acc @ Best Val 0.574, Eval F1: 0.572: 100%|█| 150\n",
      "Epoch 6/15, Train Loss: 0.00537 Val Acc: 0.617, Eval Acc: 0.587, Eval Acc @ Best Val 0.587, Eval F1: 0.595: 100%|█| 150\n",
      "Epoch 7/15, Train Loss: 0.00377 Val Acc: 0.602, Eval Acc: 0.57, Eval Acc @ Best Val 0.57, Eval F1: 0.569: 100%|█| 150/1\n",
      "Epoch 8/15, Train Loss: 0.00339 Val Acc: 0.616, Eval Acc: 0.583, Eval Acc @ Best Val 0.583, Eval F1: 0.586: 100%|█| 150\n",
      "Epoch 9/15, Train Loss: 0.00213 Val Acc: 0.612, Eval Acc: 0.582, Eval Acc @ Best Val 0.582, Eval F1: 0.591: 100%|█| 150\n",
      "Epoch 10/15, Train Loss: 0.00105 Val Acc: 0.605, Eval Acc: 0.577, Eval Acc @ Best Val 0.577, Eval F1: 0.579: 100%|█| 15\n",
      "Epoch 11/15, Train Loss: 0.000925 Val Acc: 0.624, Eval Acc: 0.586, Eval Acc @ Best Val 0.586, Eval F1: 0.587: 100%|█| 1\n",
      "Epoch 12/15, Train Loss: 0.000585 Val Acc: 0.609, Eval Acc: 0.579, Eval Acc @ Best Val 0.579, Eval F1: 0.582: 100%|█| 1\n",
      "Epoch 13/15, Train Loss: 0.000456 Val Acc: 0.607, Eval Acc: 0.594, Eval Acc @ Best Val 0.594, Eval F1: 0.596: 100%|█| 1\n",
      "Epoch 14/15, Train Loss: 0.000424 Val Acc: 0.616, Eval Acc: 0.595, Eval Acc @ Best Val 0.595, Eval F1: 0.596: 100%|█| 1\n",
      "Epoch 15/15, Train Loss: 0.000511 Val Acc: 0.604, Eval Acc: 0.579, Eval Acc @ Best Val 0.579, Eval F1: 0.58: 100%|█| 15\n"
     ]
    }
   ],
   "source": [
    "def train(criterion = nn.CrossEntropyLoss(),\n",
    "          batch_size = 32,\n",
    "          num_epochs = 30,\n",
    "          eval_every = 4,\n",
    "          params={},\n",
    "          lr=0.0005,\n",
    "          leave=False):\n",
    "    model = LSTM(**params).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    eval_every = (len(train_ids) / batch_size) // eval_every\n",
    "    best_valid_acc = 0\n",
    "    best_valid_f1 = 0\n",
    "    model.train()\n",
    "    best_preds = []\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(range(0, len(train_ids), batch_size), leave=leave, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        total_train_loss = 0.0\n",
    "        total_points = 0\n",
    "        for i in pbar:           \n",
    "            labels = train_labels[i:i+batch_size].to(device)\n",
    "            inps = train_ids[i:i+batch_size].to(device)\n",
    "            lengths = train_lengths[i:i+batch_size]#.to(device)\n",
    "            output = model(inps, lengths)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            total_train_loss += loss.item()\n",
    "            total_points += labels.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + batch_size) % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    num_correct = 0\n",
    "                    eval_batch_size = 100\n",
    "                    for j in range(0, len(valid_ids), eval_batch_size):\n",
    "                        labels = valid_labels[j:j+eval_batch_size].to(device)\n",
    "                        inps = valid_ids[j:j+eval_batch_size].to(device)\n",
    "                        lengths = valid_lengths[j:j+eval_batch_size]#.to(device)\n",
    "                        output = model(inps, lengths)\n",
    "                        output = torch.argmax(output, -1)\n",
    "                        num_correct += torch.sum(output == labels).cpu().numpy()\n",
    "                    valid_accuracy = num_correct / len(valid_ids)\n",
    "\n",
    "                    accuracy = \"N/A\"\n",
    "                    preds = np.array([])\n",
    "                    if valid_accuracy > best_valid_acc:\n",
    "                        num_correct = 0\n",
    "                        eval_batch_size = 100\n",
    "                        for j in range(0, len(eval_ids), eval_batch_size):\n",
    "                            labels = eval_labels[j:j+eval_batch_size].to(device)\n",
    "                            inps = eval_ids[j:j+eval_batch_size].to(device)\n",
    "                            lengths = eval_lengths[j:j+eval_batch_size]#.to(device)\n",
    "                            output = model(inps, lengths)\n",
    "                            output = torch.argmax(output, -1)\n",
    "                            preds = np.append(preds, output.cpu().numpy())\n",
    "                            num_correct += torch.sum(output == labels).cpu().numpy()\n",
    "                        accuracy = num_correct / len(eval_ids)\n",
    "                        best_valid_f1 = f1_score(preds, eval_labels, average='weighted')\n",
    "                        best_preds = preds\n",
    "                        best_valid_acc = accuracy\n",
    "                    pbar.set_description(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {(total_train_loss / total_points):.3} ' + \\\n",
    "                                         f'Val Acc: {valid_accuracy:.3}, Eval Acc: {accuracy:.3}, Eval Acc @ Best Val {best_valid_acc:.3}, Eval F1: {best_valid_f1:.3}')\n",
    "                model.train()\n",
    "    return best_valid_acc, best_valid_f1, preds, eval_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "acc, f1, preds, labels = train(num_epochs=15, params={\n",
    "    'hidden_dim': 128, 'emb_dim': 300, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5},\n",
    "      lr=0.001, leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-uWlHsFwUZ8z"
   },
   "outputs": [],
   "source": [
    "del train_ids\n",
    "del train_lengths\n",
    "del train_labels\n",
    "\n",
    "del eval_ids\n",
    "del eval_lengths\n",
    "del eval_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZx_-UywAcmj"
   },
   "source": [
    "####Pretrained Transformer Baseline - xlmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7xpriZKGrFCK",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:24:34,180 >> The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1607] 2022-10-30 04:24:34,191 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-10-30 04:24:34,192 >>   Num examples = 5965\n",
      "[INFO|trainer.py:1609] 2022-10-30 04:24:34,192 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1610] 2022-10-30 04:24:34,192 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1611] 2022-10-30 04:24:34,192 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1612] 2022-10-30 04:24:34,193 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-10-30 04:24:34,193 >>   Total optimization steps = 935\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='935' max='935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [935/935 02:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.903900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1852] 2022-10-30 04:26:58,435 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2656] 2022-10-30 04:26:58,453 >> Saving model checkpoint to C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_eval\n",
      "[INFO|configuration_utils.py:447] 2022-10-30 04:26:58,455 >> Configuration saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_eval\\config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-30 04:26:59,166 >> Model weights saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_eval\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2022-10-30 04:26:59,167 >> tokenizer config file saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_eval\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2022-10-30 04:26:59,168 >> Special tokens file saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_eval\\special_tokens_map.json\n",
      "[INFO|trainer.py:725] 2022-10-30 04:26:59,439 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:26:59,441 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:26:59,441 >>   Num examples = 852\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:26:59,441 >>   Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  total_flos               =   457439GF\n",
      "  train_loss               =     0.8028\n",
      "  train_runtime            = 0:02:24.25\n",
      "  train_samples            =       5965\n",
      "  train_samples_per_second =    206.746\n",
      "  train_steps_per_second   =      6.481\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089658b3e3784596ae467660329e3083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:02,152 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:02,153 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:02,154 >>   Num examples = 599\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:02,154 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c457e2f87d24663a0dc7c27dfe0db0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:03,672 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:03,674 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:03,674 >>   Num examples = 165\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:03,674 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3b8473485d4647a0b14c79a20d7846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:06,660 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:06,662 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:06,662 >>   Num examples = 1417\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:06,662 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec707c4401e466085be17416c826c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:11,379 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:11,380 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:11,380 >>   Num examples = 1019\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:11,381 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d835ae387ef343419f92a11e43edf59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:14,532 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:14,534 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:14,534 >>   Num examples = 558\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:14,534 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74cca34fda441458d03ad4a541452b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:16,573 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:16,574 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:16,574 >>   Num examples = 512\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:16,574 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74232e62e2e416eb2c1f7ec02d65482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:18,146 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:18,147 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:18,147 >>   Num examples = 306\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:18,148 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc871dfbcd740e1aa88e0541989836e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:19,115 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:19,117 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:19,117 >>   Num examples = 181\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:19,118 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50166062a18d454d80a2fbdd0b70aa83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:21,217 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:21,219 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:21,219 >>   Num examples = 852\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:21,219 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6754704067024604\n",
      "***** eval metrics *****\n",
      "  eval_accuracy           =     0.6737\n",
      "  eval_loss               =     0.7884\n",
      "  eval_runtime            = 0:00:01.53\n",
      "  eval_samples_per_second =    556.602\n",
      "  eval_steps_per_second   =     69.902\n"
     ]
    }
   ],
   "source": [
    "# Get the metric function\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
    "# predictions and label_ids field) and has to return a dictionary string to float.\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n",
    "if data_args.pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "elif training_args.fp16:\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "else:\n",
    "    data_collator = None\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "# Evaluation\n",
    "if training_args.do_eval or True:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "    max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "    \n",
    "    \n",
    "    splitted_A = os.path.join(PROJECT_DIR, 'SubtaskA', 'train', 'splitted-train-dev-test')\n",
    "    \n",
    "    try:\n",
    "        LANGUAGE_CODE\n",
    "    except NameError:\n",
    "        LANGUAGE_CODE = 'combined'\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    data = []\n",
    "    for lang in languages:\n",
    "        eval_path = os.path.join(splitted_A, lang)\n",
    "        df = pd.read_csv(eval_path + '/dev.tsv', sep='\\t')\n",
    "        df = df.dropna()\n",
    "        lang_eval = Dataset.from_pandas(df)\n",
    "        lang_eval = lang_eval.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )\n",
    "        \n",
    "        predictions, labels, metrics = trainer.predict(lang_eval, metric_key_prefix=\"eval\")\n",
    "        \n",
    "        if LANGUAGE_CODE == lang:\n",
    "            print(f1_score(np.argmax(predictions, axis=1), labels, average='weighted'))\n",
    "        \n",
    "        data.append([LANGUAGE_CODE, lang, str(list(predictions)), str(list(labels))])\n",
    "    df = pd.DataFrame(data, columns=['source', 'target', 'predictions', 'labels'])\n",
    "    df.to_csv(f'{LANGUAGE_CODE}_preds.csv', index=False)\n",
    "\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "# Prediction\n",
    "if training_args.do_predict:\n",
    "    logger.info(\"*** Predict ***\")\n",
    "    predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n",
    "\n",
    "    max_predict_samples = (\n",
    "        data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n",
    "    )\n",
    "    metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"predict\", metrics)\n",
    "    trainer.save_metrics(\"predict\", metrics)\n",
    "\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    output_predict_file = os.path.join(training_args.output_dir, \"predictions.txt\")\n",
    "    if trainer.is_world_process_zero():\n",
    "        with open(output_predict_file, \"w\") as writer:\n",
    "            writer.write(\"index\\tprediction\\n\")\n",
    "            for index, item in enumerate(predictions):\n",
    "                item = label_list[item]\n",
    "                writer.write(f\"{index}\\t{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h06uE1yLGAG_"
   },
   "source": [
    "#### KinyaBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1EGi2jpx5BvV",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2022 04:27:27 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "10/30/2022 04:27:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=128.0,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\\runs\\Oct30_04-27-27_LAPTOP-42G56F8J,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test,\n",
      "save_on_each_node=False,\n",
      "save_steps=-1,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "['positive', 'neutral', 'negative']\n",
      "10/30/2022 04:27:28 - INFO - __main__ - Sample 204 of the training set: {'text': 'RT @user: TAKE IT #SIckleCellAwareness in #Yoruba Osu kesan, osu #september je osu pataki fun wa gegebi awujo. Idi ni pe, a maa n fi…', 'label': 1, 'input_ids': [2, 1, 1, 26684, 1, 1, 1, 1, 1, 128, 1, 1, 1, 993, 223, 6, 10322, 50, 1, 17249, 675, 10322, 50, 3468, 23792, 8919, 139, 845, 107, 109, 6031, 741, 7, 1, 157, 319, 6, 21, 20560, 34, 708, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "{'loss': 13408.548, 'learning_rate': 59.55080213903744, 'epoch': 2.67}\n",
      "{'train_runtime': 166.0997, 'train_samples_per_second': 179.561, 'train_steps_per_second': 5.629, 'train_loss': 9198.849331550802, 'epoch': 5.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =  9198.8493\n",
      "  train_runtime            = 0:02:46.09\n",
      "  train_samples            =       5965\n",
      "  train_samples_per_second =    179.561\n",
      "  train_steps_per_second   =      5.629\n",
      "10/30/2022 04:30:15 - INFO - __main__ - *** Evaluate ***\n",
      "f1 score: 0.5510204081632654\n",
      "***** eval metrics *****\n",
      "  eval_accuracy           =     0.3803\n",
      "  eval_loss               =    251.986\n",
      "  eval_runtime            = 0:00:01.74\n",
      "  eval_samples_per_second =    487.025\n",
      "  eval_steps_per_second   =     61.164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[INFO|configuration_utils.py:653] 2022-10-30 04:27:27,296 >> loading configuration file config.json from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-30 04:27:27,300 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"jean-paul/KinyaBERT-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:418] 2022-10-30 04:27:27,413 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:653] 2022-10-30 04:27:27,521 >> loading configuration file config.json from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-30 04:27:27,522 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"jean-paul/KinyaBERT-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-30 04:27:27,743 >> loading file vocab.txt from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-30 04:27:27,743 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-30 04:27:27,743 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-30 04:27:27,743 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-30 04:27:27,743 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:653] 2022-10-30 04:27:27,744 >> loading configuration file config.json from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-30 04:27:27,744 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"jean-paul/KinyaBERT-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:653] 2022-10-30 04:27:27,759 >> loading configuration file config.json from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-30 04:27:27,759 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"jean-paul/KinyaBERT-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2156] 2022-10-30 04:27:27,811 >> loading weights file pytorch_model.bin from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2596] 2022-10-30 04:27:28,184 >> Some weights of the model checkpoint at jean-paul/KinyaBERT-small were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2608] 2022-10-30 04:27:28,184 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at jean-paul/KinyaBERT-small and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/6 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  33%|###3      | 2/6 [00:00<00:00, 13.69ba/s]\n",
      "Running tokenizer on train dataset:  67%|######6   | 4/6 [00:00<00:00, 14.45ba/s]\n",
      "Running tokenizer on train dataset:  83%|########3 | 5/6 [00:00<00:00, 12.34ba/s]\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Thomas\\anaconda3\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Thomas\\anaconda3\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\U0001f914' in position 124: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\starter_kit\\run_textclass.py\", line 456, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\starter_kit\\run_textclass.py\", line 342, in main\n",
      "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
      "Message: \"Sample 5238 of the training set: {'text': 'Nje iwo mo awon akoko ojo ni ede yoruba? \\U0001f914 #akokoojo #edeyoruba #yoruba #lagelufm967 https://t.co/N7p9slLWfm', 'label': 1, 'input_ids': [2, 1, 29, 160, 503, 29942, 167, 1518, 95, 35, 741, 157, 4677, 63, 205, 11030, 19, 1, 1, 1518, 95, 56, 741, 1, 4677, 63, 105, 11030, 1, 205, 11030, 1, 660, 16922, 50, 49, 57, 15937, 80, 22993, 64, 6606, 1, 1, 1, 40, 7, 279, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Thomas\\anaconda3\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Thomas\\anaconda3\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u1eb9' in position 142: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\starter_kit\\run_textclass.py\", line 456, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\starter_kit\\run_textclass.py\", line 342, in main\n",
      "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
      "Message: \"Sample 912 of the training set: {'text': 'RT @user: ... Jíjáde wa ká wa máa pàdé àgbákò. Ohun tí a ó j\\u1eb9 là \\u0144 wá l\\u1ecd Bàbá, ká má pàdé ohun tí yó j\\u1eb9 wá. #Iwure #OjoAje #Yoruba', 'label': 0, 'input_ids': [2, 1, 1, 26684, 1, 7, 7, 7, 1, 139, 1, 139, 1, 1, 1, 7, 1, 1, 21, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 35, 148, 51, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\"\n",
      "Arguments: ()\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "[INFO|trainer.py:725] 2022-10-30 04:27:29,112 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1607] 2022-10-30 04:27:29,120 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-10-30 04:27:29,120 >>   Num examples = 5965\n",
      "[INFO|trainer.py:1609] 2022-10-30 04:27:29,120 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1610] 2022-10-30 04:27:29,120 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1611] 2022-10-30 04:27:29,120 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1612] 2022-10-30 04:27:29,121 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-10-30 04:27:29,121 >>   Total optimization steps = 935\n",
      "\n",
      "  0%|          | 0/935 [00:00<?, ?it/s]\n",
      "  0%|          | 1/935 [00:01<25:44,  1.65s/it]\n",
      "  0%|          | 2/935 [00:02<13:55,  1.12it/s]\n",
      "  0%|          | 3/935 [00:02<08:47,  1.77it/s]\n",
      "  0%|          | 4/935 [00:02<06:22,  2.43it/s]\n",
      "  1%|          | 5/935 [00:02<05:03,  3.07it/s]\n",
      "  1%|          | 6/935 [00:02<04:15,  3.64it/s]\n",
      "  1%|          | 7/935 [00:02<03:44,  4.14it/s]\n",
      "  1%|          | 8/935 [00:03<03:24,  4.52it/s]\n",
      "  1%|          | 9/935 [00:03<03:11,  4.85it/s]\n",
      "  1%|1         | 10/935 [00:03<03:02,  5.07it/s]\n",
      "  1%|1         | 11/935 [00:03<02:55,  5.27it/s]\n",
      "  1%|1         | 12/935 [00:03<02:51,  5.39it/s]\n",
      "  1%|1         | 13/935 [00:03<02:48,  5.49it/s]\n",
      "  1%|1         | 14/935 [00:04<02:46,  5.52it/s]\n",
      "  2%|1         | 15/935 [00:04<02:44,  5.59it/s]\n",
      "  2%|1         | 16/935 [00:04<02:43,  5.62it/s]\n",
      "  2%|1         | 17/935 [00:04<02:42,  5.65it/s]\n",
      "  2%|1         | 18/935 [00:04<02:42,  5.63it/s]\n",
      "  2%|2         | 19/935 [00:04<02:42,  5.65it/s]\n",
      "  2%|2         | 20/935 [00:05<02:41,  5.65it/s]\n",
      "  2%|2         | 21/935 [00:05<02:41,  5.66it/s]\n",
      "  2%|2         | 22/935 [00:05<02:41,  5.64it/s]\n",
      "  2%|2         | 23/935 [00:05<02:41,  5.65it/s]\n",
      "  3%|2         | 24/935 [00:05<02:40,  5.67it/s]\n",
      "  3%|2         | 25/935 [00:06<02:40,  5.67it/s]\n",
      "  3%|2         | 26/935 [00:06<02:39,  5.70it/s]\n",
      "  3%|2         | 27/935 [00:06<02:39,  5.71it/s]\n",
      "  3%|2         | 28/935 [00:06<02:38,  5.71it/s]\n",
      "  3%|3         | 29/935 [00:06<02:39,  5.69it/s]\n",
      "  3%|3         | 30/935 [00:06<02:38,  5.71it/s]\n",
      "  3%|3         | 31/935 [00:07<02:38,  5.69it/s]\n",
      "  3%|3         | 32/935 [00:07<02:37,  5.72it/s]\n",
      "  4%|3         | 33/935 [00:07<02:38,  5.70it/s]\n",
      "  4%|3         | 34/935 [00:07<02:37,  5.71it/s]\n",
      "  4%|3         | 35/935 [00:07<02:37,  5.70it/s]\n",
      "  4%|3         | 36/935 [00:07<02:37,  5.70it/s]\n",
      "  4%|3         | 37/935 [00:08<02:38,  5.66it/s]\n",
      "  4%|4         | 38/935 [00:08<02:38,  5.68it/s]\n",
      "  4%|4         | 39/935 [00:08<02:38,  5.65it/s]\n",
      "  4%|4         | 40/935 [00:08<02:37,  5.69it/s]\n",
      "  4%|4         | 41/935 [00:08<02:37,  5.69it/s]\n",
      "  4%|4         | 42/935 [00:09<02:36,  5.70it/s]\n",
      "  5%|4         | 43/935 [00:09<02:36,  5.68it/s]\n",
      "  5%|4         | 44/935 [00:09<02:36,  5.69it/s]\n",
      "  5%|4         | 45/935 [00:09<02:35,  5.71it/s]\n",
      "  5%|4         | 46/935 [00:09<02:35,  5.70it/s]\n",
      "  5%|5         | 47/935 [00:09<02:35,  5.70it/s]\n",
      "  5%|5         | 48/935 [00:10<02:35,  5.70it/s]\n",
      "  5%|5         | 49/935 [00:10<02:34,  5.72it/s]\n",
      "  5%|5         | 50/935 [00:10<02:35,  5.71it/s]\n",
      "  5%|5         | 51/935 [00:10<02:34,  5.72it/s]\n",
      "  6%|5         | 52/935 [00:10<02:34,  5.70it/s]\n",
      "  6%|5         | 53/935 [00:10<02:34,  5.71it/s]\n",
      "  6%|5         | 54/935 [00:11<02:34,  5.70it/s]\n",
      "  6%|5         | 55/935 [00:11<02:34,  5.71it/s]\n",
      "  6%|5         | 56/935 [00:11<02:34,  5.68it/s]\n",
      "  6%|6         | 57/935 [00:11<02:33,  5.71it/s]\n",
      "  6%|6         | 58/935 [00:11<02:33,  5.71it/s]\n",
      "  6%|6         | 59/935 [00:12<02:33,  5.72it/s]\n",
      "  6%|6         | 60/935 [00:12<02:33,  5.70it/s]\n",
      "  7%|6         | 61/935 [00:12<02:32,  5.72it/s]\n",
      "  7%|6         | 62/935 [00:12<02:32,  5.71it/s]\n",
      "  7%|6         | 63/935 [00:12<02:33,  5.68it/s]\n",
      "  7%|6         | 64/935 [00:12<02:33,  5.66it/s]\n",
      "  7%|6         | 65/935 [00:13<02:33,  5.68it/s]\n",
      "  7%|7         | 66/935 [00:13<02:33,  5.67it/s]\n",
      "  7%|7         | 67/935 [00:13<02:32,  5.69it/s]\n",
      "  7%|7         | 68/935 [00:13<02:32,  5.67it/s]\n",
      "  7%|7         | 69/935 [00:13<02:32,  5.68it/s]\n",
      "  7%|7         | 70/935 [00:13<02:31,  5.70it/s]\n",
      "  8%|7         | 71/935 [00:14<02:31,  5.70it/s]\n",
      "  8%|7         | 72/935 [00:14<02:30,  5.73it/s]\n",
      "  8%|7         | 73/935 [00:14<02:31,  5.70it/s]\n",
      "  8%|7         | 74/935 [00:14<02:30,  5.71it/s]\n",
      "  8%|8         | 75/935 [00:14<02:30,  5.72it/s]\n",
      "  8%|8         | 76/935 [00:15<02:29,  5.74it/s]\n",
      "  8%|8         | 77/935 [00:15<02:30,  5.70it/s]\n",
      "  8%|8         | 78/935 [00:15<02:30,  5.70it/s]\n",
      "  8%|8         | 79/935 [00:15<02:30,  5.70it/s]\n",
      "  9%|8         | 80/935 [00:15<02:29,  5.71it/s]\n",
      "  9%|8         | 81/935 [00:15<02:30,  5.69it/s]\n",
      "  9%|8         | 82/935 [00:16<02:29,  5.70it/s]\n",
      "  9%|8         | 83/935 [00:16<02:29,  5.68it/s]\n",
      "  9%|8         | 84/935 [00:16<02:29,  5.70it/s]\n",
      "  9%|9         | 85/935 [00:16<02:28,  5.71it/s]\n",
      "  9%|9         | 86/935 [00:16<02:28,  5.70it/s]\n",
      "  9%|9         | 87/935 [00:16<02:29,  5.67it/s]\n",
      "  9%|9         | 88/935 [00:17<02:29,  5.67it/s]\n",
      " 10%|9         | 89/935 [00:17<02:29,  5.65it/s]\n",
      " 10%|9         | 90/935 [00:17<02:28,  5.68it/s]\n",
      " 10%|9         | 91/935 [00:17<02:28,  5.67it/s]\n",
      " 10%|9         | 92/935 [00:17<02:28,  5.68it/s]\n",
      " 10%|9         | 93/935 [00:17<02:28,  5.65it/s]\n",
      " 10%|#         | 94/935 [00:18<02:28,  5.66it/s]\n",
      " 10%|#         | 95/935 [00:18<02:27,  5.69it/s]\n",
      " 10%|#         | 96/935 [00:18<02:27,  5.69it/s]\n",
      " 10%|#         | 97/935 [00:18<02:27,  5.70it/s]\n",
      " 10%|#         | 98/935 [00:18<02:26,  5.69it/s]\n",
      " 11%|#         | 99/935 [00:19<02:26,  5.71it/s]\n",
      " 11%|#         | 100/935 [00:19<02:26,  5.70it/s]\n",
      " 11%|#         | 101/935 [00:19<02:25,  5.71it/s]\n",
      " 11%|#         | 102/935 [00:19<02:26,  5.69it/s]\n",
      " 11%|#1        | 103/935 [00:19<02:25,  5.72it/s]\n",
      " 11%|#1        | 104/935 [00:19<02:25,  5.70it/s]\n",
      " 11%|#1        | 105/935 [00:20<02:25,  5.72it/s]\n",
      " 11%|#1        | 106/935 [00:20<02:25,  5.70it/s]\n",
      " 11%|#1        | 107/935 [00:20<02:24,  5.72it/s]\n",
      " 12%|#1        | 108/935 [00:20<02:24,  5.71it/s]\n",
      " 12%|#1        | 109/935 [00:20<02:24,  5.73it/s]\n",
      " 12%|#1        | 110/935 [00:20<02:24,  5.69it/s]\n",
      " 12%|#1        | 111/935 [00:21<02:24,  5.71it/s]\n",
      " 12%|#1        | 112/935 [00:21<02:25,  5.67it/s]\n",
      " 12%|#2        | 113/935 [00:21<02:24,  5.69it/s]\n",
      " 12%|#2        | 114/935 [00:21<02:25,  5.66it/s]\n",
      " 12%|#2        | 115/935 [00:21<02:24,  5.66it/s]\n",
      " 12%|#2        | 116/935 [00:22<02:25,  5.64it/s]\n",
      " 13%|#2        | 117/935 [00:22<02:24,  5.66it/s]\n",
      " 13%|#2        | 118/935 [00:22<02:24,  5.67it/s]\n",
      " 13%|#2        | 119/935 [00:22<02:23,  5.69it/s]\n",
      " 13%|#2        | 120/935 [00:22<02:22,  5.71it/s]\n",
      " 13%|#2        | 121/935 [00:22<02:23,  5.68it/s]\n",
      " 13%|#3        | 122/935 [00:23<02:22,  5.70it/s]\n",
      " 13%|#3        | 123/935 [00:23<02:22,  5.70it/s]\n",
      " 13%|#3        | 124/935 [00:23<02:21,  5.73it/s]\n",
      " 13%|#3        | 125/935 [00:23<02:22,  5.69it/s]\n",
      " 13%|#3        | 126/935 [00:23<02:21,  5.70it/s]\n",
      " 14%|#3        | 127/935 [00:23<02:21,  5.70it/s]\n",
      " 14%|#3        | 128/935 [00:24<02:21,  5.70it/s]\n",
      " 14%|#3        | 129/935 [00:24<02:21,  5.68it/s]\n",
      " 14%|#3        | 130/935 [00:24<02:21,  5.69it/s]\n",
      " 14%|#4        | 131/935 [00:24<02:21,  5.67it/s]\n",
      " 14%|#4        | 132/935 [00:24<02:20,  5.70it/s]\n",
      " 14%|#4        | 133/935 [00:25<02:21,  5.68it/s]\n",
      " 14%|#4        | 134/935 [00:25<02:20,  5.69it/s]\n",
      " 14%|#4        | 135/935 [00:25<02:20,  5.68it/s]\n",
      " 15%|#4        | 136/935 [00:25<02:20,  5.69it/s]\n",
      " 15%|#4        | 137/935 [00:25<02:21,  5.66it/s]\n",
      " 15%|#4        | 138/935 [00:25<02:20,  5.68it/s]\n",
      " 15%|#4        | 139/935 [00:26<02:21,  5.64it/s]\n",
      " 15%|#4        | 140/935 [00:26<02:20,  5.66it/s]\n",
      " 15%|#5        | 141/935 [00:26<02:20,  5.63it/s]\n",
      " 15%|#5        | 142/935 [00:26<02:20,  5.65it/s]\n",
      " 15%|#5        | 143/935 [00:26<02:19,  5.67it/s]\n",
      " 15%|#5        | 144/935 [00:26<02:19,  5.68it/s]\n",
      " 16%|#5        | 145/935 [00:27<02:18,  5.70it/s]\n",
      " 16%|#5        | 146/935 [00:27<02:18,  5.68it/s]\n",
      " 16%|#5        | 147/935 [00:27<02:18,  5.70it/s]\n",
      " 16%|#5        | 148/935 [00:27<02:18,  5.70it/s]\n",
      " 16%|#5        | 149/935 [00:27<02:17,  5.72it/s]\n",
      " 16%|#6        | 150/935 [00:28<02:17,  5.69it/s]\n",
      " 16%|#6        | 151/935 [00:28<02:17,  5.70it/s]\n",
      " 16%|#6        | 152/935 [00:28<02:17,  5.69it/s]\n",
      " 16%|#6        | 153/935 [00:28<02:17,  5.70it/s]\n",
      " 16%|#6        | 154/935 [00:28<02:17,  5.66it/s]\n",
      " 17%|#6        | 155/935 [00:28<02:17,  5.67it/s]\n",
      " 17%|#6        | 156/935 [00:29<02:17,  5.66it/s]\n",
      " 17%|#6        | 157/935 [00:29<02:16,  5.69it/s]\n",
      " 17%|#6        | 158/935 [00:29<02:17,  5.66it/s]\n",
      " 17%|#7        | 159/935 [00:29<02:16,  5.68it/s]\n",
      " 17%|#7        | 160/935 [00:29<02:17,  5.63it/s]\n",
      " 17%|#7        | 161/935 [00:29<02:16,  5.66it/s]\n",
      " 17%|#7        | 162/935 [00:30<02:16,  5.65it/s]\n",
      " 17%|#7        | 163/935 [00:30<02:16,  5.64it/s]\n",
      " 18%|#7        | 164/935 [00:30<02:16,  5.66it/s]\n",
      " 18%|#7        | 165/935 [00:30<02:15,  5.69it/s]\n",
      " 18%|#7        | 166/935 [00:30<02:14,  5.70it/s]\n",
      " 18%|#7        | 167/935 [00:31<02:14,  5.70it/s]\n",
      " 18%|#7        | 168/935 [00:31<02:14,  5.70it/s]\n",
      " 18%|#8        | 169/935 [00:31<02:14,  5.71it/s]\n",
      " 18%|#8        | 170/935 [00:31<02:13,  5.72it/s]\n",
      " 18%|#8        | 171/935 [00:31<02:14,  5.70it/s]\n",
      " 18%|#8        | 172/935 [00:31<02:13,  5.71it/s]\n",
      " 19%|#8        | 173/935 [00:32<02:13,  5.71it/s]\n",
      " 19%|#8        | 174/935 [00:32<02:12,  5.73it/s]\n",
      " 19%|#8        | 175/935 [00:32<02:13,  5.71it/s]\n",
      " 19%|#8        | 176/935 [00:32<02:12,  5.71it/s]\n",
      " 19%|#8        | 177/935 [00:32<02:12,  5.70it/s]\n",
      " 19%|#9        | 178/935 [00:32<02:12,  5.72it/s]\n",
      " 19%|#9        | 179/935 [00:33<02:13,  5.67it/s]\n",
      " 19%|#9        | 180/935 [00:33<02:12,  5.68it/s]\n",
      " 19%|#9        | 181/935 [00:33<02:13,  5.64it/s]\n",
      " 19%|#9        | 182/935 [00:33<02:12,  5.67it/s]\n",
      " 20%|#9        | 183/935 [00:33<02:12,  5.66it/s]\n",
      " 20%|#9        | 184/935 [00:33<02:12,  5.68it/s]\n",
      " 20%|#9        | 185/935 [00:34<02:13,  5.62it/s]\n",
      " 20%|#9        | 186/935 [00:34<02:12,  5.64it/s]\n",
      " 20%|##        | 188/935 [00:34<01:56,  6.41it/s]\n",
      " 20%|##        | 189/935 [00:34<02:00,  6.20it/s]\n",
      " 20%|##        | 190/935 [00:34<02:02,  6.08it/s]\n",
      " 20%|##        | 191/935 [00:35<02:05,  5.94it/s]\n",
      " 21%|##        | 192/935 [00:35<02:06,  5.87it/s]\n",
      " 21%|##        | 193/935 [00:35<02:07,  5.80it/s]\n",
      " 21%|##        | 194/935 [00:35<02:08,  5.78it/s]\n",
      " 21%|##        | 195/935 [00:35<02:09,  5.72it/s]\n",
      " 21%|##        | 196/935 [00:36<02:09,  5.71it/s]\n",
      " 21%|##1       | 197/935 [00:36<02:09,  5.68it/s]\n",
      " 21%|##1       | 198/935 [00:36<02:09,  5.69it/s]\n",
      " 21%|##1       | 199/935 [00:36<02:09,  5.70it/s]\n",
      " 21%|##1       | 200/935 [00:36<02:09,  5.69it/s]\n",
      " 21%|##1       | 201/935 [00:36<02:09,  5.69it/s]\n",
      " 22%|##1       | 202/935 [00:37<02:09,  5.68it/s]\n",
      " 22%|##1       | 203/935 [00:37<02:08,  5.70it/s]\n",
      " 22%|##1       | 204/935 [00:37<02:08,  5.70it/s]\n",
      " 22%|##1       | 205/935 [00:37<02:07,  5.71it/s]\n",
      " 22%|##2       | 206/935 [00:37<02:08,  5.69it/s]\n",
      " 22%|##2       | 207/935 [00:37<02:07,  5.72it/s]\n",
      " 22%|##2       | 208/935 [00:38<02:07,  5.70it/s]\n",
      " 22%|##2       | 209/935 [00:38<02:07,  5.71it/s]\n",
      " 22%|##2       | 210/935 [00:38<02:07,  5.69it/s]\n",
      " 23%|##2       | 211/935 [00:38<02:07,  5.70it/s]\n",
      " 23%|##2       | 212/935 [00:38<02:07,  5.69it/s]\n",
      " 23%|##2       | 213/935 [00:39<02:06,  5.72it/s]\n",
      " 23%|##2       | 214/935 [00:39<02:06,  5.68it/s]\n",
      " 23%|##2       | 215/935 [00:39<02:06,  5.69it/s]\n",
      " 23%|##3       | 216/935 [00:39<02:06,  5.67it/s]\n",
      " 23%|##3       | 217/935 [00:39<02:06,  5.68it/s]\n",
      " 23%|##3       | 218/935 [00:39<02:06,  5.66it/s]\n",
      " 23%|##3       | 219/935 [00:40<02:05,  5.69it/s]\n",
      " 24%|##3       | 220/935 [00:40<02:06,  5.67it/s]\n",
      " 24%|##3       | 221/935 [00:40<02:06,  5.66it/s]\n",
      " 24%|##3       | 222/935 [00:40<02:05,  5.69it/s]\n",
      " 24%|##3       | 223/935 [00:40<02:05,  5.68it/s]\n",
      " 24%|##3       | 224/935 [00:40<02:04,  5.70it/s]\n",
      " 24%|##4       | 225/935 [00:41<02:04,  5.69it/s]\n",
      " 24%|##4       | 226/935 [00:41<02:04,  5.70it/s]\n",
      " 24%|##4       | 227/935 [00:41<02:04,  5.71it/s]\n",
      " 24%|##4       | 228/935 [00:41<02:03,  5.72it/s]\n",
      " 24%|##4       | 229/935 [00:41<02:04,  5.69it/s]\n",
      " 25%|##4       | 230/935 [00:42<02:03,  5.70it/s]\n",
      " 25%|##4       | 231/935 [00:42<02:03,  5.70it/s]\n",
      " 25%|##4       | 232/935 [00:42<02:03,  5.71it/s]\n",
      " 25%|##4       | 233/935 [00:42<02:03,  5.66it/s]\n",
      " 25%|##5       | 234/935 [00:42<02:03,  5.68it/s]\n",
      " 25%|##5       | 235/935 [00:42<02:04,  5.64it/s]\n",
      " 25%|##5       | 236/935 [00:43<02:03,  5.67it/s]\n",
      " 25%|##5       | 237/935 [00:43<02:03,  5.67it/s]\n",
      " 25%|##5       | 238/935 [00:43<02:02,  5.69it/s]\n",
      " 26%|##5       | 239/935 [00:43<02:03,  5.64it/s]\n",
      " 26%|##5       | 240/935 [00:43<02:02,  5.67it/s]\n",
      " 26%|##5       | 241/935 [00:43<02:03,  5.63it/s]\n",
      " 26%|##5       | 242/935 [00:44<02:03,  5.63it/s]\n",
      " 26%|##5       | 243/935 [00:44<02:03,  5.63it/s]\n",
      " 26%|##6       | 244/935 [00:44<02:02,  5.65it/s]\n",
      " 26%|##6       | 245/935 [00:44<02:01,  5.67it/s]\n",
      " 26%|##6       | 246/935 [00:44<02:01,  5.67it/s]\n",
      " 26%|##6       | 247/935 [00:45<02:00,  5.69it/s]\n",
      " 27%|##6       | 248/935 [00:45<02:00,  5.68it/s]\n",
      " 27%|##6       | 249/935 [00:45<02:00,  5.69it/s]\n",
      " 27%|##6       | 250/935 [00:45<02:00,  5.71it/s]\n",
      " 27%|##6       | 251/935 [00:45<02:00,  5.70it/s]\n",
      " 27%|##6       | 252/935 [00:45<02:00,  5.68it/s]\n",
      " 27%|##7       | 253/935 [00:46<01:59,  5.70it/s]\n",
      " 27%|##7       | 254/935 [00:46<01:59,  5.70it/s]\n",
      " 27%|##7       | 255/935 [00:46<01:58,  5.72it/s]\n",
      " 27%|##7       | 256/935 [00:46<01:59,  5.69it/s]\n",
      " 27%|##7       | 257/935 [00:46<01:58,  5.70it/s]\n",
      " 28%|##7       | 258/935 [00:46<01:58,  5.70it/s]\n",
      " 28%|##7       | 259/935 [00:47<01:58,  5.69it/s]\n",
      " 28%|##7       | 260/935 [00:47<01:59,  5.66it/s]\n",
      " 28%|##7       | 261/935 [00:47<01:58,  5.67it/s]\n",
      " 28%|##8       | 262/935 [00:47<01:59,  5.65it/s]\n",
      " 28%|##8       | 263/935 [00:47<01:58,  5.66it/s]\n",
      " 28%|##8       | 264/935 [00:47<01:58,  5.67it/s]\n",
      " 28%|##8       | 265/935 [00:48<01:57,  5.69it/s]\n",
      " 28%|##8       | 266/935 [00:48<01:58,  5.65it/s]\n",
      " 29%|##8       | 267/935 [00:48<01:58,  5.66it/s]\n",
      " 29%|##8       | 268/935 [00:48<01:57,  5.67it/s]\n",
      " 29%|##8       | 269/935 [00:48<01:57,  5.69it/s]\n",
      " 29%|##8       | 270/935 [00:49<01:56,  5.70it/s]\n",
      " 29%|##8       | 271/935 [00:49<01:56,  5.68it/s]\n",
      " 29%|##9       | 272/935 [00:49<01:56,  5.70it/s]\n",
      " 29%|##9       | 273/935 [00:49<01:56,  5.69it/s]\n",
      " 29%|##9       | 274/935 [00:49<01:55,  5.70it/s]\n",
      " 29%|##9       | 275/935 [00:49<01:56,  5.68it/s]\n",
      " 30%|##9       | 276/935 [00:50<01:56,  5.67it/s]\n",
      " 30%|##9       | 277/935 [00:50<01:56,  5.66it/s]\n",
      " 30%|##9       | 278/935 [00:50<01:55,  5.68it/s]\n",
      " 30%|##9       | 279/935 [00:50<01:55,  5.69it/s]\n",
      " 30%|##9       | 280/935 [00:50<01:54,  5.71it/s]\n",
      " 30%|###       | 281/935 [00:50<01:55,  5.68it/s]\n",
      " 30%|###       | 282/935 [00:51<01:54,  5.69it/s]\n",
      " 30%|###       | 283/935 [00:51<01:55,  5.66it/s]\n",
      " 30%|###       | 284/935 [00:51<01:54,  5.67it/s]\n",
      " 30%|###       | 285/935 [00:51<01:55,  5.65it/s]\n",
      " 31%|###       | 286/935 [00:51<01:54,  5.67it/s]\n",
      " 31%|###       | 287/935 [00:52<01:54,  5.64it/s]\n",
      " 31%|###       | 288/935 [00:52<01:54,  5.65it/s]\n",
      " 31%|###       | 289/935 [00:52<01:54,  5.62it/s]\n",
      " 31%|###1      | 290/935 [00:52<01:54,  5.63it/s]\n",
      " 31%|###1      | 291/935 [00:52<01:54,  5.65it/s]\n",
      " 31%|###1      | 292/935 [00:52<01:53,  5.68it/s]\n",
      " 31%|###1      | 293/935 [00:53<01:52,  5.69it/s]\n",
      " 31%|###1      | 294/935 [00:53<01:52,  5.68it/s]\n",
      " 32%|###1      | 295/935 [00:53<01:52,  5.69it/s]\n",
      " 32%|###1      | 296/935 [00:53<01:52,  5.69it/s]\n",
      " 32%|###1      | 297/935 [00:53<01:51,  5.70it/s]\n",
      " 32%|###1      | 298/935 [00:53<01:52,  5.68it/s]\n",
      " 32%|###1      | 299/935 [00:54<01:51,  5.69it/s]\n",
      " 32%|###2      | 300/935 [00:54<01:51,  5.68it/s]\n",
      " 32%|###2      | 301/935 [00:54<01:51,  5.70it/s]\n",
      " 32%|###2      | 302/935 [00:54<01:51,  5.69it/s]\n",
      " 32%|###2      | 303/935 [00:54<01:50,  5.71it/s]\n",
      " 33%|###2      | 304/935 [00:55<01:51,  5.67it/s]\n",
      " 33%|###2      | 305/935 [00:55<01:50,  5.68it/s]\n",
      " 33%|###2      | 306/935 [00:55<01:50,  5.67it/s]\n",
      " 33%|###2      | 307/935 [00:55<01:50,  5.69it/s]\n",
      " 33%|###2      | 308/935 [00:55<01:51,  5.64it/s]\n",
      " 33%|###3      | 309/935 [00:55<01:50,  5.65it/s]\n",
      " 33%|###3      | 310/935 [00:56<01:51,  5.62it/s]\n",
      " 33%|###3      | 311/935 [00:56<01:51,  5.62it/s]\n",
      " 33%|###3      | 312/935 [00:56<01:50,  5.65it/s]\n",
      " 33%|###3      | 313/935 [00:56<01:49,  5.67it/s]\n",
      " 34%|###3      | 314/935 [00:56<01:49,  5.69it/s]\n",
      " 34%|###3      | 315/935 [00:56<01:49,  5.68it/s]\n",
      " 34%|###3      | 316/935 [00:57<01:48,  5.69it/s]\n",
      " 34%|###3      | 317/935 [00:57<01:48,  5.69it/s]\n",
      " 34%|###4      | 318/935 [00:57<01:48,  5.69it/s]\n",
      " 34%|###4      | 319/935 [00:57<01:48,  5.69it/s]\n",
      " 34%|###4      | 320/935 [00:57<01:47,  5.70it/s]\n",
      " 34%|###4      | 321/935 [00:58<01:48,  5.67it/s]\n",
      " 34%|###4      | 322/935 [00:58<01:47,  5.70it/s]\n",
      " 35%|###4      | 323/935 [00:58<01:47,  5.69it/s]\n",
      " 35%|###4      | 324/935 [00:58<01:47,  5.71it/s]\n",
      " 35%|###4      | 325/935 [00:58<01:47,  5.68it/s]\n",
      " 35%|###4      | 326/935 [00:58<01:47,  5.68it/s]\n",
      " 35%|###4      | 327/935 [00:59<01:47,  5.64it/s]\n",
      " 35%|###5      | 328/935 [00:59<01:47,  5.67it/s]\n",
      " 35%|###5      | 329/935 [00:59<01:46,  5.67it/s]\n",
      " 35%|###5      | 330/935 [00:59<01:46,  5.68it/s]\n",
      " 35%|###5      | 331/935 [00:59<01:47,  5.62it/s]\n",
      " 36%|###5      | 332/935 [00:59<01:46,  5.64it/s]\n",
      " 36%|###5      | 333/935 [01:00<01:47,  5.62it/s]\n",
      " 36%|###5      | 334/935 [01:00<01:46,  5.64it/s]\n",
      " 36%|###5      | 335/935 [01:00<01:45,  5.67it/s]\n",
      " 36%|###5      | 336/935 [01:00<01:45,  5.68it/s]\n",
      " 36%|###6      | 337/935 [01:00<01:44,  5.70it/s]\n",
      " 36%|###6      | 338/935 [01:01<01:45,  5.68it/s]\n",
      " 36%|###6      | 339/935 [01:01<01:44,  5.69it/s]\n",
      " 36%|###6      | 340/935 [01:01<01:44,  5.69it/s]\n",
      " 36%|###6      | 341/935 [01:01<01:44,  5.71it/s]\n",
      " 37%|###6      | 342/935 [01:01<01:44,  5.67it/s]\n",
      " 37%|###6      | 343/935 [01:01<01:43,  5.70it/s]\n",
      " 37%|###6      | 344/935 [01:02<01:43,  5.69it/s]\n",
      " 37%|###6      | 345/935 [01:02<01:43,  5.71it/s]\n",
      " 37%|###7      | 346/935 [01:02<01:43,  5.68it/s]\n",
      " 37%|###7      | 347/935 [01:02<01:43,  5.68it/s]\n",
      " 37%|###7      | 348/935 [01:02<01:43,  5.64it/s]\n",
      " 37%|###7      | 349/935 [01:02<01:43,  5.67it/s]\n",
      " 37%|###7      | 350/935 [01:03<01:43,  5.66it/s]\n",
      " 38%|###7      | 351/935 [01:03<01:42,  5.67it/s]\n",
      " 38%|###7      | 352/935 [01:03<01:43,  5.62it/s]\n",
      " 38%|###7      | 353/935 [01:03<01:43,  5.64it/s]\n",
      " 38%|###7      | 354/935 [01:03<01:43,  5.64it/s]\n",
      " 38%|###7      | 355/935 [01:04<01:42,  5.64it/s]\n",
      " 38%|###8      | 356/935 [01:04<01:42,  5.67it/s]\n",
      " 38%|###8      | 357/935 [01:04<01:41,  5.69it/s]\n",
      " 38%|###8      | 358/935 [01:04<01:41,  5.70it/s]\n",
      " 38%|###8      | 359/935 [01:04<01:41,  5.69it/s]\n",
      " 39%|###8      | 360/935 [01:04<01:40,  5.70it/s]\n",
      " 39%|###8      | 361/935 [01:05<01:40,  5.71it/s]\n",
      " 39%|###8      | 362/935 [01:05<01:40,  5.70it/s]\n",
      " 39%|###8      | 363/935 [01:05<01:40,  5.68it/s]\n",
      " 39%|###8      | 364/935 [01:05<01:40,  5.69it/s]\n",
      " 39%|###9      | 365/935 [01:05<01:40,  5.68it/s]\n",
      " 39%|###9      | 366/935 [01:05<01:39,  5.71it/s]\n",
      " 39%|###9      | 367/935 [01:06<01:39,  5.68it/s]\n",
      " 39%|###9      | 368/935 [01:06<01:39,  5.69it/s]\n",
      " 39%|###9      | 369/935 [01:06<01:39,  5.67it/s]\n",
      " 40%|###9      | 370/935 [01:06<01:39,  5.68it/s]\n",
      " 40%|###9      | 371/935 [01:06<01:39,  5.65it/s]\n",
      " 40%|###9      | 372/935 [01:07<01:39,  5.68it/s]\n",
      " 40%|###9      | 373/935 [01:07<01:39,  5.64it/s]\n",
      " 40%|####      | 375/935 [01:07<01:27,  6.40it/s]\n",
      " 40%|####      | 376/935 [01:07<01:29,  6.22it/s]\n",
      " 40%|####      | 377/935 [01:07<01:32,  6.03it/s]\n",
      " 40%|####      | 378/935 [01:08<01:33,  5.94it/s]\n",
      " 41%|####      | 379/935 [01:08<01:35,  5.85it/s]\n",
      " 41%|####      | 380/935 [01:08<01:35,  5.82it/s]\n",
      " 41%|####      | 381/935 [01:08<01:35,  5.78it/s]\n",
      " 41%|####      | 382/935 [01:08<01:36,  5.75it/s]\n",
      " 41%|####      | 383/935 [01:08<01:36,  5.70it/s]\n",
      " 41%|####1     | 384/935 [01:09<01:36,  5.70it/s]\n",
      " 41%|####1     | 385/935 [01:09<01:37,  5.67it/s]\n",
      " 41%|####1     | 386/935 [01:09<01:36,  5.68it/s]\n",
      " 41%|####1     | 387/935 [01:09<01:36,  5.66it/s]\n",
      " 41%|####1     | 388/935 [01:09<01:36,  5.69it/s]\n",
      " 42%|####1     | 389/935 [01:09<01:36,  5.65it/s]\n",
      " 42%|####1     | 390/935 [01:10<01:36,  5.64it/s]\n",
      " 42%|####1     | 391/935 [01:10<01:35,  5.67it/s]\n",
      " 42%|####1     | 392/935 [01:10<01:35,  5.66it/s]\n",
      " 42%|####2     | 393/935 [01:10<01:35,  5.69it/s]\n",
      " 42%|####2     | 394/935 [01:10<01:35,  5.68it/s]\n",
      " 42%|####2     | 395/935 [01:11<01:34,  5.70it/s]\n",
      " 42%|####2     | 396/935 [01:11<01:34,  5.68it/s]\n",
      " 42%|####2     | 397/935 [01:11<01:34,  5.70it/s]\n",
      " 43%|####2     | 398/935 [01:11<01:34,  5.70it/s]\n",
      " 43%|####2     | 399/935 [01:11<01:34,  5.70it/s]\n",
      " 43%|####2     | 400/935 [01:11<01:34,  5.68it/s]\n",
      " 43%|####2     | 401/935 [01:12<01:33,  5.69it/s]\n",
      " 43%|####2     | 402/935 [01:12<01:33,  5.68it/s]\n",
      " 43%|####3     | 403/935 [01:12<01:33,  5.69it/s]\n",
      " 43%|####3     | 404/935 [01:12<01:33,  5.66it/s]\n",
      " 43%|####3     | 405/935 [01:12<01:33,  5.66it/s]\n",
      " 43%|####3     | 406/935 [01:12<01:33,  5.63it/s]\n",
      " 44%|####3     | 407/935 [01:13<01:33,  5.64it/s]\n",
      " 44%|####3     | 408/935 [01:13<01:33,  5.62it/s]\n",
      " 44%|####3     | 409/935 [01:13<01:33,  5.65it/s]\n",
      " 44%|####3     | 410/935 [01:13<01:32,  5.67it/s]\n",
      " 44%|####3     | 411/935 [01:13<01:32,  5.67it/s]\n",
      " 44%|####4     | 412/935 [01:13<01:31,  5.69it/s]\n",
      " 44%|####4     | 413/935 [01:14<01:31,  5.68it/s]\n",
      " 44%|####4     | 414/935 [01:14<01:31,  5.69it/s]\n",
      " 44%|####4     | 415/935 [01:14<01:31,  5.70it/s]\n",
      " 44%|####4     | 416/935 [01:14<01:31,  5.69it/s]\n",
      " 45%|####4     | 417/935 [01:14<01:31,  5.66it/s]\n",
      " 45%|####4     | 418/935 [01:15<01:30,  5.69it/s]\n",
      " 45%|####4     | 419/935 [01:15<01:30,  5.68it/s]\n",
      " 45%|####4     | 420/935 [01:15<01:30,  5.71it/s]\n",
      " 45%|####5     | 421/935 [01:15<01:30,  5.67it/s]\n",
      " 45%|####5     | 422/935 [01:15<01:30,  5.68it/s]\n",
      " 45%|####5     | 423/935 [01:15<01:30,  5.64it/s]\n",
      " 45%|####5     | 424/935 [01:16<01:30,  5.66it/s]\n",
      " 45%|####5     | 425/935 [01:16<01:30,  5.66it/s]\n",
      " 46%|####5     | 426/935 [01:16<01:29,  5.67it/s]\n",
      " 46%|####5     | 427/935 [01:16<01:29,  5.65it/s]\n",
      " 46%|####5     | 428/935 [01:16<01:29,  5.66it/s]\n",
      " 46%|####5     | 429/935 [01:17<01:29,  5.63it/s]\n",
      " 46%|####5     | 430/935 [01:17<01:29,  5.63it/s]\n",
      " 46%|####6     | 431/935 [01:17<01:29,  5.62it/s]\n",
      " 46%|####6     | 432/935 [01:17<01:29,  5.65it/s]\n",
      " 46%|####6     | 433/935 [01:17<01:28,  5.65it/s]\n",
      " 46%|####6     | 434/935 [01:17<01:28,  5.68it/s]\n",
      " 47%|####6     | 435/935 [01:18<01:27,  5.70it/s]\n",
      " 47%|####6     | 436/935 [01:18<01:27,  5.70it/s]\n",
      " 47%|####6     | 437/935 [01:18<01:27,  5.70it/s]\n",
      " 47%|####6     | 438/935 [01:18<01:27,  5.71it/s]\n",
      " 47%|####6     | 439/935 [01:18<01:26,  5.71it/s]\n",
      " 47%|####7     | 440/935 [01:18<01:26,  5.69it/s]\n",
      " 47%|####7     | 441/935 [01:19<01:26,  5.71it/s]\n",
      " 47%|####7     | 442/935 [01:19<01:26,  5.70it/s]\n",
      " 47%|####7     | 443/935 [01:19<01:26,  5.69it/s]\n",
      " 47%|####7     | 444/935 [01:19<01:26,  5.66it/s]\n",
      " 48%|####7     | 445/935 [01:19<01:25,  5.71it/s]\n",
      " 48%|####7     | 446/935 [01:19<01:25,  5.70it/s]\n",
      " 48%|####7     | 447/935 [01:20<01:25,  5.69it/s]\n",
      " 48%|####7     | 448/935 [01:20<01:26,  5.65it/s]\n",
      " 48%|####8     | 449/935 [01:20<01:25,  5.66it/s]\n",
      " 48%|####8     | 450/935 [01:20<01:26,  5.62it/s]\n",
      " 48%|####8     | 451/935 [01:20<01:25,  5.64it/s]\n",
      " 48%|####8     | 452/935 [01:21<01:26,  5.61it/s]\n",
      " 48%|####8     | 453/935 [01:21<01:25,  5.65it/s]\n",
      " 49%|####8     | 454/935 [01:21<01:24,  5.68it/s]\n",
      " 49%|####8     | 455/935 [01:21<01:24,  5.67it/s]\n",
      " 49%|####8     | 456/935 [01:21<01:24,  5.69it/s]\n",
      " 49%|####8     | 457/935 [01:21<01:24,  5.69it/s]\n",
      " 49%|####8     | 458/935 [01:22<01:23,  5.69it/s]\n",
      " 49%|####9     | 459/935 [01:22<01:23,  5.68it/s]\n",
      " 49%|####9     | 460/935 [01:22<01:23,  5.69it/s]\n",
      " 49%|####9     | 461/935 [01:22<01:23,  5.67it/s]\n",
      " 49%|####9     | 462/935 [01:22<01:23,  5.69it/s]\n",
      " 50%|####9     | 463/935 [01:22<01:22,  5.70it/s]\n",
      " 50%|####9     | 464/935 [01:23<01:22,  5.71it/s]\n",
      " 50%|####9     | 465/935 [01:23<01:22,  5.67it/s]\n",
      " 50%|####9     | 466/935 [01:23<01:22,  5.68it/s]\n",
      " 50%|####9     | 467/935 [01:23<01:22,  5.65it/s]\n",
      " 50%|#####     | 468/935 [01:23<01:22,  5.68it/s]\n",
      " 50%|#####     | 469/935 [01:24<01:22,  5.68it/s]\n",
      " 50%|#####     | 470/935 [01:24<01:21,  5.69it/s]\n",
      " 50%|#####     | 471/935 [01:24<01:22,  5.65it/s]\n",
      " 50%|#####     | 472/935 [01:24<01:21,  5.66it/s]\n",
      " 51%|#####     | 473/935 [01:24<01:21,  5.65it/s]\n",
      " 51%|#####     | 474/935 [01:24<01:21,  5.68it/s]\n",
      " 51%|#####     | 475/935 [01:25<01:21,  5.65it/s]\n",
      " 51%|#####     | 476/935 [01:25<01:20,  5.67it/s]\n",
      " 51%|#####1    | 477/935 [01:25<01:21,  5.63it/s]\n",
      " 51%|#####1    | 478/935 [01:25<01:21,  5.63it/s]\n",
      " 51%|#####1    | 479/935 [01:25<01:20,  5.67it/s]\n",
      " 51%|#####1    | 480/935 [01:25<01:20,  5.67it/s]\n",
      " 51%|#####1    | 481/935 [01:26<01:19,  5.70it/s]\n",
      " 52%|#####1    | 482/935 [01:26<01:19,  5.67it/s]\n",
      " 52%|#####1    | 483/935 [01:26<01:19,  5.68it/s]\n",
      " 52%|#####1    | 484/935 [01:26<01:19,  5.65it/s]\n",
      " 52%|#####1    | 485/935 [01:26<01:19,  5.69it/s]\n",
      " 52%|#####1    | 486/935 [01:27<01:18,  5.69it/s]\n",
      " 52%|#####2    | 487/935 [01:27<01:18,  5.70it/s]\n",
      " 52%|#####2    | 488/935 [01:27<01:18,  5.67it/s]\n",
      " 52%|#####2    | 489/935 [01:27<01:18,  5.69it/s]\n",
      " 52%|#####2    | 490/935 [01:27<01:18,  5.67it/s]\n",
      " 53%|#####2    | 491/935 [01:27<01:17,  5.70it/s]\n",
      " 53%|#####2    | 492/935 [01:28<01:18,  5.65it/s]\n",
      " 53%|#####2    | 493/935 [01:28<01:17,  5.67it/s]\n",
      " 53%|#####2    | 494/935 [01:28<01:18,  5.64it/s]\n",
      " 53%|#####2    | 495/935 [01:28<01:18,  5.64it/s]\n",
      " 53%|#####3    | 496/935 [01:28<01:18,  5.62it/s]\n",
      " 53%|#####3    | 497/935 [01:28<01:17,  5.65it/s]\n",
      " 53%|#####3    | 498/935 [01:29<01:17,  5.67it/s]\n",
      " 53%|#####3    | 499/935 [01:29<01:16,  5.66it/s]\n",
      " 53%|#####3    | 500/935 [01:29<01:16,  5.68it/s]\n",
      "                                                 \n",
      "\n",
      " 53%|#####3    | 500/935 [01:29<01:16,  5.68it/s]\n",
      " 54%|#####3    | 501/935 [01:29<01:17,  5.61it/s]\n",
      " 54%|#####3    | 502/935 [01:29<01:16,  5.63it/s]\n",
      " 54%|#####3    | 503/935 [01:30<01:16,  5.64it/s]\n",
      " 54%|#####3    | 504/935 [01:30<01:16,  5.67it/s]\n",
      " 54%|#####4    | 505/935 [01:30<01:15,  5.67it/s]\n",
      " 54%|#####4    | 506/935 [01:30<01:15,  5.70it/s]\n",
      " 54%|#####4    | 507/935 [01:30<01:15,  5.68it/s]\n",
      " 54%|#####4    | 508/935 [01:30<01:14,  5.70it/s]\n",
      " 54%|#####4    | 509/935 [01:31<01:15,  5.68it/s]\n",
      " 55%|#####4    | 510/935 [01:31<01:14,  5.71it/s]\n",
      " 55%|#####4    | 511/935 [01:31<01:14,  5.67it/s]\n",
      " 55%|#####4    | 512/935 [01:31<01:14,  5.68it/s]\n",
      " 55%|#####4    | 513/935 [01:31<01:14,  5.66it/s]\n",
      " 55%|#####4    | 514/935 [01:31<01:14,  5.69it/s]\n",
      " 55%|#####5    | 515/935 [01:32<01:14,  5.67it/s]\n",
      " 55%|#####5    | 516/935 [01:32<01:14,  5.66it/s]\n",
      " 55%|#####5    | 517/935 [01:32<01:14,  5.62it/s]\n",
      " 55%|#####5    | 518/935 [01:32<01:14,  5.62it/s]\n",
      " 56%|#####5    | 519/935 [01:32<01:14,  5.60it/s]\n",
      " 56%|#####5    | 520/935 [01:33<01:13,  5.61it/s]\n",
      " 56%|#####5    | 521/935 [01:33<01:13,  5.65it/s]\n",
      " 56%|#####5    | 522/935 [01:33<01:12,  5.67it/s]\n",
      " 56%|#####5    | 523/935 [01:33<01:12,  5.69it/s]\n",
      " 56%|#####6    | 524/935 [01:33<01:12,  5.68it/s]\n",
      " 56%|#####6    | 525/935 [01:33<01:11,  5.70it/s]\n",
      " 56%|#####6    | 526/935 [01:34<01:11,  5.68it/s]\n",
      " 56%|#####6    | 527/935 [01:34<01:11,  5.70it/s]\n",
      " 56%|#####6    | 528/935 [01:34<01:11,  5.68it/s]\n",
      " 57%|#####6    | 529/935 [01:34<01:11,  5.69it/s]\n",
      " 57%|#####6    | 530/935 [01:34<01:11,  5.67it/s]\n",
      " 57%|#####6    | 531/935 [01:34<01:11,  5.67it/s]\n",
      " 57%|#####6    | 532/935 [01:35<01:10,  5.69it/s]\n",
      " 57%|#####7    | 533/935 [01:35<01:10,  5.69it/s]\n",
      " 57%|#####7    | 534/935 [01:35<01:10,  5.65it/s]\n",
      " 57%|#####7    | 535/935 [01:35<01:10,  5.65it/s]\n",
      " 57%|#####7    | 536/935 [01:35<01:10,  5.64it/s]\n",
      " 57%|#####7    | 537/935 [01:36<01:10,  5.65it/s]\n",
      " 58%|#####7    | 538/935 [01:36<01:10,  5.64it/s]\n",
      " 58%|#####7    | 539/935 [01:36<01:09,  5.67it/s]\n",
      " 58%|#####7    | 540/935 [01:36<01:09,  5.65it/s]\n",
      " 58%|#####7    | 541/935 [01:36<01:09,  5.66it/s]\n",
      " 58%|#####7    | 542/935 [01:36<01:09,  5.68it/s]\n",
      " 58%|#####8    | 543/935 [01:37<01:09,  5.66it/s]\n",
      " 58%|#####8    | 544/935 [01:37<01:08,  5.69it/s]\n",
      " 58%|#####8    | 545/935 [01:37<01:08,  5.70it/s]\n",
      " 58%|#####8    | 546/935 [01:37<01:08,  5.70it/s]\n",
      " 59%|#####8    | 547/935 [01:37<01:08,  5.67it/s]\n",
      " 59%|#####8    | 548/935 [01:37<01:08,  5.69it/s]\n",
      " 59%|#####8    | 549/935 [01:38<01:08,  5.67it/s]\n",
      " 59%|#####8    | 550/935 [01:38<01:07,  5.71it/s]\n",
      " 59%|#####8    | 551/935 [01:38<01:07,  5.66it/s]\n",
      " 59%|#####9    | 552/935 [01:38<01:07,  5.68it/s]\n",
      " 59%|#####9    | 553/935 [01:38<01:07,  5.65it/s]\n",
      " 59%|#####9    | 554/935 [01:39<01:07,  5.66it/s]\n",
      " 59%|#####9    | 555/935 [01:39<01:07,  5.66it/s]\n",
      " 59%|#####9    | 556/935 [01:39<01:06,  5.68it/s]\n",
      " 60%|#####9    | 557/935 [01:39<01:06,  5.64it/s]\n",
      " 60%|#####9    | 558/935 [01:39<01:06,  5.65it/s]\n",
      " 60%|#####9    | 559/935 [01:39<01:06,  5.62it/s]\n",
      " 60%|#####9    | 560/935 [01:40<01:06,  5.63it/s]\n",
      " 60%|######    | 562/935 [01:40<00:58,  6.39it/s]\n",
      " 60%|######    | 563/935 [01:40<01:00,  6.18it/s]\n",
      " 60%|######    | 564/935 [01:40<01:01,  6.05it/s]\n",
      " 60%|######    | 565/935 [01:40<01:02,  5.93it/s]\n",
      " 61%|######    | 566/935 [01:41<01:02,  5.87it/s]\n",
      " 61%|######    | 567/935 [01:41<01:03,  5.78it/s]\n",
      " 61%|######    | 568/935 [01:41<01:03,  5.76it/s]\n",
      " 61%|######    | 569/935 [01:41<01:04,  5.70it/s]\n",
      " 61%|######    | 570/935 [01:41<01:04,  5.68it/s]\n",
      " 61%|######1   | 571/935 [01:41<01:04,  5.66it/s]\n",
      " 61%|######1   | 572/935 [01:42<01:03,  5.68it/s]\n",
      " 61%|######1   | 573/935 [01:42<01:03,  5.68it/s]\n",
      " 61%|######1   | 574/935 [01:42<01:03,  5.68it/s]\n",
      " 61%|######1   | 575/935 [01:42<01:03,  5.68it/s]\n",
      " 62%|######1   | 576/935 [01:42<01:03,  5.67it/s]\n",
      " 62%|######1   | 577/935 [01:43<01:03,  5.67it/s]\n",
      " 62%|######1   | 578/935 [01:43<01:02,  5.70it/s]\n",
      " 62%|######1   | 579/935 [01:43<01:02,  5.71it/s]\n",
      " 62%|######2   | 580/935 [01:43<01:02,  5.68it/s]\n",
      " 62%|######2   | 581/935 [01:43<01:02,  5.71it/s]\n",
      " 62%|######2   | 582/935 [01:43<01:02,  5.69it/s]\n",
      " 62%|######2   | 583/935 [01:44<01:01,  5.70it/s]\n",
      " 62%|######2   | 584/935 [01:44<01:01,  5.67it/s]\n",
      " 63%|######2   | 585/935 [01:44<01:01,  5.68it/s]\n",
      " 63%|######2   | 586/935 [01:44<01:01,  5.65it/s]\n",
      " 63%|######2   | 587/935 [01:44<01:01,  5.67it/s]\n",
      " 63%|######2   | 588/935 [01:44<01:01,  5.66it/s]\n",
      " 63%|######2   | 589/935 [01:45<01:00,  5.67it/s]\n",
      " 63%|######3   | 590/935 [01:45<01:01,  5.64it/s]\n",
      " 63%|######3   | 591/935 [01:45<01:01,  5.63it/s]\n",
      " 63%|######3   | 592/935 [01:45<01:01,  5.58it/s]\n",
      " 63%|######3   | 593/935 [01:45<01:01,  5.59it/s]\n",
      " 64%|######3   | 594/935 [01:46<01:00,  5.59it/s]\n",
      " 64%|######3   | 595/935 [01:46<01:00,  5.61it/s]\n",
      " 64%|######3   | 596/935 [01:46<01:00,  5.65it/s]\n",
      " 64%|######3   | 597/935 [01:46<00:59,  5.67it/s]\n",
      " 64%|######3   | 598/935 [01:46<00:59,  5.70it/s]\n",
      " 64%|######4   | 599/935 [01:46<00:59,  5.67it/s]\n",
      " 64%|######4   | 600/935 [01:47<00:59,  5.67it/s]\n",
      " 64%|######4   | 601/935 [01:47<00:58,  5.67it/s]\n",
      " 64%|######4   | 602/935 [01:47<00:58,  5.69it/s]\n",
      " 64%|######4   | 603/935 [01:47<00:58,  5.70it/s]\n",
      " 65%|######4   | 604/935 [01:47<00:58,  5.70it/s]\n",
      " 65%|######4   | 605/935 [01:47<00:58,  5.68it/s]\n",
      " 65%|######4   | 606/935 [01:48<00:57,  5.70it/s]\n",
      " 65%|######4   | 607/935 [01:48<00:57,  5.69it/s]\n",
      " 65%|######5   | 608/935 [01:48<00:57,  5.69it/s]\n",
      " 65%|######5   | 609/935 [01:48<00:57,  5.66it/s]\n",
      " 65%|######5   | 610/935 [01:48<00:57,  5.65it/s]\n",
      " 65%|######5   | 611/935 [01:49<00:57,  5.64it/s]\n",
      " 65%|######5   | 612/935 [01:49<00:57,  5.65it/s]\n",
      " 66%|######5   | 613/935 [01:49<00:57,  5.63it/s]\n",
      " 66%|######5   | 614/935 [01:49<00:56,  5.65it/s]\n",
      " 66%|######5   | 615/935 [01:49<00:56,  5.66it/s]\n",
      " 66%|######5   | 616/935 [01:49<00:56,  5.65it/s]\n",
      " 66%|######5   | 617/935 [01:50<00:56,  5.67it/s]\n",
      " 66%|######6   | 618/935 [01:50<00:55,  5.66it/s]\n",
      " 66%|######6   | 619/935 [01:50<00:55,  5.68it/s]\n",
      " 66%|######6   | 620/935 [01:50<00:55,  5.69it/s]\n",
      " 66%|######6   | 621/935 [01:50<00:55,  5.70it/s]\n",
      " 67%|######6   | 622/935 [01:50<00:55,  5.68it/s]\n",
      " 67%|######6   | 623/935 [01:51<00:54,  5.68it/s]\n",
      " 67%|######6   | 624/935 [01:51<00:54,  5.69it/s]\n",
      " 67%|######6   | 625/935 [01:51<00:54,  5.70it/s]\n",
      " 67%|######6   | 626/935 [01:51<00:54,  5.65it/s]\n",
      " 67%|######7   | 627/935 [01:51<00:54,  5.68it/s]\n",
      " 67%|######7   | 628/935 [01:52<00:54,  5.65it/s]\n",
      " 67%|######7   | 629/935 [01:52<00:53,  5.68it/s]\n",
      " 67%|######7   | 630/935 [01:52<00:53,  5.66it/s]\n",
      " 67%|######7   | 631/935 [01:52<00:53,  5.68it/s]\n",
      " 68%|######7   | 632/935 [01:52<00:53,  5.65it/s]\n",
      " 68%|######7   | 633/935 [01:52<00:53,  5.66it/s]\n",
      " 68%|######7   | 634/935 [01:53<00:53,  5.63it/s]\n",
      " 68%|######7   | 635/935 [01:53<00:53,  5.64it/s]\n",
      " 68%|######8   | 636/935 [01:53<00:52,  5.64it/s]\n",
      " 68%|######8   | 637/935 [01:53<00:52,  5.67it/s]\n",
      " 68%|######8   | 638/935 [01:53<00:52,  5.65it/s]\n",
      " 68%|######8   | 639/935 [01:53<00:52,  5.65it/s]\n",
      " 68%|######8   | 640/935 [01:54<00:52,  5.66it/s]\n",
      " 69%|######8   | 641/935 [01:54<00:51,  5.67it/s]\n",
      " 69%|######8   | 642/935 [01:54<00:51,  5.69it/s]\n",
      " 69%|######8   | 643/935 [01:54<00:51,  5.69it/s]\n",
      " 69%|######8   | 644/935 [01:54<00:51,  5.70it/s]\n",
      " 69%|######8   | 645/935 [01:55<00:51,  5.67it/s]\n",
      " 69%|######9   | 646/935 [01:55<00:50,  5.70it/s]\n",
      " 69%|######9   | 647/935 [01:55<00:50,  5.71it/s]\n",
      " 69%|######9   | 648/935 [01:55<00:50,  5.70it/s]\n",
      " 69%|######9   | 649/935 [01:55<00:50,  5.68it/s]\n",
      " 70%|######9   | 650/935 [01:55<00:50,  5.68it/s]\n",
      " 70%|######9   | 651/935 [01:56<00:50,  5.67it/s]\n",
      " 70%|######9   | 652/935 [01:56<00:49,  5.68it/s]\n",
      " 70%|######9   | 653/935 [01:56<00:49,  5.66it/s]\n",
      " 70%|######9   | 654/935 [01:56<00:49,  5.68it/s]\n",
      " 70%|#######   | 655/935 [01:56<00:49,  5.65it/s]\n",
      " 70%|#######   | 656/935 [01:56<00:49,  5.65it/s]\n",
      " 70%|#######   | 657/935 [01:57<00:49,  5.64it/s]\n",
      " 70%|#######   | 658/935 [01:57<00:48,  5.67it/s]\n",
      " 70%|#######   | 659/935 [01:57<00:48,  5.64it/s]\n",
      " 71%|#######   | 660/935 [01:57<00:48,  5.65it/s]\n",
      " 71%|#######   | 661/935 [01:57<00:48,  5.67it/s]\n",
      " 71%|#######   | 662/935 [01:58<00:48,  5.67it/s]\n",
      " 71%|#######   | 663/935 [01:58<00:47,  5.68it/s]\n",
      " 71%|#######1  | 664/935 [01:58<00:47,  5.67it/s]\n",
      " 71%|#######1  | 665/935 [01:58<00:47,  5.70it/s]\n",
      " 71%|#######1  | 666/935 [01:58<00:47,  5.69it/s]\n",
      " 71%|#######1  | 667/935 [01:58<00:47,  5.68it/s]\n",
      " 71%|#######1  | 668/935 [01:59<00:46,  5.68it/s]\n",
      " 72%|#######1  | 669/935 [01:59<00:46,  5.69it/s]\n",
      " 72%|#######1  | 670/935 [01:59<00:46,  5.68it/s]\n",
      " 72%|#######1  | 671/935 [01:59<00:46,  5.68it/s]\n",
      " 72%|#######1  | 672/935 [01:59<00:46,  5.67it/s]\n",
      " 72%|#######1  | 673/935 [01:59<00:46,  5.69it/s]\n",
      " 72%|#######2  | 674/935 [02:00<00:46,  5.67it/s]\n",
      " 72%|#######2  | 675/935 [02:00<00:45,  5.68it/s]\n",
      " 72%|#######2  | 676/935 [02:00<00:46,  5.61it/s]\n",
      " 72%|#######2  | 677/935 [02:00<00:45,  5.64it/s]\n",
      " 73%|#######2  | 678/935 [02:00<00:45,  5.60it/s]\n",
      " 73%|#######2  | 679/935 [02:01<00:45,  5.62it/s]\n",
      " 73%|#######2  | 680/935 [02:01<00:45,  5.61it/s]\n",
      " 73%|#######2  | 681/935 [02:01<00:45,  5.63it/s]\n",
      " 73%|#######2  | 682/935 [02:01<00:44,  5.64it/s]\n",
      " 73%|#######3  | 683/935 [02:01<00:44,  5.66it/s]\n",
      " 73%|#######3  | 684/935 [02:01<00:44,  5.68it/s]\n",
      " 73%|#######3  | 685/935 [02:02<00:44,  5.66it/s]\n",
      " 73%|#######3  | 686/935 [02:02<00:43,  5.68it/s]\n",
      " 73%|#######3  | 687/935 [02:02<00:43,  5.69it/s]\n",
      " 74%|#######3  | 688/935 [02:02<00:43,  5.72it/s]\n",
      " 74%|#######3  | 689/935 [02:02<00:43,  5.68it/s]\n",
      " 74%|#######3  | 690/935 [02:02<00:43,  5.69it/s]\n",
      " 74%|#######3  | 691/935 [02:03<00:43,  5.66it/s]\n",
      " 74%|#######4  | 692/935 [02:03<00:42,  5.67it/s]\n",
      " 74%|#######4  | 693/935 [02:03<00:42,  5.65it/s]\n",
      " 74%|#######4  | 694/935 [02:03<00:42,  5.67it/s]\n",
      " 74%|#######4  | 695/935 [02:03<00:42,  5.65it/s]\n",
      " 74%|#######4  | 696/935 [02:04<00:42,  5.66it/s]\n",
      " 75%|#######4  | 697/935 [02:04<00:42,  5.64it/s]\n",
      " 75%|#######4  | 698/935 [02:04<00:41,  5.66it/s]\n",
      " 75%|#######4  | 699/935 [02:04<00:41,  5.65it/s]\n",
      " 75%|#######4  | 700/935 [02:04<00:41,  5.66it/s]\n",
      " 75%|#######4  | 701/935 [02:04<00:41,  5.64it/s]\n",
      " 75%|#######5  | 702/935 [02:05<00:41,  5.63it/s]\n",
      " 75%|#######5  | 703/935 [02:05<00:41,  5.63it/s]\n",
      " 75%|#######5  | 704/935 [02:05<00:41,  5.63it/s]\n",
      " 75%|#######5  | 705/935 [02:05<00:40,  5.67it/s]\n",
      " 76%|#######5  | 706/935 [02:05<00:40,  5.67it/s]\n",
      " 76%|#######5  | 707/935 [02:05<00:40,  5.69it/s]\n",
      " 76%|#######5  | 708/935 [02:06<00:40,  5.67it/s]\n",
      " 76%|#######5  | 709/935 [02:06<00:39,  5.67it/s]\n",
      " 76%|#######5  | 710/935 [02:06<00:39,  5.66it/s]\n",
      " 76%|#######6  | 711/935 [02:06<00:39,  5.69it/s]\n",
      " 76%|#######6  | 712/935 [02:06<00:39,  5.68it/s]\n",
      " 76%|#######6  | 713/935 [02:07<00:38,  5.69it/s]\n",
      " 76%|#######6  | 714/935 [02:07<00:38,  5.68it/s]\n",
      " 76%|#######6  | 715/935 [02:07<00:38,  5.68it/s]\n",
      " 77%|#######6  | 716/935 [02:07<00:38,  5.67it/s]\n",
      " 77%|#######6  | 717/935 [02:07<00:38,  5.68it/s]\n",
      " 77%|#######6  | 718/935 [02:07<00:38,  5.66it/s]\n",
      " 77%|#######6  | 719/935 [02:08<00:38,  5.64it/s]\n",
      " 77%|#######7  | 720/935 [02:08<00:38,  5.61it/s]\n",
      " 77%|#######7  | 721/935 [02:08<00:38,  5.63it/s]\n",
      " 77%|#######7  | 722/935 [02:08<00:37,  5.63it/s]\n",
      " 77%|#######7  | 723/935 [02:08<00:37,  5.62it/s]\n",
      " 77%|#######7  | 724/935 [02:08<00:37,  5.66it/s]\n",
      " 78%|#######7  | 725/935 [02:09<00:37,  5.68it/s]\n",
      " 78%|#######7  | 726/935 [02:09<00:36,  5.68it/s]\n",
      " 78%|#######7  | 727/935 [02:09<00:36,  5.68it/s]\n",
      " 78%|#######7  | 728/935 [02:09<00:36,  5.67it/s]\n",
      " 78%|#######7  | 729/935 [02:09<00:36,  5.66it/s]\n",
      " 78%|#######8  | 730/935 [02:10<00:36,  5.68it/s]\n",
      " 78%|#######8  | 731/935 [02:10<00:35,  5.68it/s]\n",
      " 78%|#######8  | 732/935 [02:10<00:35,  5.69it/s]\n",
      " 78%|#######8  | 733/935 [02:10<00:35,  5.66it/s]\n",
      " 79%|#######8  | 734/935 [02:10<00:35,  5.69it/s]\n",
      " 79%|#######8  | 735/935 [02:10<00:35,  5.68it/s]\n",
      " 79%|#######8  | 736/935 [02:11<00:34,  5.69it/s]\n",
      " 79%|#######8  | 737/935 [02:11<00:35,  5.65it/s]\n",
      " 79%|#######8  | 738/935 [02:11<00:34,  5.67it/s]\n",
      " 79%|#######9  | 739/935 [02:11<00:34,  5.64it/s]\n",
      " 79%|#######9  | 740/935 [02:11<00:34,  5.65it/s]\n",
      " 79%|#######9  | 741/935 [02:11<00:34,  5.64it/s]\n",
      " 79%|#######9  | 742/935 [02:12<00:34,  5.64it/s]\n",
      " 79%|#######9  | 743/935 [02:12<00:33,  5.66it/s]\n",
      " 80%|#######9  | 744/935 [02:12<00:33,  5.66it/s]\n",
      " 80%|#######9  | 745/935 [02:12<00:33,  5.62it/s]\n",
      " 80%|#######9  | 746/935 [02:12<00:33,  5.63it/s]\n",
      " 80%|#######9  | 747/935 [02:13<00:33,  5.65it/s]\n",
      " 80%|########  | 749/935 [02:13<00:29,  6.36it/s]\n",
      " 80%|########  | 750/935 [02:13<00:29,  6.18it/s]\n",
      " 80%|########  | 751/935 [02:13<00:30,  6.00it/s]\n",
      " 80%|########  | 752/935 [02:13<00:31,  5.90it/s]\n",
      " 81%|########  | 753/935 [02:14<00:31,  5.82it/s]\n",
      " 81%|########  | 754/935 [02:14<00:31,  5.79it/s]\n",
      " 81%|########  | 755/935 [02:14<00:31,  5.75it/s]\n",
      " 81%|########  | 756/935 [02:14<00:31,  5.73it/s]\n",
      " 81%|########  | 757/935 [02:14<00:31,  5.72it/s]\n",
      " 81%|########1 | 758/935 [02:14<00:31,  5.69it/s]\n",
      " 81%|########1 | 759/935 [02:15<00:30,  5.68it/s]\n",
      " 81%|########1 | 760/935 [02:15<00:30,  5.68it/s]\n",
      " 81%|########1 | 761/935 [02:15<00:30,  5.71it/s]\n",
      " 81%|########1 | 762/935 [02:15<00:30,  5.68it/s]\n",
      " 82%|########1 | 763/935 [02:15<00:30,  5.69it/s]\n",
      " 82%|########1 | 764/935 [02:15<00:30,  5.67it/s]\n",
      " 82%|########1 | 765/935 [02:16<00:29,  5.69it/s]\n",
      " 82%|########1 | 766/935 [02:16<00:29,  5.68it/s]\n",
      " 82%|########2 | 767/935 [02:16<00:29,  5.68it/s]\n",
      " 82%|########2 | 768/935 [02:16<00:29,  5.65it/s]\n",
      " 82%|########2 | 769/935 [02:16<00:29,  5.66it/s]\n",
      " 82%|########2 | 770/935 [02:17<00:29,  5.64it/s]\n",
      " 82%|########2 | 771/935 [02:17<00:28,  5.67it/s]\n",
      " 83%|########2 | 772/935 [02:17<00:28,  5.65it/s]\n",
      " 83%|########2 | 773/935 [02:17<00:28,  5.65it/s]\n",
      " 83%|########2 | 774/935 [02:17<00:28,  5.60it/s]\n",
      " 83%|########2 | 775/935 [02:17<00:28,  5.62it/s]\n",
      " 83%|########2 | 776/935 [02:18<00:28,  5.63it/s]\n",
      " 83%|########3 | 777/935 [02:18<00:28,  5.63it/s]\n",
      " 83%|########3 | 778/935 [02:18<00:27,  5.66it/s]\n",
      " 83%|########3 | 779/935 [02:18<00:27,  5.68it/s]\n",
      " 83%|########3 | 780/935 [02:18<00:27,  5.68it/s]\n",
      " 84%|########3 | 781/935 [02:18<00:27,  5.67it/s]\n",
      " 84%|########3 | 782/935 [02:19<00:26,  5.67it/s]\n",
      " 84%|########3 | 783/935 [02:19<00:26,  5.66it/s]\n",
      " 84%|########3 | 784/935 [02:19<00:26,  5.68it/s]\n",
      " 84%|########3 | 785/935 [02:19<00:26,  5.67it/s]\n",
      " 84%|########4 | 786/935 [02:19<00:26,  5.69it/s]\n",
      " 84%|########4 | 787/935 [02:20<00:26,  5.67it/s]\n",
      " 84%|########4 | 788/935 [02:20<00:25,  5.69it/s]\n",
      " 84%|########4 | 789/935 [02:20<00:25,  5.67it/s]\n",
      " 84%|########4 | 790/935 [02:20<00:25,  5.69it/s]\n",
      " 85%|########4 | 791/935 [02:20<00:25,  5.66it/s]\n",
      " 85%|########4 | 792/935 [02:20<00:25,  5.66it/s]\n",
      " 85%|########4 | 793/935 [02:21<00:25,  5.62it/s]\n",
      " 85%|########4 | 794/935 [02:21<00:25,  5.63it/s]\n",
      " 85%|########5 | 795/935 [02:21<00:24,  5.63it/s]\n",
      " 85%|########5 | 796/935 [02:21<00:24,  5.64it/s]\n",
      " 85%|########5 | 797/935 [02:21<00:24,  5.68it/s]\n",
      " 85%|########5 | 798/935 [02:21<00:24,  5.68it/s]\n",
      " 85%|########5 | 799/935 [02:22<00:23,  5.69it/s]\n",
      " 86%|########5 | 800/935 [02:22<00:23,  5.67it/s]\n",
      " 86%|########5 | 801/935 [02:22<00:23,  5.68it/s]\n",
      " 86%|########5 | 802/935 [02:22<00:23,  5.69it/s]\n",
      " 86%|########5 | 803/935 [02:22<00:23,  5.69it/s]\n",
      " 86%|########5 | 804/935 [02:23<00:23,  5.67it/s]\n",
      " 86%|########6 | 805/935 [02:23<00:22,  5.67it/s]\n",
      " 86%|########6 | 806/935 [02:23<00:22,  5.65it/s]\n",
      " 86%|########6 | 807/935 [02:23<00:22,  5.67it/s]\n",
      " 86%|########6 | 808/935 [02:23<00:22,  5.68it/s]\n",
      " 87%|########6 | 809/935 [02:23<00:22,  5.69it/s]\n",
      " 87%|########6 | 810/935 [02:24<00:22,  5.64it/s]\n",
      " 87%|########6 | 811/935 [02:24<00:21,  5.66it/s]\n",
      " 87%|########6 | 812/935 [02:24<00:21,  5.64it/s]\n",
      " 87%|########6 | 813/935 [02:24<00:21,  5.67it/s]\n",
      " 87%|########7 | 814/935 [02:24<00:21,  5.65it/s]\n",
      " 87%|########7 | 815/935 [02:24<00:21,  5.67it/s]\n",
      " 87%|########7 | 816/935 [02:25<00:21,  5.60it/s]\n",
      " 87%|########7 | 817/935 [02:25<00:20,  5.64it/s]\n",
      " 87%|########7 | 818/935 [02:25<00:20,  5.60it/s]\n",
      " 88%|########7 | 819/935 [02:25<00:20,  5.61it/s]\n",
      " 88%|########7 | 820/935 [02:25<00:20,  5.64it/s]\n",
      " 88%|########7 | 821/935 [02:26<00:20,  5.64it/s]\n",
      " 88%|########7 | 822/935 [02:26<00:19,  5.67it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|########8 | 823/935 [02:26<00:19,  5.67it/s]\n",
      " 88%|########8 | 824/935 [02:26<00:19,  5.68it/s]\n",
      " 88%|########8 | 825/935 [02:26<00:19,  5.67it/s]\n",
      " 88%|########8 | 826/935 [02:26<00:19,  5.70it/s]\n",
      " 88%|########8 | 827/935 [02:27<00:19,  5.68it/s]\n",
      " 89%|########8 | 828/935 [02:27<00:18,  5.69it/s]\n",
      " 89%|########8 | 829/935 [02:27<00:18,  5.66it/s]\n",
      " 89%|########8 | 830/935 [02:27<00:18,  5.66it/s]\n",
      " 89%|########8 | 831/935 [02:27<00:18,  5.64it/s]\n",
      " 89%|########8 | 832/935 [02:27<00:18,  5.66it/s]\n",
      " 89%|########9 | 833/935 [02:28<00:18,  5.64it/s]\n",
      " 89%|########9 | 834/935 [02:28<00:17,  5.67it/s]\n",
      " 89%|########9 | 835/935 [02:28<00:17,  5.63it/s]\n",
      " 89%|########9 | 836/935 [02:28<00:17,  5.64it/s]\n",
      " 90%|########9 | 837/935 [02:28<00:17,  5.61it/s]\n",
      " 90%|########9 | 838/935 [02:29<00:17,  5.63it/s]\n",
      " 90%|########9 | 839/935 [02:29<00:16,  5.66it/s]\n",
      " 90%|########9 | 840/935 [02:29<00:16,  5.68it/s]\n",
      " 90%|########9 | 841/935 [02:29<00:16,  5.69it/s]\n",
      " 90%|######### | 842/935 [02:29<00:16,  5.68it/s]\n",
      " 90%|######### | 843/935 [02:29<00:16,  5.69it/s]\n",
      " 90%|######### | 844/935 [02:30<00:16,  5.66it/s]\n",
      " 90%|######### | 845/935 [02:30<00:15,  5.67it/s]\n",
      " 90%|######### | 846/935 [02:30<00:15,  5.68it/s]\n",
      " 91%|######### | 847/935 [02:30<00:15,  5.70it/s]\n",
      " 91%|######### | 848/935 [02:30<00:15,  5.68it/s]\n",
      " 91%|######### | 849/935 [02:30<00:15,  5.70it/s]\n",
      " 91%|######### | 850/935 [02:31<00:14,  5.67it/s]\n",
      " 91%|#########1| 851/935 [02:31<00:14,  5.68it/s]\n",
      " 91%|#########1| 852/935 [02:31<00:14,  5.65it/s]\n",
      " 91%|#########1| 853/935 [02:31<00:14,  5.65it/s]\n",
      " 91%|#########1| 854/935 [02:31<00:14,  5.58it/s]\n",
      " 91%|#########1| 855/935 [02:32<00:14,  5.63it/s]\n",
      " 92%|#########1| 856/935 [02:32<00:14,  5.61it/s]\n",
      " 92%|#########1| 857/935 [02:32<00:13,  5.62it/s]\n",
      " 92%|#########1| 858/935 [02:32<00:13,  5.61it/s]\n",
      " 92%|#########1| 859/935 [02:32<00:13,  5.64it/s]\n",
      " 92%|#########1| 860/935 [02:32<00:13,  5.64it/s]\n",
      " 92%|#########2| 861/935 [02:33<00:13,  5.66it/s]\n",
      " 92%|#########2| 862/935 [02:33<00:12,  5.67it/s]\n",
      " 92%|#########2| 863/935 [02:33<00:12,  5.65it/s]\n",
      " 92%|#########2| 864/935 [02:33<00:12,  5.68it/s]\n",
      " 93%|#########2| 865/935 [02:33<00:12,  5.68it/s]\n",
      " 93%|#########2| 866/935 [02:33<00:12,  5.70it/s]\n",
      " 93%|#########2| 867/935 [02:34<00:11,  5.67it/s]\n",
      " 93%|#########2| 868/935 [02:34<00:11,  5.68it/s]\n",
      " 93%|#########2| 869/935 [02:34<00:11,  5.68it/s]\n",
      " 93%|#########3| 870/935 [02:34<00:11,  5.69it/s]\n",
      " 93%|#########3| 871/935 [02:34<00:11,  5.67it/s]\n",
      " 93%|#########3| 872/935 [02:35<00:11,  5.68it/s]\n",
      " 93%|#########3| 873/935 [02:35<00:10,  5.66it/s]\n",
      " 93%|#########3| 874/935 [02:35<00:10,  5.68it/s]\n",
      " 94%|#########3| 875/935 [02:35<00:10,  5.67it/s]\n",
      " 94%|#########3| 876/935 [02:35<00:10,  5.68it/s]\n",
      " 94%|#########3| 877/935 [02:35<00:10,  5.63it/s]\n",
      " 94%|#########3| 878/935 [02:36<00:10,  5.65it/s]\n",
      " 94%|#########4| 879/935 [02:36<00:10,  5.59it/s]\n",
      " 94%|#########4| 880/935 [02:36<00:09,  5.60it/s]\n",
      " 94%|#########4| 881/935 [02:36<00:09,  5.63it/s]\n",
      " 94%|#########4| 882/935 [02:36<00:09,  5.63it/s]\n",
      " 94%|#########4| 883/935 [02:36<00:09,  5.66it/s]\n",
      " 95%|#########4| 884/935 [02:37<00:08,  5.67it/s]\n",
      " 95%|#########4| 885/935 [02:37<00:08,  5.69it/s]\n",
      " 95%|#########4| 886/935 [02:37<00:08,  5.67it/s]\n",
      " 95%|#########4| 887/935 [02:37<00:08,  5.67it/s]\n",
      " 95%|#########4| 888/935 [02:37<00:08,  5.66it/s]\n",
      " 95%|#########5| 889/935 [02:38<00:08,  5.67it/s]\n",
      " 95%|#########5| 890/935 [02:38<00:07,  5.68it/s]\n",
      " 95%|#########5| 891/935 [02:38<00:07,  5.68it/s]\n",
      " 95%|#########5| 892/935 [02:38<00:07,  5.66it/s]\n",
      " 96%|#########5| 893/935 [02:38<00:07,  5.68it/s]\n",
      " 96%|#########5| 894/935 [02:38<00:07,  5.65it/s]\n",
      " 96%|#########5| 895/935 [02:39<00:07,  5.67it/s]\n",
      " 96%|#########5| 896/935 [02:39<00:06,  5.64it/s]\n",
      " 96%|#########5| 897/935 [02:39<00:06,  5.66it/s]\n",
      " 96%|#########6| 898/935 [02:39<00:06,  5.61it/s]\n",
      " 96%|#########6| 899/935 [02:39<00:06,  5.63it/s]\n",
      " 96%|#########6| 900/935 [02:39<00:06,  5.61it/s]\n",
      " 96%|#########6| 901/935 [02:40<00:06,  5.63it/s]\n",
      " 96%|#########6| 902/935 [02:40<00:05,  5.67it/s]\n",
      " 97%|#########6| 903/935 [02:40<00:05,  5.67it/s]\n",
      " 97%|#########6| 904/935 [02:40<00:05,  5.67it/s]\n",
      " 97%|#########6| 905/935 [02:40<00:05,  5.67it/s]\n",
      " 97%|#########6| 906/935 [02:41<00:05,  5.66it/s]\n",
      " 97%|#########7| 907/935 [02:41<00:04,  5.68it/s]\n",
      " 97%|#########7| 908/935 [02:41<00:04,  5.71it/s]\n",
      " 97%|#########7| 909/935 [02:41<00:04,  5.67it/s]\n",
      " 97%|#########7| 910/935 [02:41<00:04,  5.68it/s]\n",
      " 97%|#########7| 911/935 [02:41<00:04,  5.67it/s]\n",
      " 98%|#########7| 912/935 [02:42<00:04,  5.70it/s]\n",
      " 98%|#########7| 913/935 [02:42<00:03,  5.68it/s]\n",
      " 98%|#########7| 914/935 [02:42<00:03,  5.69it/s]\n",
      " 98%|#########7| 915/935 [02:42<00:03,  5.66it/s]\n",
      " 98%|#########7| 916/935 [02:42<00:03,  5.68it/s]\n",
      " 98%|#########8| 917/935 [02:42<00:03,  5.65it/s]\n",
      " 98%|#########8| 918/935 [02:43<00:02,  5.67it/s]\n",
      " 98%|#########8| 919/935 [02:43<00:02,  5.63it/s]\n",
      " 98%|#########8| 920/935 [02:43<00:02,  5.66it/s]\n",
      " 99%|#########8| 921/935 [02:43<00:02,  5.62it/s]\n",
      " 99%|#########8| 922/935 [02:43<00:02,  5.64it/s]\n",
      " 99%|#########8| 923/935 [02:44<00:02,  5.56it/s]\n",
      " 99%|#########8| 924/935 [02:44<00:01,  5.60it/s]\n",
      " 99%|#########8| 925/935 [02:44<00:01,  5.64it/s]\n",
      " 99%|#########9| 926/935 [02:44<00:01,  5.63it/s]\n",
      " 99%|#########9| 927/935 [02:44<00:01,  5.64it/s]\n",
      " 99%|#########9| 928/935 [02:44<00:01,  5.65it/s]\n",
      " 99%|#########9| 929/935 [02:45<00:01,  5.67it/s]\n",
      " 99%|#########9| 930/935 [02:45<00:00,  5.66it/s]\n",
      "100%|#########9| 931/935 [02:45<00:00,  5.65it/s]\n",
      "100%|#########9| 932/935 [02:45<00:00,  5.64it/s]\n",
      "100%|#########9| 933/935 [02:45<00:00,  5.67it/s]\n",
      "100%|#########9| 934/935 [02:46<00:00,  5.66it/s][INFO|trainer.py:1852] 2022-10-30 04:30:15,209 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                                 \n",
      "\n",
      "100%|##########| 935/935 [02:46<00:00,  5.66it/s]\n",
      "100%|##########| 935/935 [02:46<00:00,  5.63it/s]\n",
      "[INFO|trainer.py:2656] 2022-10-30 04:30:15,220 >> Saving model checkpoint to C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\n",
      "[INFO|configuration_utils.py:447] 2022-10-30 04:30:15,222 >> Configuration saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\\config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-30 04:30:15,572 >> Model weights saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2022-10-30 04:30:15,573 >> tokenizer config file saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2022-10-30 04:30:15,573 >> Special tokens file saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\\special_tokens_map.json\n",
      "[INFO|trainer.py:725] 2022-10-30 04:30:15,604 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:30:15,605 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:30:15,605 >>   Num examples = 852\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:30:15,605 >>   Batch size = 8\n",
      "\n",
      "  0%|          | 0/107 [00:00<?, ?it/s]\n",
      "  7%|7         | 8/107 [00:00<00:01, 68.95it/s]\n",
      " 14%|#4        | 15/107 [00:00<00:01, 63.88it/s]\n",
      " 21%|##        | 22/107 [00:00<00:01, 62.26it/s]\n",
      " 27%|##7       | 29/107 [00:00<00:01, 61.29it/s]\n",
      " 34%|###3      | 36/107 [00:00<00:01, 61.13it/s]\n",
      " 40%|####      | 43/107 [00:00<00:01, 61.04it/s]\n",
      " 47%|####6     | 50/107 [00:00<00:00, 60.98it/s]\n",
      " 53%|#####3    | 57/107 [00:00<00:00, 61.00it/s]\n",
      " 60%|#####9    | 64/107 [00:01<00:00, 61.14it/s]\n",
      " 66%|######6   | 71/107 [00:01<00:00, 61.05it/s]\n",
      " 73%|#######2  | 78/107 [00:01<00:00, 61.33it/s]\n",
      " 79%|#######9  | 85/107 [00:01<00:00, 61.46it/s]\n",
      " 86%|########5 | 92/107 [00:01<00:00, 60.48it/s]\n",
      " 93%|#########2| 99/107 [00:01<00:00, 60.28it/s]\n",
      " 99%|#########9| 106/107 [00:01<00:00, 60.77it/s]\n",
      "100%|##########| 107/107 [00:01<00:00, 61.02it/s]\n",
      "[INFO|trainer.py:725] 2022-10-30 04:30:17,377 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:30:17,377 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:30:17,378 >>   Num examples = 852\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:30:17,378 >>   Batch size = 8\n",
      "\n",
      "  0%|          | 0/107 [00:00<?, ?it/s]\n",
      "  7%|7         | 8/107 [00:00<00:01, 70.78it/s]\n",
      " 15%|#4        | 16/107 [00:00<00:01, 64.72it/s]\n",
      " 21%|##1       | 23/107 [00:00<00:01, 63.26it/s]\n",
      " 28%|##8       | 30/107 [00:00<00:01, 62.76it/s]\n",
      " 35%|###4      | 37/107 [00:00<00:01, 62.46it/s]\n",
      " 41%|####1     | 44/107 [00:00<00:01, 61.74it/s]\n",
      " 48%|####7     | 51/107 [00:00<00:00, 61.45it/s]\n",
      " 54%|#####4    | 58/107 [00:00<00:00, 61.43it/s]\n",
      " 61%|######    | 65/107 [00:01<00:00, 61.25it/s]\n",
      " 67%|######7   | 72/107 [00:01<00:00, 60.48it/s]\n",
      " 74%|#######3  | 79/107 [00:01<00:00, 60.75it/s]\n",
      " 80%|########  | 86/107 [00:01<00:00, 61.11it/s]\n",
      " 87%|########6 | 93/107 [00:01<00:00, 61.35it/s]\n",
      " 93%|#########3| 100/107 [00:01<00:00, 61.53it/s]\n",
      "100%|##########| 107/107 [00:01<00:00, 62.48it/s]\n",
      "100%|##########| 107/107 [00:01<00:00, 61.80it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev-test', LANGUAGE_CODE)\n",
    "OUTPUT_DIR = os.path.join(PROJECT_DIR, 'models', LANGUAGE_CODE + '_no_test')\n",
    "kinya = 'jean-paul/KinyaBERT-small'\n",
    "\n",
    "!python starter_kit/run_textclass.py \\\n",
    "  --model_name_or_path {kinya} \\\n",
    "  --data_dir {DATA_DIR} \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
    "  --learning_rate {MAXIMUM_SEQUENCE_LENGTH} \\\n",
    "  --num_train_epochs {NUMBER_OF_TRAINING_EPOCHS} \\\n",
    "  --max_seq_length {MAXIMUM_SEQUENCE_LENGTH} \\\n",
    "  --output_dir {OUTPUT_DIR} \\\n",
    "  --save_steps {SAVE_STEPS} \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fi_pJl3N9RcT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d203a7fbe37afbb990fedfc21c321928443618f3d7b991e0237ff71906aa031f"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "008a7cb40017467b8d4450e52e0c3ec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2691cee6ebda4659afdc7afc3d7c3ccf",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d96193cb0ef436987f37d611e0c3c23",
      "value": 9
     }
    },
    "01bbb58eeced4ed4920756d513870c5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08e3f965a5ea4b338aba411dbb969ec6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c8da5d1f22f4f2990196f1d1164dc72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1193886e44d94cc9927d05680e26348d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_edd1fc29c36a4beda3528a5b962d5d4e",
      "placeholder": "​",
      "style": "IPY_MODEL_508778f87d1a4733a86bc7466ce6c689",
      "value": " 9/10 [00:18&lt;00:01,  1.89s/ba]"
     }
    },
    "1d96193cb0ef436987f37d611e0c3c23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2048f8794fef499cb0757fad16437a92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c8da5d1f22f4f2990196f1d1164dc72",
      "placeholder": "​",
      "style": "IPY_MODEL_47c11996d2f142779998e1ce52519e62",
      "value": " 1/2 [00:02&lt;00:01,  1.86s/ba]"
     }
    },
    "2691cee6ebda4659afdc7afc3d7c3ccf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "426e41f07cb145408a12146dff293ad6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b31b9c1562a4505bd84039e344eb03a",
      "placeholder": "​",
      "style": "IPY_MODEL_ac7711f32d374b74b6f7949db6d2cb94",
      "value": "Running tokenizer on validation dataset:  50%"
     }
    },
    "47c11996d2f142779998e1ce52519e62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b31b9c1562a4505bd84039e344eb03a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e6edbccc9224ba89714e6d0e67fe76e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aafb2c69887549648e9c7044d6a344f3",
       "IPY_MODEL_008a7cb40017467b8d4450e52e0c3ec5",
       "IPY_MODEL_1193886e44d94cc9927d05680e26348d"
      ],
      "layout": "IPY_MODEL_01bbb58eeced4ed4920756d513870c5f"
     }
    },
    "508778f87d1a4733a86bc7466ce6c689": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e01536f748a4689b0c1ab32258b0161": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "865bc4814f3742cdb1d2230f141a8e72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e01536f748a4689b0c1ab32258b0161",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6cc64f17d074ff68cced49559981438",
      "value": 1
     }
    },
    "877d1854199e4362877dd8774f10bc23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_426e41f07cb145408a12146dff293ad6",
       "IPY_MODEL_865bc4814f3742cdb1d2230f141a8e72",
       "IPY_MODEL_2048f8794fef499cb0757fad16437a92"
      ],
      "layout": "IPY_MODEL_92b9d2b9889a4d92a3907c5791df3091"
     }
    },
    "92b9d2b9889a4d92a3907c5791df3091": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aafb2c69887549648e9c7044d6a344f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08e3f965a5ea4b338aba411dbb969ec6",
      "placeholder": "​",
      "style": "IPY_MODEL_b4943d26f11e46358a64ca534efade84",
      "value": "Running tokenizer on train dataset:  90%"
     }
    },
    "ac7711f32d374b74b6f7949db6d2cb94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4943d26f11e46358a64ca534efade84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "edd1fc29c36a4beda3528a5b962d5d4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6cc64f17d074ff68cced49559981438": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
