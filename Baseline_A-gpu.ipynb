{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEkdLm_JF84s"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/afrisenti-semeval/afrisent-semeval-2023/main/afrisenti-logo.png\" width=\"30%\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXY0zNpmLM1Z"
   },
   "source": [
    "<center>\n",
    "\n",
    "#SemEval 2023 Shared Task 12: AfriSenti (Task A)\n",
    "\n",
    "###Starter Notebook\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3wrnOfUBE7A"
   },
   "source": [
    "Baseline code based on the starter code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4BB4JDx5W8d"
   },
   "source": [
    "#1) Installations and imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DObkW3ulM7yg"
   },
   "source": [
    "##a. Mount drive (if you are running on colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-OZxUWIMqtq"
   },
   "source": [
    "##b. Clone or update competition repository\n",
    "\n",
    "After cloning, under MyDrive, you will see afrisenti-semeval-2023 folder with all the the data for the afrisenti shared task (training and dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17192,
     "status": "ok",
     "timestamp": 1666932930576,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "OsuZvweV6jzo",
    "outputId": "601e0364-95f3-4c0e-d3e9-fa63688e3546"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Language to train sentiment classifier for\n",
    "# am dz ha ig ma pcm pt sw yo\n",
    "\n",
    "LANGUAGE_CODE = 'yo'\n",
    "folder = ''\n",
    "\n",
    "colab = False\n",
    "\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    proj_folder = '/content/drive/MyDrive'\n",
    "else:\n",
    "    proj_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6974,
     "status": "ok",
     "timestamp": 1666932941234,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "2392TKadMvqT",
    "outputId": "a5427066-2d8f-4e1f-ccda-46eddb636031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Downloads\\afrisent\n",
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\n"
     ]
    }
   ],
   "source": [
    "%cd {proj_folder}\n",
    "\n",
    "\n",
    "PROJECT_DIR = f'{proj_folder}/afrisent-semeval-2023'\n",
    "PROJECT_GITHUB_URL = 'https://github.com/afrisenti-semeval/afrisent-semeval-2023.git'\n",
    "\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  !git clone {PROJECT_GITHUB_URL}\n",
    "else:\n",
    "  %cd {PROJECT_DIR}\n",
    "  pass\n",
    "#   !git pull {PROJECT_GITHUB_URL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb03Gp9fUN8C"
   },
   "source": [
    "##c. Install required libraries\n",
    "\n",
    "- Set the project dire\n",
    "ctory in the cell below, where the requirements file should also be located, and run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Cbmi_mQ4k3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\n",
      "Requirement already satisfied: pandas in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 1)) (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 2)) (1.21.5)\n",
      "Requirement already satisfied: transformers in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 3)) (4.23.1)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 4)) (1.12.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 5)) (1.7.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 6)) (3.19.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 7)) (1.0.2)\n",
      "Requirement already satisfied: accelerate in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 8)) (0.13.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 9)) (0.1.97)\n",
      "Requirement already satisfied: datasets>=1.8.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 10)) (2.6.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from -r starter_kit/requirements.txt (line 11)) (0.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from pandas->-r starter_kit/requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from pandas->-r starter_kit/requirements.txt (line 1)) (2021.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (4.64.0)\n",
      "Requirement already satisfied: requests in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (0.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from transformers->-r starter_kit/requirements.txt (line 3)) (2022.3.15)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from torch>=1.3->-r starter_kit/requirements.txt (line 4)) (4.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from scikit-learn->-r starter_kit/requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from scikit-learn->-r starter_kit/requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from accelerate->-r starter_kit/requirements.txt (line 8)) (5.8.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (2022.2.0)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (0.3.5.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (3.1.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (0.18.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (3.8.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (0.70.13)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (9.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers->-r starter_kit/requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->-r starter_kit/requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->transformers->-r starter_kit/requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->transformers->-r starter_kit/requirements.txt (line 3)) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->transformers->-r starter_kit/requirements.txt (line 3)) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->transformers->-r starter_kit/requirements.txt (line 3)) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers->-r starter_kit/requirements.txt (line 3)) (0.4.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (5.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.6.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.8.0->-r starter_kit/requirements.txt (line 10)) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "%cd {PROJECT_DIR}\n",
    "\n",
    "if os.path.isdir(PROJECT_DIR):\n",
    "  #The requirements file should be in PROJECT_DIR\n",
    "  if os.path.isfile(os.path.join(PROJECT_DIR, 'starter_kit/requirements.txt')):\n",
    "    !pip install -r starter_kit/requirements.txt\n",
    "  else:\n",
    "    print('requirements.txt file not found')\n",
    "\n",
    "else:\n",
    "  print(\"Project directory not found, please check again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2zszKhh2Ufb3"
   },
   "source": [
    "##d. Import libraries\n",
    "\n",
    "Import libraries below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8QIl420aUM1O"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Please don not edit anything here\n",
    "languages = ['am', 'dz', 'ha', 'ig', 'ma', 'pcm', 'pt', 'sw', 'yo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoRyMJMDJ7lF"
   },
   "source": [
    "#2) Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wk5vMnrXMSS"
   },
   "source": [
    "##a. Formatting\n",
    "\n",
    "The training dataset that was provided for the competition is in the following format:\n",
    "\n",
    "| ID | text | label |\n",
    "| --- | --- | --- |\n",
    "| twt001 | example text | negative |\n",
    "| twt002 | example text | positive |\n",
    "| ... | ... | ... |\n",
    "\n",
    "However, the code in the starter kit do not expect the \n",
    "ID and require the training (and evaluation) data to be in the following format\n",
    "\n",
    "|text | label |\n",
    "|--- | --- |\n",
    "|example text | negative |\n",
    "|example text | positive |\n",
    "|... | ... |\n",
    "\n",
    "To reformat the data run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1666884271530,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "QzoSbWC678Zm",
    "outputId": "69eacc7f-cad8-4bad-9f0f-71c4c32758ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory found.\n"
     ]
    }
   ],
   "source": [
    "# Training Data Paths\n",
    "\n",
    "TASK = 'SubtaskA'\n",
    "TRAINING_DATA_DIR = os.path.join(PROJECT_DIR, TASK, 'train')\n",
    "FORMATTED_TRAIN_DATA = os.path.join(TRAINING_DATA_DIR, 'formatted-train-data')\n",
    "\n",
    "if os.path.isdir(TRAINING_DATA_DIR):\n",
    "  print('Data directory found.')\n",
    "  if not os.path.isdir(FORMATTED_TRAIN_DATA):\n",
    "    print('Creating directory to store formatted data.')\n",
    "    os.mkdir(FORMATTED_TRAIN_DATA)\n",
    "else:\n",
    "  print(TRAINING_DATA_DIR + ' is not a valid directory or does not exist!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1666884272475,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "ssyIZOUJMrzM",
    "outputId": "7a883beb-a7cd-44ca-8ab8-84147f25295d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\SubtaskA\\train\n",
      "formatted-train-data skipped!\n",
      "README.txt skipped!\n",
      "splitted-train-dev-test skipped!\n"
     ]
    }
   ],
   "source": [
    "%cd {TRAINING_DATA_DIR}\n",
    "\n",
    "training_files = os.listdir()\n",
    "\n",
    "if len(training_files) > 0:\n",
    "  for training_file in training_files:\n",
    "    if training_file.endswith('.tsv'):\n",
    "\n",
    "      data = training_file.split('_')[0]\n",
    "      if not os.path.isdir(os.path.join(FORMATTED_TRAIN_DATA, data)):\n",
    "        print(data, 'Creating directory to store train, dev and test splits.')\n",
    "        os.mkdir(os.path.join(FORMATTED_TRAIN_DATA, data))\n",
    "      \n",
    "      df = pd.read_csv(training_file, sep='\\t', names=['ID', 'text', 'label'], header=0)\n",
    "      df[['text', 'label']].to_csv(os.path.join(FORMATTED_TRAIN_DATA, data, 'train.tsv'), sep='\\t', index=False)\n",
    "    else:\n",
    "      print(training_file + ' skipped!')\n",
    "else:\n",
    "  print('No files are found in this directory!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S2Dup8GHl1Q"
   },
   "source": [
    "After running the code above, a new folder (called formated-train-data) with formated files is created in the \"datasets\" folder in the train sub-folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5LgeVN_wXGrq"
   },
   "source": [
    "##b. <font color='red'>`(Optional) Creating Evaluation (Dev and Test) sets from the available training data`</font>\n",
    "\n",
    "You may wish to create train and evaluation (dev and test) sets from the training data provided. If you wish to do so, you can run any of the cells below`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APxVxL06lfux"
   },
   "source": [
    "###i. If you want to create both the Dev and Test sets, run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1371,
     "status": "ok",
     "timestamp": 1666884273844,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "aVq1Blz0YF2b",
    "outputId": "f1397651-bded-49cb-e9ff-e9eac2dfe952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory found.\n",
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\SubtaskA\\train\\formatted-train-data\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir(FORMATTED_TRAIN_DATA):\n",
    "  print('Data directory found.')\n",
    "  SPLITTED_DATA = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev-test')\n",
    "  if not os.path.isdir(SPLITTED_DATA):\n",
    "    print('Creating directory to store train, dev and test splits.')\n",
    "    os.mkdir(SPLITTED_DATA)\n",
    "else:\n",
    "  print(FORMATTED_TRAIN_DATA + ' is not a valid directory or does not exist!')\n",
    "\n",
    "%cd {FORMATTED_TRAIN_DATA}\n",
    "formatted_training_files = os.listdir()\n",
    "\n",
    "if len(formatted_training_files) > 0:\n",
    "  for data_name in formatted_training_files:\n",
    "    formatted_training_file = os.path.join(data_name, 'train.tsv')\n",
    "    if os.path.isfile(formatted_training_file):\n",
    "      labeled_tweets = pd.read_csv(formatted_training_file, sep='\\t', names=['text', 'label'], header=0)\n",
    "      train, dev, test = np.split(labeled_tweets.sample(frac=1, random_state=42), [int(.7*len(labeled_tweets)), int(.8*len(labeled_tweets))])\n",
    "\n",
    "      if not os.path.isdir(os.path.join(SPLITTED_DATA, data_name)):\n",
    "        print(data_name, 'Creating directory to store train, dev and test splits.')\n",
    "        os.mkdir(os.path.join(SPLITTED_DATA, data_name))\n",
    "\n",
    "      train.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'train.tsv'), sep='\\t', index=False)\n",
    "      dev.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name, 'dev.tsv'), sep='\\t', index=False)\n",
    "      test.sample(frac=1).to_csv(os.path.join(SPLITTED_DATA, data_name,'test.tsv'), sep='\\t', index=False)\n",
    "    else:\n",
    "      print(training_file + ' is not a supported file!')\n",
    "else:\n",
    "  print('No files are found in this directory!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDoyRlje3Rm7"
   },
   "source": [
    "#3) Training setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AaXec415s0f"
   },
   "source": [
    "##a. Set project parameters\n",
    "\n",
    "For a list of models that be used for fine-tuning, you can check [HERE](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1666884274396,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "M0TKIFrE5ybV",
    "outputId": "d0b5ee8b-20f7-4fa3-b8b6-65b773c70060"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\n",
      "Everything set. You can now start model training.\n"
     ]
    }
   ],
   "source": [
    "%cd {PROJECT_DIR}\n",
    "\n",
    "if LANGUAGE_CODE in languages:\n",
    "  # Model Training Parameters\n",
    "  MODEL_NAME_OR_PATH = 'Davlan/afro-xlmr-mini'\n",
    "  BATCH_SIZE = 32\n",
    "  LEARNING_RATE = 5e-5\n",
    "  NUMBER_OF_TRAINING_EPOCHS = 5\n",
    "  MAXIMUM_SEQUENCE_LENGTH = 128\n",
    "  SAVE_STEPS = -1\n",
    "\n",
    "  print('Everything set. You can now start model training.')\n",
    "\n",
    "else:\n",
    "  print(\"Invalid language code/Dataset not released. Please input any of the following released data\\n\\n\\t- 'am'\\n\\t- 'dz'\\n\\t- 'ha'\\n\\t- 'ig'\\n\\t- 'ma'\\n\\t- 'pcm'\\n\\t- 'pt'\\n\\t- 'sw'\\n\\t- 'yo'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2qcCQnU8dgQ"
   },
   "source": [
    "##b. Train the model\n",
    "\n",
    "In the section below, we provide three options: \n",
    "\n",
    "- 1) training model without any validation; \n",
    "- 2) training model with validation but without testing; \n",
    "- 3) training a model with validation and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fEw-qcEhYnx"
   },
   "source": [
    "###Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sE22Nmnx9lf1"
   },
   "source": [
    "\n",
    "\n",
    "####Starter Code: Datasets, etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 19003,
     "status": "ok",
     "timestamp": 1666879952672,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "ERP2sja3i3uQ",
    "outputId": "730084f3-6cde-4ef4-a638-861aed0addea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1710cbdb5f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import warnings\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "import evaluate\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from datasets import Features, Value, ClassLabel, load_dataset, Dataset\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/text-classification/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "\n",
    "\n",
    "np.random.seed(420)\n",
    "torch.manual_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1666879952673,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "2k6XKSygMnT4",
    "outputId": "debd529a-c939-47d6-ae2c-60dd40430f4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Thomas\\\\Downloads\\\\afrisent/afrisent-semeval-2023\\\\SubtaskA\\\\train'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAINING_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EJPw829sIRY3"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev-test', LANGUAGE_CODE)\n",
    "EVAL_DIR = os.path.join(PROJECT_DIR, TASK, 'dev')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_DIR, 'models', LANGUAGE_CODE + '_no_eval')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUMBER_OF_TRAINING_EPOCHS,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    overwrite_output_dir=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "60upFv0znHN5"
   },
   "outputs": [],
   "source": [
    "data_args = SimpleNamespace(**{\n",
    "    'max_seq_length': MAXIMUM_SEQUENCE_LENGTH,\n",
    "    'overwrite_cache': False,\n",
    "    'pad_to_max_length': True,\n",
    "    'max_train_samples': None,\n",
    "    'max_eval_samples': None,\n",
    "    'max_predict_samples': None\n",
    "})\n",
    "\n",
    "model_args = SimpleNamespace(**{\n",
    "    'model_name_or_path': MODEL_NAME_OR_PATH,\n",
    "    'config_name': None,\n",
    "    'tokenizer_name': None,\n",
    "    'data_dir': DATA_DIR,\n",
    "    'eval_dir': EVAL_DIR,\n",
    "    'cache_dir': None,\n",
    "    'do_lower_case': None,\n",
    "    'use_fast_tokenizer': True,\n",
    "    'model_revision': 'main',\n",
    "    'use_auth_token': False,\n",
    "    'ignore_mismatched_sizes': False\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2Nx63rcbnHXy"
   },
   "outputs": [],
   "source": [
    "# See all possible arguments in src/transformers/training_args.py\n",
    "# or by passing the --help flag to this script.\n",
    "# We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "# parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "# model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "#send_example_telemetry(\"run_xnli\", model_args)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "\n",
    "log_level = 0 #training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1666879952910,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "yGlFrGztnhVX",
    "outputId": "52cf93f7-869b-4240-dd91-546daaa57a85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2022 04:22:41 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "# Detecting last checkpoint.\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1666879952911,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "ZSh2DwJzM_cI",
    "outputId": "326b1c0d-2cd5-4516-a142-7e0342714bd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yo'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGUAGE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1666883839778,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "hZc-mzkXnhdF",
    "outputId": "06e5bc16-fd84-4c40-f720-3558328f4fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'neutral', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
    "# download the dataset.\n",
    "# Downloading and loading xnli dataset from the hub.\n",
    "\n",
    "\n",
    "if training_args.do_train:\n",
    "\n",
    "    #train_dataset = load_dataset('csv', data_files={'train': model_args.data_dir + '/train.csv'}, cache_dir=model_args.cache_dir)\n",
    "    #df = train_dataset[\"train\"].to_pandas()\n",
    "    #label_list = df['label'].unique().tolist()\n",
    "    #label_list = train_dataset.features[\"label\"].names\n",
    "    df = pd.read_csv(model_args.data_dir + '/train.tsv', sep='\\t')\n",
    "    df = df.dropna()\n",
    "    train_dataset = Dataset.from_pandas(df)\n",
    "    label_list = df['label'].unique().tolist()\n",
    "\n",
    "if training_args.do_eval:\n",
    "    #eval_dataset = load_dataset('csv', data_files={'validation': model_args.data_dir + '/dev.csv'}, cache_dir=model_args.cache_dir)\n",
    "\n",
    "    #df = eval_dataset[\"validation\"].to_pandas()\n",
    "    #label_list = df['label'].unique().tolist()\n",
    "    #label_list = eval_dataset.features[\"label\"].names\n",
    "    df = pd.read_csv(model_args.data_dir + '/dev.tsv', sep='\\t')\n",
    "    df = df.dropna()\n",
    "    eval_dataset = Dataset.from_pandas(df)\n",
    "    label_list = df['label'].unique().tolist()\n",
    "\n",
    "if training_args.do_predict:\n",
    "    #predict_dataset = load_dataset('csv', data_files={'test': model_args.data_dir + '/test.csv'}, cache_dir=model_args.cache_dir)\n",
    "\n",
    "    #df = predict_dataset[\"test\"].to_pandas()\n",
    "    #label_list = df['label'].unique().tolist()\n",
    "    #label_list = predict_dataset.features[\"label\"].names\n",
    "    df = pd.read_csv(model_args.data_dir + '/test.tsv', sep='\\t')\n",
    "    df = df.dropna()\n",
    "    predict_dataset = Dataset.from_pandas(df)\n",
    "    label_list = df['label'].unique().tolist()\n",
    "\n",
    "# Labels\n",
    "num_labels = len(label_list)\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9gxi4Ll-HFQ"
   },
   "source": [
    "####Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6173,
     "status": "ok",
     "timestamp": 1666883849304,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "pd_l2PG3nhiY",
    "outputId": "5753544f-0e2b-489c-edfc-1014449d4170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|modeling_utils.py:2596] 2022-10-30 04:22:42,741 >> Some weights of the model checkpoint at Davlan/afro-xlmr-mini were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2608] 2022-10-30 04:22:42,742 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-mini and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "#         finetuning_task=\"xnli\",\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    do_lower_case=model_args.do_lower_case,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=model_args.use_fast_tokenizer,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    "    ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1666883849305,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "slMaLbYvnHmP",
    "outputId": "7c6e8b47-3511-42b8-a689-317cd74f3190"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    def preprocess_function(examples):\\n    # Tokenize the texts\\n    return tokenizer(\\n        examples[\"premise\"],\\n        examples[\"hypothesis\"],\\n        padding=padding,\\n        max_length=data_args.max_seq_length,\\n        truncation=True,\\n    )\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing the datasets\n",
    "# Padding strategy\n",
    "if data_args.pad_to_max_length:\n",
    "    padding = \"max_length\"\n",
    "else:\n",
    "    # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n",
    "    padding = False\n",
    "\n",
    "# Some models have set the order of the labels to use, so let's make sure we do use it.\n",
    "label_to_id = None\n",
    "label_to_id = {v: i for i, v in enumerate(label_list)}\n",
    "\n",
    "if label_to_id is not None:\n",
    "    model.config.label2id = label_to_id\n",
    "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "'''\n",
    "    def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    return tokenizer(\n",
    "        examples[\"premise\"],\n",
    "        examples[\"hypothesis\"],\n",
    "        padding=padding,\n",
    "        max_length=data_args.max_seq_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 21194,
     "status": "ok",
     "timestamp": 1666883870487,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "gKAQ2leboGN3",
    "outputId": "4d40c5b1-5009-4979-cf98-dfbbd898627e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdd40bf3f254dac801e83233903a0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4816cce7c24e329bd5a1cad9d32b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Tokenize the texts\n",
    "    #print\n",
    "    texts =(examples['text'],)\n",
    "    result = tokenizer(*texts, padding=padding, max_length=data_args.max_seq_length, truncation=True)\n",
    "    #print(examples['text'])\n",
    "    #result = tokenizer(examples['text'], examples['text'], padding=padding, max_length=data_args.max_seq_length, truncation=True)\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    \n",
    "    result['length'], result[\"tokenized\"] = [], []\n",
    "    for input_ids in result['input_ids']:\n",
    "        toks = tokenizer.convert_ids_to_tokens(input_ids, skip_special_tokens=True)\n",
    "        result['length'].append(len(toks)+2)\n",
    "        result['tokenized'].append(' '.join(toks))\n",
    "    return result\n",
    "\n",
    "if training_args.do_train:\n",
    "    if data_args.max_train_samples is not None:\n",
    "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "    with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "        train_dataset = train_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )\n",
    "    # Log a few random samples from the training set:\n",
    "    for index in random.sample(range(len(train_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "\n",
    "if training_args.do_eval:\n",
    "    if data_args.max_eval_samples is not None:\n",
    "        max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "        eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "    with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )\n",
    "\n",
    "if training_args.do_predict:\n",
    "    if data_args.max_predict_samples is not None:\n",
    "        max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
    "        predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
    "    with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n",
    "        predict_dataset = predict_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on prediction dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1666883921333,
     "user": {
      "displayName": "Thomas Lu",
      "userId": "07958275653434804774"
     },
     "user_tz": 240
    },
    "id": "6Hi6VwK5O1ve",
    "outputId": "1d30958b-4b3d-409f-e9c2-97f9fb4a07f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask', 'length', 'tokenized'],\n",
       "     num_rows: 5965\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask', 'length', 'tokenized'],\n",
       "     num_rows: 852\n",
       " }))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNWVct2XARap"
   },
   "source": [
    "####LSTM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "U6a2g_l1rZan"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6tSt7NFOezKe"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=128, emb_dim=300, num_layers=1, dropout=0.5, lstm_dropout=0.0):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(len(tokenizer), emb_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=lstm_dropout)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(2*hidden_dim, 3)\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "\n",
    "        text_emb = self.embedding(text)\n",
    "\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.hidden_dim]\n",
    "        out_reverse = output[:, 0, self.hidden_dim:]\n",
    "        out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        text_fea = self.drop(out_reduced)\n",
    "\n",
    "        text_fea = self.fc(text_fea)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = text_fea\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "KzLIu8Jo7QJq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193, 4772, 852)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pts = len(train_dataset)\n",
    "shuffled_ids = np.arange(num_pts, dtype=int)\n",
    "np.random.shuffle(shuffled_ids)\n",
    "\n",
    "valid_ids = torch.LongTensor(np.array(train_dataset['input_ids'])[shuffled_ids[:num_pts // 5]])\n",
    "valid_lengths = torch.LongTensor(np.array(train_dataset['length'])[shuffled_ids[:num_pts // 5]]).cpu()\n",
    "valid_labels = torch.LongTensor(np.array(train_dataset['label'])[shuffled_ids[:num_pts // 5]])\n",
    "\n",
    "train_ids = torch.LongTensor(np.array(train_dataset['input_ids'])[shuffled_ids[num_pts // 5:]])\n",
    "train_lengths = torch.LongTensor(np.array(train_dataset['length'])[shuffled_ids[num_pts // 5:]]).cpu()\n",
    "train_labels = torch.LongTensor(np.array(train_dataset['label'])[shuffled_ids[num_pts // 5:]])\n",
    "\n",
    "eval_ids = torch.LongTensor(eval_dataset['input_ids'])\n",
    "eval_lengths = torch.LongTensor(eval_dataset['length']).cpu()\n",
    "eval_labels = torch.LongTensor(eval_dataset['label'])\n",
    "len(valid_ids), len(train_ids), len(eval_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "n_ZIAx_8UZzA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 0.0312 Val Acc: 0.583, Eval Acc: 0.526, Eval Acc @ Best Val 0.526, Eval F1: 0.566: 100%|| 150/\n",
      "Epoch 2/15, Train Loss: 0.0253 Val Acc: 0.597, Eval Acc: 0.56, Eval Acc @ Best Val 0.56, Eval F1: 0.575: 100%|| 150/15\n",
      "Epoch 3/15, Train Loss: 0.0192 Val Acc: 0.598, Eval Acc: 0.581, Eval Acc @ Best Val 0.581, Eval F1: 0.585: 100%|| 150/\n",
      "Epoch 4/15, Train Loss: 0.0128 Val Acc: 0.605, Eval Acc: 0.575, Eval Acc @ Best Val 0.575, Eval F1: 0.574: 100%|| 150/\n",
      "Epoch 5/15, Train Loss: 0.00816 Val Acc: 0.593, Eval Acc: 0.574, Eval Acc @ Best Val 0.574, Eval F1: 0.572: 100%|| 150\n",
      "Epoch 6/15, Train Loss: 0.00537 Val Acc: 0.617, Eval Acc: 0.587, Eval Acc @ Best Val 0.587, Eval F1: 0.595: 100%|| 150\n",
      "Epoch 7/15, Train Loss: 0.00377 Val Acc: 0.602, Eval Acc: 0.57, Eval Acc @ Best Val 0.57, Eval F1: 0.569: 100%|| 150/1\n",
      "Epoch 8/15, Train Loss: 0.00339 Val Acc: 0.616, Eval Acc: 0.583, Eval Acc @ Best Val 0.583, Eval F1: 0.586: 100%|| 150\n",
      "Epoch 9/15, Train Loss: 0.00213 Val Acc: 0.612, Eval Acc: 0.582, Eval Acc @ Best Val 0.582, Eval F1: 0.591: 100%|| 150\n",
      "Epoch 10/15, Train Loss: 0.00105 Val Acc: 0.605, Eval Acc: 0.577, Eval Acc @ Best Val 0.577, Eval F1: 0.579: 100%|| 15\n",
      "Epoch 11/15, Train Loss: 0.000925 Val Acc: 0.624, Eval Acc: 0.586, Eval Acc @ Best Val 0.586, Eval F1: 0.587: 100%|| 1\n",
      "Epoch 12/15, Train Loss: 0.000585 Val Acc: 0.609, Eval Acc: 0.579, Eval Acc @ Best Val 0.579, Eval F1: 0.582: 100%|| 1\n",
      "Epoch 13/15, Train Loss: 0.000456 Val Acc: 0.607, Eval Acc: 0.594, Eval Acc @ Best Val 0.594, Eval F1: 0.596: 100%|| 1\n",
      "Epoch 14/15, Train Loss: 0.000424 Val Acc: 0.616, Eval Acc: 0.595, Eval Acc @ Best Val 0.595, Eval F1: 0.596: 100%|| 1\n",
      "Epoch 15/15, Train Loss: 0.000511 Val Acc: 0.604, Eval Acc: 0.579, Eval Acc @ Best Val 0.579, Eval F1: 0.58: 100%|| 15\n"
     ]
    }
   ],
   "source": [
    "def train(criterion = nn.CrossEntropyLoss(),\n",
    "          batch_size = 32,\n",
    "          num_epochs = 30,\n",
    "          eval_every = 4,\n",
    "          params={},\n",
    "          lr=0.0005,\n",
    "          leave=False):\n",
    "    model = LSTM(**params).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    eval_every = (len(train_ids) / batch_size) // eval_every\n",
    "    best_valid_acc = 0\n",
    "    best_valid_f1 = 0\n",
    "    model.train()\n",
    "    best_preds = []\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar = tqdm(range(0, len(train_ids), batch_size), leave=leave, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        total_train_loss = 0.0\n",
    "        total_points = 0\n",
    "        for i in pbar:           \n",
    "            labels = train_labels[i:i+batch_size].to(device)\n",
    "            inps = train_ids[i:i+batch_size].to(device)\n",
    "            lengths = train_lengths[i:i+batch_size]#.to(device)\n",
    "            output = model(inps, lengths)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            total_train_loss += loss.item()\n",
    "            total_points += labels.size(0)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i + batch_size) % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    num_correct = 0\n",
    "                    eval_batch_size = 100\n",
    "                    for j in range(0, len(valid_ids), eval_batch_size):\n",
    "                        labels = valid_labels[j:j+eval_batch_size].to(device)\n",
    "                        inps = valid_ids[j:j+eval_batch_size].to(device)\n",
    "                        lengths = valid_lengths[j:j+eval_batch_size]#.to(device)\n",
    "                        output = model(inps, lengths)\n",
    "                        output = torch.argmax(output, -1)\n",
    "                        num_correct += torch.sum(output == labels).cpu().numpy()\n",
    "                    valid_accuracy = num_correct / len(valid_ids)\n",
    "\n",
    "                    accuracy = \"N/A\"\n",
    "                    preds = np.array([])\n",
    "                    if valid_accuracy > best_valid_acc:\n",
    "                        num_correct = 0\n",
    "                        eval_batch_size = 100\n",
    "                        for j in range(0, len(eval_ids), eval_batch_size):\n",
    "                            labels = eval_labels[j:j+eval_batch_size].to(device)\n",
    "                            inps = eval_ids[j:j+eval_batch_size].to(device)\n",
    "                            lengths = eval_lengths[j:j+eval_batch_size]#.to(device)\n",
    "                            output = model(inps, lengths)\n",
    "                            output = torch.argmax(output, -1)\n",
    "                            preds = np.append(preds, output.cpu().numpy())\n",
    "                            num_correct += torch.sum(output == labels).cpu().numpy()\n",
    "                        accuracy = num_correct / len(eval_ids)\n",
    "                        best_valid_f1 = f1_score(preds, eval_labels, average='weighted')\n",
    "                        best_preds = preds\n",
    "                        best_valid_acc = accuracy\n",
    "                    pbar.set_description(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {(total_train_loss / total_points):.3} ' + \\\n",
    "                                         f'Val Acc: {valid_accuracy:.3}, Eval Acc: {accuracy:.3}, Eval Acc @ Best Val {best_valid_acc:.3}, Eval F1: {best_valid_f1:.3}')\n",
    "                model.train()\n",
    "    return best_valid_acc, best_valid_f1, preds, eval_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "acc, f1, preds, labels = train(num_epochs=15, params={\n",
    "    'hidden_dim': 128, 'emb_dim': 300, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5},\n",
    "      lr=0.001, leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-uWlHsFwUZ8z"
   },
   "outputs": [],
   "source": [
    "del train_ids\n",
    "del train_lengths\n",
    "del train_labels\n",
    "\n",
    "del eval_ids\n",
    "del eval_lengths\n",
    "del eval_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZx_-UywAcmj"
   },
   "source": [
    "####Pretrained Transformer Baseline - xlmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7xpriZKGrFCK",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:24:34,180 >> The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1607] 2022-10-30 04:24:34,191 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-10-30 04:24:34,192 >>   Num examples = 5965\n",
      "[INFO|trainer.py:1609] 2022-10-30 04:24:34,192 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1610] 2022-10-30 04:24:34,192 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1611] 2022-10-30 04:24:34,192 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1612] 2022-10-30 04:24:34,193 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-10-30 04:24:34,193 >>   Total optimization steps = 935\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='935' max='935' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [935/935 02:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.903900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1852] 2022-10-30 04:26:58,435 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:2656] 2022-10-30 04:26:58,453 >> Saving model checkpoint to C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_eval\n",
      "[INFO|configuration_utils.py:447] 2022-10-30 04:26:58,455 >> Configuration saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_eval\\config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-30 04:26:59,166 >> Model weights saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_eval\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2022-10-30 04:26:59,167 >> tokenizer config file saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_eval\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2022-10-30 04:26:59,168 >> Special tokens file saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_eval\\special_tokens_map.json\n",
      "[INFO|trainer.py:725] 2022-10-30 04:26:59,439 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:26:59,441 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:26:59,441 >>   Num examples = 852\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:26:59,441 >>   Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  total_flos               =   457439GF\n",
      "  train_loss               =     0.8028\n",
      "  train_runtime            = 0:02:24.25\n",
      "  train_samples            =       5965\n",
      "  train_samples_per_second =    206.746\n",
      "  train_steps_per_second   =      6.481\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089658b3e3784596ae467660329e3083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:02,152 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:02,153 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:02,154 >>   Num examples = 599\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:02,154 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c457e2f87d24663a0dc7c27dfe0db0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:03,672 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:03,674 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:03,674 >>   Num examples = 165\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:03,674 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3b8473485d4647a0b14c79a20d7846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:06,660 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:06,662 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:06,662 >>   Num examples = 1417\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:06,662 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec707c4401e466085be17416c826c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:11,379 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:11,380 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:11,380 >>   Num examples = 1019\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:11,381 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d835ae387ef343419f92a11e43edf59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:14,532 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:14,534 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:14,534 >>   Num examples = 558\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:14,534 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74cca34fda441458d03ad4a541452b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:16,573 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:16,574 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:16,574 >>   Num examples = 512\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:16,574 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74232e62e2e416eb2c1f7ec02d65482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:18,146 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:18,147 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:18,147 >>   Num examples = 306\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:18,148 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc871dfbcd740e1aa88e0541989836e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:19,115 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:19,117 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:19,117 >>   Num examples = 181\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:19,118 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50166062a18d454d80a2fbdd0b70aa83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:725] 2022-10-30 04:27:21,217 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:27:21,219 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:27:21,219 >>   Num examples = 852\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:27:21,219 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6754704067024604\n",
      "***** eval metrics *****\n",
      "  eval_accuracy           =     0.6737\n",
      "  eval_loss               =     0.7884\n",
      "  eval_runtime            = 0:00:01.53\n",
      "  eval_samples_per_second =    556.602\n",
      "  eval_steps_per_second   =     69.902\n"
     ]
    }
   ],
   "source": [
    "# Get the metric function\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
    "# predictions and label_ids field) and has to return a dictionary string to float.\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return metric.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "# Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n",
    "if data_args.pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "elif training_args.fp16:\n",
    "    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "else:\n",
    "    data_collator = None\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Training\n",
    "if training_args.do_train:\n",
    "    checkpoint = None\n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    metrics = train_result.metrics\n",
    "    max_train_samples = (\n",
    "        data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "    )\n",
    "    metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "    trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "# Evaluation\n",
    "if training_args.do_eval or True:\n",
    "    logger.info(\"*** Evaluate ***\")\n",
    "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "\n",
    "    max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "    metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "    \n",
    "    \n",
    "    splitted_A = os.path.join(PROJECT_DIR, 'SubtaskA', 'train', 'splitted-train-dev-test')\n",
    "    \n",
    "    try:\n",
    "        LANGUAGE_CODE\n",
    "    except NameError:\n",
    "        LANGUAGE_CODE = 'combined'\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    data = []\n",
    "    for lang in languages:\n",
    "        eval_path = os.path.join(splitted_A, lang)\n",
    "        df = pd.read_csv(eval_path + '/dev.tsv', sep='\\t')\n",
    "        df = df.dropna()\n",
    "        lang_eval = Dataset.from_pandas(df)\n",
    "        lang_eval = lang_eval.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )\n",
    "        \n",
    "        predictions, labels, metrics = trainer.predict(lang_eval, metric_key_prefix=\"eval\")\n",
    "        \n",
    "        if LANGUAGE_CODE == lang:\n",
    "            print(f1_score(np.argmax(predictions, axis=1), labels, average='weighted'))\n",
    "        \n",
    "        data.append([LANGUAGE_CODE, lang, str(list(predictions)), str(list(labels))])\n",
    "    df = pd.DataFrame(data, columns=['source', 'target', 'predictions', 'labels'])\n",
    "    df.to_csv(f'{LANGUAGE_CODE}_preds.csv', index=False)\n",
    "\n",
    "\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "# Prediction\n",
    "if training_args.do_predict:\n",
    "    logger.info(\"*** Predict ***\")\n",
    "    predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n",
    "\n",
    "    max_predict_samples = (\n",
    "        data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n",
    "    )\n",
    "    metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n",
    "\n",
    "    trainer.log_metrics(\"predict\", metrics)\n",
    "    trainer.save_metrics(\"predict\", metrics)\n",
    "\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    output_predict_file = os.path.join(training_args.output_dir, \"predictions.txt\")\n",
    "    if trainer.is_world_process_zero():\n",
    "        with open(output_predict_file, \"w\") as writer:\n",
    "            writer.write(\"index\\tprediction\\n\")\n",
    "            for index, item in enumerate(predictions):\n",
    "                item = label_list[item]\n",
    "                writer.write(f\"{index}\\t{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h06uE1yLGAG_"
   },
   "source": [
    "#### KinyaBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1EGi2jpx5BvV",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/30/2022 04:27:27 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "10/30/2022 04:27:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=128.0,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\\runs\\Oct30_04-27-27_LAPTOP-42G56F8J,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=32,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test,\n",
      "save_on_each_node=False,\n",
      "save_steps=-1,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "['positive', 'neutral', 'negative']\n",
      "10/30/2022 04:27:28 - INFO - __main__ - Sample 204 of the training set: {'text': 'RT @user: TAKE IT #SIckleCellAwareness in #Yoruba Osu kesan, osu #september je osu pataki fun wa gegebi awujo. Idi ni pe, a maa n fi', 'label': 1, 'input_ids': [2, 1, 1, 26684, 1, 1, 1, 1, 1, 128, 1, 1, 1, 993, 223, 6, 10322, 50, 1, 17249, 675, 10322, 50, 3468, 23792, 8919, 139, 845, 107, 109, 6031, 741, 7, 1, 157, 319, 6, 21, 20560, 34, 708, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
      "{'loss': 13408.548, 'learning_rate': 59.55080213903744, 'epoch': 2.67}\n",
      "{'train_runtime': 166.0997, 'train_samples_per_second': 179.561, 'train_steps_per_second': 5.629, 'train_loss': 9198.849331550802, 'epoch': 5.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  train_loss               =  9198.8493\n",
      "  train_runtime            = 0:02:46.09\n",
      "  train_samples            =       5965\n",
      "  train_samples_per_second =    179.561\n",
      "  train_steps_per_second   =      5.629\n",
      "10/30/2022 04:30:15 - INFO - __main__ - *** Evaluate ***\n",
      "f1 score: 0.5510204081632654\n",
      "***** eval metrics *****\n",
      "  eval_accuracy           =     0.3803\n",
      "  eval_loss               =    251.986\n",
      "  eval_runtime            = 0:00:01.74\n",
      "  eval_samples_per_second =    487.025\n",
      "  eval_steps_per_second   =     61.164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[INFO|configuration_utils.py:653] 2022-10-30 04:27:27,296 >> loading configuration file config.json from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-30 04:27:27,300 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"jean-paul/KinyaBERT-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:418] 2022-10-30 04:27:27,413 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:653] 2022-10-30 04:27:27,521 >> loading configuration file config.json from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-30 04:27:27,522 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"jean-paul/KinyaBERT-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-30 04:27:27,743 >> loading file vocab.txt from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-30 04:27:27,743 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-30 04:27:27,743 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-30 04:27:27,743 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2022-10-30 04:27:27,743 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:653] 2022-10-30 04:27:27,744 >> loading configuration file config.json from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-30 04:27:27,744 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"jean-paul/KinyaBERT-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:653] 2022-10-30 04:27:27,759 >> loading configuration file config.json from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\config.json\n",
      "[INFO|configuration_utils.py:705] 2022-10-30 04:27:27,759 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"jean-paul/KinyaBERT-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2156] 2022-10-30 04:27:27,811 >> loading weights file pytorch_model.bin from cache at C:\\Users\\Thomas/.cache\\huggingface\\hub\\models--jean-paul--KinyaBERT-small\\snapshots\\4575f11375376ae29a8b19610f70c07ee04d02eb\\pytorch_model.bin\n",
      "[WARNING|modeling_utils.py:2596] 2022-10-30 04:27:28,184 >> Some weights of the model checkpoint at jean-paul/KinyaBERT-small were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2608] 2022-10-30 04:27:28,184 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at jean-paul/KinyaBERT-small and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/6 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  33%|###3      | 2/6 [00:00<00:00, 13.69ba/s]\n",
      "Running tokenizer on train dataset:  67%|######6   | 4/6 [00:00<00:00, 14.45ba/s]\n",
      "Running tokenizer on train dataset:  83%|########3 | 5/6 [00:00<00:00, 12.34ba/s]\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Thomas\\anaconda3\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Thomas\\anaconda3\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\U0001f914' in position 124: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\starter_kit\\run_textclass.py\", line 456, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\starter_kit\\run_textclass.py\", line 342, in main\n",
      "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
      "Message: \"Sample 5238 of the training set: {'text': 'Nje iwo mo awon akoko ojo ni ede yoruba? \\U0001f914 #akokoojo #edeyoruba #yoruba #lagelufm967 https://t.co/N7p9slLWfm', 'label': 1, 'input_ids': [2, 1, 29, 160, 503, 29942, 167, 1518, 95, 35, 741, 157, 4677, 63, 205, 11030, 19, 1, 1, 1518, 95, 56, 741, 1, 4677, 63, 105, 11030, 1, 205, 11030, 1, 660, 16922, 50, 49, 57, 15937, 80, 22993, 64, 6606, 1, 1, 1, 40, 7, 279, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\"\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Thomas\\anaconda3\\lib\\logging\\__init__.py\", line 1086, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\Thomas\\anaconda3\\lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u1eb9' in position 142: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\starter_kit\\run_textclass.py\", line 456, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\Thomas\\Downloads\\afrisent\\afrisent-semeval-2023\\starter_kit\\run_textclass.py\", line 342, in main\n",
      "    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
      "Message: \"Sample 912 of the training set: {'text': 'RT @user: ... Jjde wa k wa ma pd gbk. Ohun t a  j\\u1eb9 l \\u0144 w l\\u1ecd Bb, k m pd ohun t y j\\u1eb9 w. #Iwure #OjoAje #Yoruba', 'label': 0, 'input_ids': [2, 1, 1, 26684, 1, 7, 7, 7, 1, 139, 1, 139, 1, 1, 1, 7, 1, 1, 21, 1, 1, 1, 1, 1, 1, 1, 6, 1, 1, 1, 35, 148, 51, 1, 1, 1, 1, 7, 1, 1, 1, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\"\n",
      "Arguments: ()\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "[INFO|trainer.py:725] 2022-10-30 04:27:29,112 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1607] 2022-10-30 04:27:29,120 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2022-10-30 04:27:29,120 >>   Num examples = 5965\n",
      "[INFO|trainer.py:1609] 2022-10-30 04:27:29,120 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:1610] 2022-10-30 04:27:29,120 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1611] 2022-10-30 04:27:29,120 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1612] 2022-10-30 04:27:29,121 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2022-10-30 04:27:29,121 >>   Total optimization steps = 935\n",
      "\n",
      "  0%|          | 0/935 [00:00<?, ?it/s]\n",
      "  0%|          | 1/935 [00:01<25:44,  1.65s/it]\n",
      "  0%|          | 2/935 [00:02<13:55,  1.12it/s]\n",
      "  0%|          | 3/935 [00:02<08:47,  1.77it/s]\n",
      "  0%|          | 4/935 [00:02<06:22,  2.43it/s]\n",
      "  1%|          | 5/935 [00:02<05:03,  3.07it/s]\n",
      "  1%|          | 6/935 [00:02<04:15,  3.64it/s]\n",
      "  1%|          | 7/935 [00:02<03:44,  4.14it/s]\n",
      "  1%|          | 8/935 [00:03<03:24,  4.52it/s]\n",
      "  1%|          | 9/935 [00:03<03:11,  4.85it/s]\n",
      "  1%|1         | 10/935 [00:03<03:02,  5.07it/s]\n",
      "  1%|1         | 11/935 [00:03<02:55,  5.27it/s]\n",
      "  1%|1         | 12/935 [00:03<02:51,  5.39it/s]\n",
      "  1%|1         | 13/935 [00:03<02:48,  5.49it/s]\n",
      "  1%|1         | 14/935 [00:04<02:46,  5.52it/s]\n",
      "  2%|1         | 15/935 [00:04<02:44,  5.59it/s]\n",
      "  2%|1         | 16/935 [00:04<02:43,  5.62it/s]\n",
      "  2%|1         | 17/935 [00:04<02:42,  5.65it/s]\n",
      "  2%|1         | 18/935 [00:04<02:42,  5.63it/s]\n",
      "  2%|2         | 19/935 [00:04<02:42,  5.65it/s]\n",
      "  2%|2         | 20/935 [00:05<02:41,  5.65it/s]\n",
      "  2%|2         | 21/935 [00:05<02:41,  5.66it/s]\n",
      "  2%|2         | 22/935 [00:05<02:41,  5.64it/s]\n",
      "  2%|2         | 23/935 [00:05<02:41,  5.65it/s]\n",
      "  3%|2         | 24/935 [00:05<02:40,  5.67it/s]\n",
      "  3%|2         | 25/935 [00:06<02:40,  5.67it/s]\n",
      "  3%|2         | 26/935 [00:06<02:39,  5.70it/s]\n",
      "  3%|2         | 27/935 [00:06<02:39,  5.71it/s]\n",
      "  3%|2         | 28/935 [00:06<02:38,  5.71it/s]\n",
      "  3%|3         | 29/935 [00:06<02:39,  5.69it/s]\n",
      "  3%|3         | 30/935 [00:06<02:38,  5.71it/s]\n",
      "  3%|3         | 31/935 [00:07<02:38,  5.69it/s]\n",
      "  3%|3         | 32/935 [00:07<02:37,  5.72it/s]\n",
      "  4%|3         | 33/935 [00:07<02:38,  5.70it/s]\n",
      "  4%|3         | 34/935 [00:07<02:37,  5.71it/s]\n",
      "  4%|3         | 35/935 [00:07<02:37,  5.70it/s]\n",
      "  4%|3         | 36/935 [00:07<02:37,  5.70it/s]\n",
      "  4%|3         | 37/935 [00:08<02:38,  5.66it/s]\n",
      "  4%|4         | 38/935 [00:08<02:38,  5.68it/s]\n",
      "  4%|4         | 39/935 [00:08<02:38,  5.65it/s]\n",
      "  4%|4         | 40/935 [00:08<02:37,  5.69it/s]\n",
      "  4%|4         | 41/935 [00:08<02:37,  5.69it/s]\n",
      "  4%|4         | 42/935 [00:09<02:36,  5.70it/s]\n",
      "  5%|4         | 43/935 [00:09<02:36,  5.68it/s]\n",
      "  5%|4         | 44/935 [00:09<02:36,  5.69it/s]\n",
      "  5%|4         | 45/935 [00:09<02:35,  5.71it/s]\n",
      "  5%|4         | 46/935 [00:09<02:35,  5.70it/s]\n",
      "  5%|5         | 47/935 [00:09<02:35,  5.70it/s]\n",
      "  5%|5         | 48/935 [00:10<02:35,  5.70it/s]\n",
      "  5%|5         | 49/935 [00:10<02:34,  5.72it/s]\n",
      "  5%|5         | 50/935 [00:10<02:35,  5.71it/s]\n",
      "  5%|5         | 51/935 [00:10<02:34,  5.72it/s]\n",
      "  6%|5         | 52/935 [00:10<02:34,  5.70it/s]\n",
      "  6%|5         | 53/935 [00:10<02:34,  5.71it/s]\n",
      "  6%|5         | 54/935 [00:11<02:34,  5.70it/s]\n",
      "  6%|5         | 55/935 [00:11<02:34,  5.71it/s]\n",
      "  6%|5         | 56/935 [00:11<02:34,  5.68it/s]\n",
      "  6%|6         | 57/935 [00:11<02:33,  5.71it/s]\n",
      "  6%|6         | 58/935 [00:11<02:33,  5.71it/s]\n",
      "  6%|6         | 59/935 [00:12<02:33,  5.72it/s]\n",
      "  6%|6         | 60/935 [00:12<02:33,  5.70it/s]\n",
      "  7%|6         | 61/935 [00:12<02:32,  5.72it/s]\n",
      "  7%|6         | 62/935 [00:12<02:32,  5.71it/s]\n",
      "  7%|6         | 63/935 [00:12<02:33,  5.68it/s]\n",
      "  7%|6         | 64/935 [00:12<02:33,  5.66it/s]\n",
      "  7%|6         | 65/935 [00:13<02:33,  5.68it/s]\n",
      "  7%|7         | 66/935 [00:13<02:33,  5.67it/s]\n",
      "  7%|7         | 67/935 [00:13<02:32,  5.69it/s]\n",
      "  7%|7         | 68/935 [00:13<02:32,  5.67it/s]\n",
      "  7%|7         | 69/935 [00:13<02:32,  5.68it/s]\n",
      "  7%|7         | 70/935 [00:13<02:31,  5.70it/s]\n",
      "  8%|7         | 71/935 [00:14<02:31,  5.70it/s]\n",
      "  8%|7         | 72/935 [00:14<02:30,  5.73it/s]\n",
      "  8%|7         | 73/935 [00:14<02:31,  5.70it/s]\n",
      "  8%|7         | 74/935 [00:14<02:30,  5.71it/s]\n",
      "  8%|8         | 75/935 [00:14<02:30,  5.72it/s]\n",
      "  8%|8         | 76/935 [00:15<02:29,  5.74it/s]\n",
      "  8%|8         | 77/935 [00:15<02:30,  5.70it/s]\n",
      "  8%|8         | 78/935 [00:15<02:30,  5.70it/s]\n",
      "  8%|8         | 79/935 [00:15<02:30,  5.70it/s]\n",
      "  9%|8         | 80/935 [00:15<02:29,  5.71it/s]\n",
      "  9%|8         | 81/935 [00:15<02:30,  5.69it/s]\n",
      "  9%|8         | 82/935 [00:16<02:29,  5.70it/s]\n",
      "  9%|8         | 83/935 [00:16<02:29,  5.68it/s]\n",
      "  9%|8         | 84/935 [00:16<02:29,  5.70it/s]\n",
      "  9%|9         | 85/935 [00:16<02:28,  5.71it/s]\n",
      "  9%|9         | 86/935 [00:16<02:28,  5.70it/s]\n",
      "  9%|9         | 87/935 [00:16<02:29,  5.67it/s]\n",
      "  9%|9         | 88/935 [00:17<02:29,  5.67it/s]\n",
      " 10%|9         | 89/935 [00:17<02:29,  5.65it/s]\n",
      " 10%|9         | 90/935 [00:17<02:28,  5.68it/s]\n",
      " 10%|9         | 91/935 [00:17<02:28,  5.67it/s]\n",
      " 10%|9         | 92/935 [00:17<02:28,  5.68it/s]\n",
      " 10%|9         | 93/935 [00:17<02:28,  5.65it/s]\n",
      " 10%|#         | 94/935 [00:18<02:28,  5.66it/s]\n",
      " 10%|#         | 95/935 [00:18<02:27,  5.69it/s]\n",
      " 10%|#         | 96/935 [00:18<02:27,  5.69it/s]\n",
      " 10%|#         | 97/935 [00:18<02:27,  5.70it/s]\n",
      " 10%|#         | 98/935 [00:18<02:26,  5.69it/s]\n",
      " 11%|#         | 99/935 [00:19<02:26,  5.71it/s]\n",
      " 11%|#         | 100/935 [00:19<02:26,  5.70it/s]\n",
      " 11%|#         | 101/935 [00:19<02:25,  5.71it/s]\n",
      " 11%|#         | 102/935 [00:19<02:26,  5.69it/s]\n",
      " 11%|#1        | 103/935 [00:19<02:25,  5.72it/s]\n",
      " 11%|#1        | 104/935 [00:19<02:25,  5.70it/s]\n",
      " 11%|#1        | 105/935 [00:20<02:25,  5.72it/s]\n",
      " 11%|#1        | 106/935 [00:20<02:25,  5.70it/s]\n",
      " 11%|#1        | 107/935 [00:20<02:24,  5.72it/s]\n",
      " 12%|#1        | 108/935 [00:20<02:24,  5.71it/s]\n",
      " 12%|#1        | 109/935 [00:20<02:24,  5.73it/s]\n",
      " 12%|#1        | 110/935 [00:20<02:24,  5.69it/s]\n",
      " 12%|#1        | 111/935 [00:21<02:24,  5.71it/s]\n",
      " 12%|#1        | 112/935 [00:21<02:25,  5.67it/s]\n",
      " 12%|#2        | 113/935 [00:21<02:24,  5.69it/s]\n",
      " 12%|#2        | 114/935 [00:21<02:25,  5.66it/s]\n",
      " 12%|#2        | 115/935 [00:21<02:24,  5.66it/s]\n",
      " 12%|#2        | 116/935 [00:22<02:25,  5.64it/s]\n",
      " 13%|#2        | 117/935 [00:22<02:24,  5.66it/s]\n",
      " 13%|#2        | 118/935 [00:22<02:24,  5.67it/s]\n",
      " 13%|#2        | 119/935 [00:22<02:23,  5.69it/s]\n",
      " 13%|#2        | 120/935 [00:22<02:22,  5.71it/s]\n",
      " 13%|#2        | 121/935 [00:22<02:23,  5.68it/s]\n",
      " 13%|#3        | 122/935 [00:23<02:22,  5.70it/s]\n",
      " 13%|#3        | 123/935 [00:23<02:22,  5.70it/s]\n",
      " 13%|#3        | 124/935 [00:23<02:21,  5.73it/s]\n",
      " 13%|#3        | 125/935 [00:23<02:22,  5.69it/s]\n",
      " 13%|#3        | 126/935 [00:23<02:21,  5.70it/s]\n",
      " 14%|#3        | 127/935 [00:23<02:21,  5.70it/s]\n",
      " 14%|#3        | 128/935 [00:24<02:21,  5.70it/s]\n",
      " 14%|#3        | 129/935 [00:24<02:21,  5.68it/s]\n",
      " 14%|#3        | 130/935 [00:24<02:21,  5.69it/s]\n",
      " 14%|#4        | 131/935 [00:24<02:21,  5.67it/s]\n",
      " 14%|#4        | 132/935 [00:24<02:20,  5.70it/s]\n",
      " 14%|#4        | 133/935 [00:25<02:21,  5.68it/s]\n",
      " 14%|#4        | 134/935 [00:25<02:20,  5.69it/s]\n",
      " 14%|#4        | 135/935 [00:25<02:20,  5.68it/s]\n",
      " 15%|#4        | 136/935 [00:25<02:20,  5.69it/s]\n",
      " 15%|#4        | 137/935 [00:25<02:21,  5.66it/s]\n",
      " 15%|#4        | 138/935 [00:25<02:20,  5.68it/s]\n",
      " 15%|#4        | 139/935 [00:26<02:21,  5.64it/s]\n",
      " 15%|#4        | 140/935 [00:26<02:20,  5.66it/s]\n",
      " 15%|#5        | 141/935 [00:26<02:20,  5.63it/s]\n",
      " 15%|#5        | 142/935 [00:26<02:20,  5.65it/s]\n",
      " 15%|#5        | 143/935 [00:26<02:19,  5.67it/s]\n",
      " 15%|#5        | 144/935 [00:26<02:19,  5.68it/s]\n",
      " 16%|#5        | 145/935 [00:27<02:18,  5.70it/s]\n",
      " 16%|#5        | 146/935 [00:27<02:18,  5.68it/s]\n",
      " 16%|#5        | 147/935 [00:27<02:18,  5.70it/s]\n",
      " 16%|#5        | 148/935 [00:27<02:18,  5.70it/s]\n",
      " 16%|#5        | 149/935 [00:27<02:17,  5.72it/s]\n",
      " 16%|#6        | 150/935 [00:28<02:17,  5.69it/s]\n",
      " 16%|#6        | 151/935 [00:28<02:17,  5.70it/s]\n",
      " 16%|#6        | 152/935 [00:28<02:17,  5.69it/s]\n",
      " 16%|#6        | 153/935 [00:28<02:17,  5.70it/s]\n",
      " 16%|#6        | 154/935 [00:28<02:17,  5.66it/s]\n",
      " 17%|#6        | 155/935 [00:28<02:17,  5.67it/s]\n",
      " 17%|#6        | 156/935 [00:29<02:17,  5.66it/s]\n",
      " 17%|#6        | 157/935 [00:29<02:16,  5.69it/s]\n",
      " 17%|#6        | 158/935 [00:29<02:17,  5.66it/s]\n",
      " 17%|#7        | 159/935 [00:29<02:16,  5.68it/s]\n",
      " 17%|#7        | 160/935 [00:29<02:17,  5.63it/s]\n",
      " 17%|#7        | 161/935 [00:29<02:16,  5.66it/s]\n",
      " 17%|#7        | 162/935 [00:30<02:16,  5.65it/s]\n",
      " 17%|#7        | 163/935 [00:30<02:16,  5.64it/s]\n",
      " 18%|#7        | 164/935 [00:30<02:16,  5.66it/s]\n",
      " 18%|#7        | 165/935 [00:30<02:15,  5.69it/s]\n",
      " 18%|#7        | 166/935 [00:30<02:14,  5.70it/s]\n",
      " 18%|#7        | 167/935 [00:31<02:14,  5.70it/s]\n",
      " 18%|#7        | 168/935 [00:31<02:14,  5.70it/s]\n",
      " 18%|#8        | 169/935 [00:31<02:14,  5.71it/s]\n",
      " 18%|#8        | 170/935 [00:31<02:13,  5.72it/s]\n",
      " 18%|#8        | 171/935 [00:31<02:14,  5.70it/s]\n",
      " 18%|#8        | 172/935 [00:31<02:13,  5.71it/s]\n",
      " 19%|#8        | 173/935 [00:32<02:13,  5.71it/s]\n",
      " 19%|#8        | 174/935 [00:32<02:12,  5.73it/s]\n",
      " 19%|#8        | 175/935 [00:32<02:13,  5.71it/s]\n",
      " 19%|#8        | 176/935 [00:32<02:12,  5.71it/s]\n",
      " 19%|#8        | 177/935 [00:32<02:12,  5.70it/s]\n",
      " 19%|#9        | 178/935 [00:32<02:12,  5.72it/s]\n",
      " 19%|#9        | 179/935 [00:33<02:13,  5.67it/s]\n",
      " 19%|#9        | 180/935 [00:33<02:12,  5.68it/s]\n",
      " 19%|#9        | 181/935 [00:33<02:13,  5.64it/s]\n",
      " 19%|#9        | 182/935 [00:33<02:12,  5.67it/s]\n",
      " 20%|#9        | 183/935 [00:33<02:12,  5.66it/s]\n",
      " 20%|#9        | 184/935 [00:33<02:12,  5.68it/s]\n",
      " 20%|#9        | 185/935 [00:34<02:13,  5.62it/s]\n",
      " 20%|#9        | 186/935 [00:34<02:12,  5.64it/s]\n",
      " 20%|##        | 188/935 [00:34<01:56,  6.41it/s]\n",
      " 20%|##        | 189/935 [00:34<02:00,  6.20it/s]\n",
      " 20%|##        | 190/935 [00:34<02:02,  6.08it/s]\n",
      " 20%|##        | 191/935 [00:35<02:05,  5.94it/s]\n",
      " 21%|##        | 192/935 [00:35<02:06,  5.87it/s]\n",
      " 21%|##        | 193/935 [00:35<02:07,  5.80it/s]\n",
      " 21%|##        | 194/935 [00:35<02:08,  5.78it/s]\n",
      " 21%|##        | 195/935 [00:35<02:09,  5.72it/s]\n",
      " 21%|##        | 196/935 [00:36<02:09,  5.71it/s]\n",
      " 21%|##1       | 197/935 [00:36<02:09,  5.68it/s]\n",
      " 21%|##1       | 198/935 [00:36<02:09,  5.69it/s]\n",
      " 21%|##1       | 199/935 [00:36<02:09,  5.70it/s]\n",
      " 21%|##1       | 200/935 [00:36<02:09,  5.69it/s]\n",
      " 21%|##1       | 201/935 [00:36<02:09,  5.69it/s]\n",
      " 22%|##1       | 202/935 [00:37<02:09,  5.68it/s]\n",
      " 22%|##1       | 203/935 [00:37<02:08,  5.70it/s]\n",
      " 22%|##1       | 204/935 [00:37<02:08,  5.70it/s]\n",
      " 22%|##1       | 205/935 [00:37<02:07,  5.71it/s]\n",
      " 22%|##2       | 206/935 [00:37<02:08,  5.69it/s]\n",
      " 22%|##2       | 207/935 [00:37<02:07,  5.72it/s]\n",
      " 22%|##2       | 208/935 [00:38<02:07,  5.70it/s]\n",
      " 22%|##2       | 209/935 [00:38<02:07,  5.71it/s]\n",
      " 22%|##2       | 210/935 [00:38<02:07,  5.69it/s]\n",
      " 23%|##2       | 211/935 [00:38<02:07,  5.70it/s]\n",
      " 23%|##2       | 212/935 [00:38<02:07,  5.69it/s]\n",
      " 23%|##2       | 213/935 [00:39<02:06,  5.72it/s]\n",
      " 23%|##2       | 214/935 [00:39<02:06,  5.68it/s]\n",
      " 23%|##2       | 215/935 [00:39<02:06,  5.69it/s]\n",
      " 23%|##3       | 216/935 [00:39<02:06,  5.67it/s]\n",
      " 23%|##3       | 217/935 [00:39<02:06,  5.68it/s]\n",
      " 23%|##3       | 218/935 [00:39<02:06,  5.66it/s]\n",
      " 23%|##3       | 219/935 [00:40<02:05,  5.69it/s]\n",
      " 24%|##3       | 220/935 [00:40<02:06,  5.67it/s]\n",
      " 24%|##3       | 221/935 [00:40<02:06,  5.66it/s]\n",
      " 24%|##3       | 222/935 [00:40<02:05,  5.69it/s]\n",
      " 24%|##3       | 223/935 [00:40<02:05,  5.68it/s]\n",
      " 24%|##3       | 224/935 [00:40<02:04,  5.70it/s]\n",
      " 24%|##4       | 225/935 [00:41<02:04,  5.69it/s]\n",
      " 24%|##4       | 226/935 [00:41<02:04,  5.70it/s]\n",
      " 24%|##4       | 227/935 [00:41<02:04,  5.71it/s]\n",
      " 24%|##4       | 228/935 [00:41<02:03,  5.72it/s]\n",
      " 24%|##4       | 229/935 [00:41<02:04,  5.69it/s]\n",
      " 25%|##4       | 230/935 [00:42<02:03,  5.70it/s]\n",
      " 25%|##4       | 231/935 [00:42<02:03,  5.70it/s]\n",
      " 25%|##4       | 232/935 [00:42<02:03,  5.71it/s]\n",
      " 25%|##4       | 233/935 [00:42<02:03,  5.66it/s]\n",
      " 25%|##5       | 234/935 [00:42<02:03,  5.68it/s]\n",
      " 25%|##5       | 235/935 [00:42<02:04,  5.64it/s]\n",
      " 25%|##5       | 236/935 [00:43<02:03,  5.67it/s]\n",
      " 25%|##5       | 237/935 [00:43<02:03,  5.67it/s]\n",
      " 25%|##5       | 238/935 [00:43<02:02,  5.69it/s]\n",
      " 26%|##5       | 239/935 [00:43<02:03,  5.64it/s]\n",
      " 26%|##5       | 240/935 [00:43<02:02,  5.67it/s]\n",
      " 26%|##5       | 241/935 [00:43<02:03,  5.63it/s]\n",
      " 26%|##5       | 242/935 [00:44<02:03,  5.63it/s]\n",
      " 26%|##5       | 243/935 [00:44<02:03,  5.63it/s]\n",
      " 26%|##6       | 244/935 [00:44<02:02,  5.65it/s]\n",
      " 26%|##6       | 245/935 [00:44<02:01,  5.67it/s]\n",
      " 26%|##6       | 246/935 [00:44<02:01,  5.67it/s]\n",
      " 26%|##6       | 247/935 [00:45<02:00,  5.69it/s]\n",
      " 27%|##6       | 248/935 [00:45<02:00,  5.68it/s]\n",
      " 27%|##6       | 249/935 [00:45<02:00,  5.69it/s]\n",
      " 27%|##6       | 250/935 [00:45<02:00,  5.71it/s]\n",
      " 27%|##6       | 251/935 [00:45<02:00,  5.70it/s]\n",
      " 27%|##6       | 252/935 [00:45<02:00,  5.68it/s]\n",
      " 27%|##7       | 253/935 [00:46<01:59,  5.70it/s]\n",
      " 27%|##7       | 254/935 [00:46<01:59,  5.70it/s]\n",
      " 27%|##7       | 255/935 [00:46<01:58,  5.72it/s]\n",
      " 27%|##7       | 256/935 [00:46<01:59,  5.69it/s]\n",
      " 27%|##7       | 257/935 [00:46<01:58,  5.70it/s]\n",
      " 28%|##7       | 258/935 [00:46<01:58,  5.70it/s]\n",
      " 28%|##7       | 259/935 [00:47<01:58,  5.69it/s]\n",
      " 28%|##7       | 260/935 [00:47<01:59,  5.66it/s]\n",
      " 28%|##7       | 261/935 [00:47<01:58,  5.67it/s]\n",
      " 28%|##8       | 262/935 [00:47<01:59,  5.65it/s]\n",
      " 28%|##8       | 263/935 [00:47<01:58,  5.66it/s]\n",
      " 28%|##8       | 264/935 [00:47<01:58,  5.67it/s]\n",
      " 28%|##8       | 265/935 [00:48<01:57,  5.69it/s]\n",
      " 28%|##8       | 266/935 [00:48<01:58,  5.65it/s]\n",
      " 29%|##8       | 267/935 [00:48<01:58,  5.66it/s]\n",
      " 29%|##8       | 268/935 [00:48<01:57,  5.67it/s]\n",
      " 29%|##8       | 269/935 [00:48<01:57,  5.69it/s]\n",
      " 29%|##8       | 270/935 [00:49<01:56,  5.70it/s]\n",
      " 29%|##8       | 271/935 [00:49<01:56,  5.68it/s]\n",
      " 29%|##9       | 272/935 [00:49<01:56,  5.70it/s]\n",
      " 29%|##9       | 273/935 [00:49<01:56,  5.69it/s]\n",
      " 29%|##9       | 274/935 [00:49<01:55,  5.70it/s]\n",
      " 29%|##9       | 275/935 [00:49<01:56,  5.68it/s]\n",
      " 30%|##9       | 276/935 [00:50<01:56,  5.67it/s]\n",
      " 30%|##9       | 277/935 [00:50<01:56,  5.66it/s]\n",
      " 30%|##9       | 278/935 [00:50<01:55,  5.68it/s]\n",
      " 30%|##9       | 279/935 [00:50<01:55,  5.69it/s]\n",
      " 30%|##9       | 280/935 [00:50<01:54,  5.71it/s]\n",
      " 30%|###       | 281/935 [00:50<01:55,  5.68it/s]\n",
      " 30%|###       | 282/935 [00:51<01:54,  5.69it/s]\n",
      " 30%|###       | 283/935 [00:51<01:55,  5.66it/s]\n",
      " 30%|###       | 284/935 [00:51<01:54,  5.67it/s]\n",
      " 30%|###       | 285/935 [00:51<01:55,  5.65it/s]\n",
      " 31%|###       | 286/935 [00:51<01:54,  5.67it/s]\n",
      " 31%|###       | 287/935 [00:52<01:54,  5.64it/s]\n",
      " 31%|###       | 288/935 [00:52<01:54,  5.65it/s]\n",
      " 31%|###       | 289/935 [00:52<01:54,  5.62it/s]\n",
      " 31%|###1      | 290/935 [00:52<01:54,  5.63it/s]\n",
      " 31%|###1      | 291/935 [00:52<01:54,  5.65it/s]\n",
      " 31%|###1      | 292/935 [00:52<01:53,  5.68it/s]\n",
      " 31%|###1      | 293/935 [00:53<01:52,  5.69it/s]\n",
      " 31%|###1      | 294/935 [00:53<01:52,  5.68it/s]\n",
      " 32%|###1      | 295/935 [00:53<01:52,  5.69it/s]\n",
      " 32%|###1      | 296/935 [00:53<01:52,  5.69it/s]\n",
      " 32%|###1      | 297/935 [00:53<01:51,  5.70it/s]\n",
      " 32%|###1      | 298/935 [00:53<01:52,  5.68it/s]\n",
      " 32%|###1      | 299/935 [00:54<01:51,  5.69it/s]\n",
      " 32%|###2      | 300/935 [00:54<01:51,  5.68it/s]\n",
      " 32%|###2      | 301/935 [00:54<01:51,  5.70it/s]\n",
      " 32%|###2      | 302/935 [00:54<01:51,  5.69it/s]\n",
      " 32%|###2      | 303/935 [00:54<01:50,  5.71it/s]\n",
      " 33%|###2      | 304/935 [00:55<01:51,  5.67it/s]\n",
      " 33%|###2      | 305/935 [00:55<01:50,  5.68it/s]\n",
      " 33%|###2      | 306/935 [00:55<01:50,  5.67it/s]\n",
      " 33%|###2      | 307/935 [00:55<01:50,  5.69it/s]\n",
      " 33%|###2      | 308/935 [00:55<01:51,  5.64it/s]\n",
      " 33%|###3      | 309/935 [00:55<01:50,  5.65it/s]\n",
      " 33%|###3      | 310/935 [00:56<01:51,  5.62it/s]\n",
      " 33%|###3      | 311/935 [00:56<01:51,  5.62it/s]\n",
      " 33%|###3      | 312/935 [00:56<01:50,  5.65it/s]\n",
      " 33%|###3      | 313/935 [00:56<01:49,  5.67it/s]\n",
      " 34%|###3      | 314/935 [00:56<01:49,  5.69it/s]\n",
      " 34%|###3      | 315/935 [00:56<01:49,  5.68it/s]\n",
      " 34%|###3      | 316/935 [00:57<01:48,  5.69it/s]\n",
      " 34%|###3      | 317/935 [00:57<01:48,  5.69it/s]\n",
      " 34%|###4      | 318/935 [00:57<01:48,  5.69it/s]\n",
      " 34%|###4      | 319/935 [00:57<01:48,  5.69it/s]\n",
      " 34%|###4      | 320/935 [00:57<01:47,  5.70it/s]\n",
      " 34%|###4      | 321/935 [00:58<01:48,  5.67it/s]\n",
      " 34%|###4      | 322/935 [00:58<01:47,  5.70it/s]\n",
      " 35%|###4      | 323/935 [00:58<01:47,  5.69it/s]\n",
      " 35%|###4      | 324/935 [00:58<01:47,  5.71it/s]\n",
      " 35%|###4      | 325/935 [00:58<01:47,  5.68it/s]\n",
      " 35%|###4      | 326/935 [00:58<01:47,  5.68it/s]\n",
      " 35%|###4      | 327/935 [00:59<01:47,  5.64it/s]\n",
      " 35%|###5      | 328/935 [00:59<01:47,  5.67it/s]\n",
      " 35%|###5      | 329/935 [00:59<01:46,  5.67it/s]\n",
      " 35%|###5      | 330/935 [00:59<01:46,  5.68it/s]\n",
      " 35%|###5      | 331/935 [00:59<01:47,  5.62it/s]\n",
      " 36%|###5      | 332/935 [00:59<01:46,  5.64it/s]\n",
      " 36%|###5      | 333/935 [01:00<01:47,  5.62it/s]\n",
      " 36%|###5      | 334/935 [01:00<01:46,  5.64it/s]\n",
      " 36%|###5      | 335/935 [01:00<01:45,  5.67it/s]\n",
      " 36%|###5      | 336/935 [01:00<01:45,  5.68it/s]\n",
      " 36%|###6      | 337/935 [01:00<01:44,  5.70it/s]\n",
      " 36%|###6      | 338/935 [01:01<01:45,  5.68it/s]\n",
      " 36%|###6      | 339/935 [01:01<01:44,  5.69it/s]\n",
      " 36%|###6      | 340/935 [01:01<01:44,  5.69it/s]\n",
      " 36%|###6      | 341/935 [01:01<01:44,  5.71it/s]\n",
      " 37%|###6      | 342/935 [01:01<01:44,  5.67it/s]\n",
      " 37%|###6      | 343/935 [01:01<01:43,  5.70it/s]\n",
      " 37%|###6      | 344/935 [01:02<01:43,  5.69it/s]\n",
      " 37%|###6      | 345/935 [01:02<01:43,  5.71it/s]\n",
      " 37%|###7      | 346/935 [01:02<01:43,  5.68it/s]\n",
      " 37%|###7      | 347/935 [01:02<01:43,  5.68it/s]\n",
      " 37%|###7      | 348/935 [01:02<01:43,  5.64it/s]\n",
      " 37%|###7      | 349/935 [01:02<01:43,  5.67it/s]\n",
      " 37%|###7      | 350/935 [01:03<01:43,  5.66it/s]\n",
      " 38%|###7      | 351/935 [01:03<01:42,  5.67it/s]\n",
      " 38%|###7      | 352/935 [01:03<01:43,  5.62it/s]\n",
      " 38%|###7      | 353/935 [01:03<01:43,  5.64it/s]\n",
      " 38%|###7      | 354/935 [01:03<01:43,  5.64it/s]\n",
      " 38%|###7      | 355/935 [01:04<01:42,  5.64it/s]\n",
      " 38%|###8      | 356/935 [01:04<01:42,  5.67it/s]\n",
      " 38%|###8      | 357/935 [01:04<01:41,  5.69it/s]\n",
      " 38%|###8      | 358/935 [01:04<01:41,  5.70it/s]\n",
      " 38%|###8      | 359/935 [01:04<01:41,  5.69it/s]\n",
      " 39%|###8      | 360/935 [01:04<01:40,  5.70it/s]\n",
      " 39%|###8      | 361/935 [01:05<01:40,  5.71it/s]\n",
      " 39%|###8      | 362/935 [01:05<01:40,  5.70it/s]\n",
      " 39%|###8      | 363/935 [01:05<01:40,  5.68it/s]\n",
      " 39%|###8      | 364/935 [01:05<01:40,  5.69it/s]\n",
      " 39%|###9      | 365/935 [01:05<01:40,  5.68it/s]\n",
      " 39%|###9      | 366/935 [01:05<01:39,  5.71it/s]\n",
      " 39%|###9      | 367/935 [01:06<01:39,  5.68it/s]\n",
      " 39%|###9      | 368/935 [01:06<01:39,  5.69it/s]\n",
      " 39%|###9      | 369/935 [01:06<01:39,  5.67it/s]\n",
      " 40%|###9      | 370/935 [01:06<01:39,  5.68it/s]\n",
      " 40%|###9      | 371/935 [01:06<01:39,  5.65it/s]\n",
      " 40%|###9      | 372/935 [01:07<01:39,  5.68it/s]\n",
      " 40%|###9      | 373/935 [01:07<01:39,  5.64it/s]\n",
      " 40%|####      | 375/935 [01:07<01:27,  6.40it/s]\n",
      " 40%|####      | 376/935 [01:07<01:29,  6.22it/s]\n",
      " 40%|####      | 377/935 [01:07<01:32,  6.03it/s]\n",
      " 40%|####      | 378/935 [01:08<01:33,  5.94it/s]\n",
      " 41%|####      | 379/935 [01:08<01:35,  5.85it/s]\n",
      " 41%|####      | 380/935 [01:08<01:35,  5.82it/s]\n",
      " 41%|####      | 381/935 [01:08<01:35,  5.78it/s]\n",
      " 41%|####      | 382/935 [01:08<01:36,  5.75it/s]\n",
      " 41%|####      | 383/935 [01:08<01:36,  5.70it/s]\n",
      " 41%|####1     | 384/935 [01:09<01:36,  5.70it/s]\n",
      " 41%|####1     | 385/935 [01:09<01:37,  5.67it/s]\n",
      " 41%|####1     | 386/935 [01:09<01:36,  5.68it/s]\n",
      " 41%|####1     | 387/935 [01:09<01:36,  5.66it/s]\n",
      " 41%|####1     | 388/935 [01:09<01:36,  5.69it/s]\n",
      " 42%|####1     | 389/935 [01:09<01:36,  5.65it/s]\n",
      " 42%|####1     | 390/935 [01:10<01:36,  5.64it/s]\n",
      " 42%|####1     | 391/935 [01:10<01:35,  5.67it/s]\n",
      " 42%|####1     | 392/935 [01:10<01:35,  5.66it/s]\n",
      " 42%|####2     | 393/935 [01:10<01:35,  5.69it/s]\n",
      " 42%|####2     | 394/935 [01:10<01:35,  5.68it/s]\n",
      " 42%|####2     | 395/935 [01:11<01:34,  5.70it/s]\n",
      " 42%|####2     | 396/935 [01:11<01:34,  5.68it/s]\n",
      " 42%|####2     | 397/935 [01:11<01:34,  5.70it/s]\n",
      " 43%|####2     | 398/935 [01:11<01:34,  5.70it/s]\n",
      " 43%|####2     | 399/935 [01:11<01:34,  5.70it/s]\n",
      " 43%|####2     | 400/935 [01:11<01:34,  5.68it/s]\n",
      " 43%|####2     | 401/935 [01:12<01:33,  5.69it/s]\n",
      " 43%|####2     | 402/935 [01:12<01:33,  5.68it/s]\n",
      " 43%|####3     | 403/935 [01:12<01:33,  5.69it/s]\n",
      " 43%|####3     | 404/935 [01:12<01:33,  5.66it/s]\n",
      " 43%|####3     | 405/935 [01:12<01:33,  5.66it/s]\n",
      " 43%|####3     | 406/935 [01:12<01:33,  5.63it/s]\n",
      " 44%|####3     | 407/935 [01:13<01:33,  5.64it/s]\n",
      " 44%|####3     | 408/935 [01:13<01:33,  5.62it/s]\n",
      " 44%|####3     | 409/935 [01:13<01:33,  5.65it/s]\n",
      " 44%|####3     | 410/935 [01:13<01:32,  5.67it/s]\n",
      " 44%|####3     | 411/935 [01:13<01:32,  5.67it/s]\n",
      " 44%|####4     | 412/935 [01:13<01:31,  5.69it/s]\n",
      " 44%|####4     | 413/935 [01:14<01:31,  5.68it/s]\n",
      " 44%|####4     | 414/935 [01:14<01:31,  5.69it/s]\n",
      " 44%|####4     | 415/935 [01:14<01:31,  5.70it/s]\n",
      " 44%|####4     | 416/935 [01:14<01:31,  5.69it/s]\n",
      " 45%|####4     | 417/935 [01:14<01:31,  5.66it/s]\n",
      " 45%|####4     | 418/935 [01:15<01:30,  5.69it/s]\n",
      " 45%|####4     | 419/935 [01:15<01:30,  5.68it/s]\n",
      " 45%|####4     | 420/935 [01:15<01:30,  5.71it/s]\n",
      " 45%|####5     | 421/935 [01:15<01:30,  5.67it/s]\n",
      " 45%|####5     | 422/935 [01:15<01:30,  5.68it/s]\n",
      " 45%|####5     | 423/935 [01:15<01:30,  5.64it/s]\n",
      " 45%|####5     | 424/935 [01:16<01:30,  5.66it/s]\n",
      " 45%|####5     | 425/935 [01:16<01:30,  5.66it/s]\n",
      " 46%|####5     | 426/935 [01:16<01:29,  5.67it/s]\n",
      " 46%|####5     | 427/935 [01:16<01:29,  5.65it/s]\n",
      " 46%|####5     | 428/935 [01:16<01:29,  5.66it/s]\n",
      " 46%|####5     | 429/935 [01:17<01:29,  5.63it/s]\n",
      " 46%|####5     | 430/935 [01:17<01:29,  5.63it/s]\n",
      " 46%|####6     | 431/935 [01:17<01:29,  5.62it/s]\n",
      " 46%|####6     | 432/935 [01:17<01:29,  5.65it/s]\n",
      " 46%|####6     | 433/935 [01:17<01:28,  5.65it/s]\n",
      " 46%|####6     | 434/935 [01:17<01:28,  5.68it/s]\n",
      " 47%|####6     | 435/935 [01:18<01:27,  5.70it/s]\n",
      " 47%|####6     | 436/935 [01:18<01:27,  5.70it/s]\n",
      " 47%|####6     | 437/935 [01:18<01:27,  5.70it/s]\n",
      " 47%|####6     | 438/935 [01:18<01:27,  5.71it/s]\n",
      " 47%|####6     | 439/935 [01:18<01:26,  5.71it/s]\n",
      " 47%|####7     | 440/935 [01:18<01:26,  5.69it/s]\n",
      " 47%|####7     | 441/935 [01:19<01:26,  5.71it/s]\n",
      " 47%|####7     | 442/935 [01:19<01:26,  5.70it/s]\n",
      " 47%|####7     | 443/935 [01:19<01:26,  5.69it/s]\n",
      " 47%|####7     | 444/935 [01:19<01:26,  5.66it/s]\n",
      " 48%|####7     | 445/935 [01:19<01:25,  5.71it/s]\n",
      " 48%|####7     | 446/935 [01:19<01:25,  5.70it/s]\n",
      " 48%|####7     | 447/935 [01:20<01:25,  5.69it/s]\n",
      " 48%|####7     | 448/935 [01:20<01:26,  5.65it/s]\n",
      " 48%|####8     | 449/935 [01:20<01:25,  5.66it/s]\n",
      " 48%|####8     | 450/935 [01:20<01:26,  5.62it/s]\n",
      " 48%|####8     | 451/935 [01:20<01:25,  5.64it/s]\n",
      " 48%|####8     | 452/935 [01:21<01:26,  5.61it/s]\n",
      " 48%|####8     | 453/935 [01:21<01:25,  5.65it/s]\n",
      " 49%|####8     | 454/935 [01:21<01:24,  5.68it/s]\n",
      " 49%|####8     | 455/935 [01:21<01:24,  5.67it/s]\n",
      " 49%|####8     | 456/935 [01:21<01:24,  5.69it/s]\n",
      " 49%|####8     | 457/935 [01:21<01:24,  5.69it/s]\n",
      " 49%|####8     | 458/935 [01:22<01:23,  5.69it/s]\n",
      " 49%|####9     | 459/935 [01:22<01:23,  5.68it/s]\n",
      " 49%|####9     | 460/935 [01:22<01:23,  5.69it/s]\n",
      " 49%|####9     | 461/935 [01:22<01:23,  5.67it/s]\n",
      " 49%|####9     | 462/935 [01:22<01:23,  5.69it/s]\n",
      " 50%|####9     | 463/935 [01:22<01:22,  5.70it/s]\n",
      " 50%|####9     | 464/935 [01:23<01:22,  5.71it/s]\n",
      " 50%|####9     | 465/935 [01:23<01:22,  5.67it/s]\n",
      " 50%|####9     | 466/935 [01:23<01:22,  5.68it/s]\n",
      " 50%|####9     | 467/935 [01:23<01:22,  5.65it/s]\n",
      " 50%|#####     | 468/935 [01:23<01:22,  5.68it/s]\n",
      " 50%|#####     | 469/935 [01:24<01:22,  5.68it/s]\n",
      " 50%|#####     | 470/935 [01:24<01:21,  5.69it/s]\n",
      " 50%|#####     | 471/935 [01:24<01:22,  5.65it/s]\n",
      " 50%|#####     | 472/935 [01:24<01:21,  5.66it/s]\n",
      " 51%|#####     | 473/935 [01:24<01:21,  5.65it/s]\n",
      " 51%|#####     | 474/935 [01:24<01:21,  5.68it/s]\n",
      " 51%|#####     | 475/935 [01:25<01:21,  5.65it/s]\n",
      " 51%|#####     | 476/935 [01:25<01:20,  5.67it/s]\n",
      " 51%|#####1    | 477/935 [01:25<01:21,  5.63it/s]\n",
      " 51%|#####1    | 478/935 [01:25<01:21,  5.63it/s]\n",
      " 51%|#####1    | 479/935 [01:25<01:20,  5.67it/s]\n",
      " 51%|#####1    | 480/935 [01:25<01:20,  5.67it/s]\n",
      " 51%|#####1    | 481/935 [01:26<01:19,  5.70it/s]\n",
      " 52%|#####1    | 482/935 [01:26<01:19,  5.67it/s]\n",
      " 52%|#####1    | 483/935 [01:26<01:19,  5.68it/s]\n",
      " 52%|#####1    | 484/935 [01:26<01:19,  5.65it/s]\n",
      " 52%|#####1    | 485/935 [01:26<01:19,  5.69it/s]\n",
      " 52%|#####1    | 486/935 [01:27<01:18,  5.69it/s]\n",
      " 52%|#####2    | 487/935 [01:27<01:18,  5.70it/s]\n",
      " 52%|#####2    | 488/935 [01:27<01:18,  5.67it/s]\n",
      " 52%|#####2    | 489/935 [01:27<01:18,  5.69it/s]\n",
      " 52%|#####2    | 490/935 [01:27<01:18,  5.67it/s]\n",
      " 53%|#####2    | 491/935 [01:27<01:17,  5.70it/s]\n",
      " 53%|#####2    | 492/935 [01:28<01:18,  5.65it/s]\n",
      " 53%|#####2    | 493/935 [01:28<01:17,  5.67it/s]\n",
      " 53%|#####2    | 494/935 [01:28<01:18,  5.64it/s]\n",
      " 53%|#####2    | 495/935 [01:28<01:18,  5.64it/s]\n",
      " 53%|#####3    | 496/935 [01:28<01:18,  5.62it/s]\n",
      " 53%|#####3    | 497/935 [01:28<01:17,  5.65it/s]\n",
      " 53%|#####3    | 498/935 [01:29<01:17,  5.67it/s]\n",
      " 53%|#####3    | 499/935 [01:29<01:16,  5.66it/s]\n",
      " 53%|#####3    | 500/935 [01:29<01:16,  5.68it/s]\n",
      "                                                 \n",
      "\n",
      " 53%|#####3    | 500/935 [01:29<01:16,  5.68it/s]\n",
      " 54%|#####3    | 501/935 [01:29<01:17,  5.61it/s]\n",
      " 54%|#####3    | 502/935 [01:29<01:16,  5.63it/s]\n",
      " 54%|#####3    | 503/935 [01:30<01:16,  5.64it/s]\n",
      " 54%|#####3    | 504/935 [01:30<01:16,  5.67it/s]\n",
      " 54%|#####4    | 505/935 [01:30<01:15,  5.67it/s]\n",
      " 54%|#####4    | 506/935 [01:30<01:15,  5.70it/s]\n",
      " 54%|#####4    | 507/935 [01:30<01:15,  5.68it/s]\n",
      " 54%|#####4    | 508/935 [01:30<01:14,  5.70it/s]\n",
      " 54%|#####4    | 509/935 [01:31<01:15,  5.68it/s]\n",
      " 55%|#####4    | 510/935 [01:31<01:14,  5.71it/s]\n",
      " 55%|#####4    | 511/935 [01:31<01:14,  5.67it/s]\n",
      " 55%|#####4    | 512/935 [01:31<01:14,  5.68it/s]\n",
      " 55%|#####4    | 513/935 [01:31<01:14,  5.66it/s]\n",
      " 55%|#####4    | 514/935 [01:31<01:14,  5.69it/s]\n",
      " 55%|#####5    | 515/935 [01:32<01:14,  5.67it/s]\n",
      " 55%|#####5    | 516/935 [01:32<01:14,  5.66it/s]\n",
      " 55%|#####5    | 517/935 [01:32<01:14,  5.62it/s]\n",
      " 55%|#####5    | 518/935 [01:32<01:14,  5.62it/s]\n",
      " 56%|#####5    | 519/935 [01:32<01:14,  5.60it/s]\n",
      " 56%|#####5    | 520/935 [01:33<01:13,  5.61it/s]\n",
      " 56%|#####5    | 521/935 [01:33<01:13,  5.65it/s]\n",
      " 56%|#####5    | 522/935 [01:33<01:12,  5.67it/s]\n",
      " 56%|#####5    | 523/935 [01:33<01:12,  5.69it/s]\n",
      " 56%|#####6    | 524/935 [01:33<01:12,  5.68it/s]\n",
      " 56%|#####6    | 525/935 [01:33<01:11,  5.70it/s]\n",
      " 56%|#####6    | 526/935 [01:34<01:11,  5.68it/s]\n",
      " 56%|#####6    | 527/935 [01:34<01:11,  5.70it/s]\n",
      " 56%|#####6    | 528/935 [01:34<01:11,  5.68it/s]\n",
      " 57%|#####6    | 529/935 [01:34<01:11,  5.69it/s]\n",
      " 57%|#####6    | 530/935 [01:34<01:11,  5.67it/s]\n",
      " 57%|#####6    | 531/935 [01:34<01:11,  5.67it/s]\n",
      " 57%|#####6    | 532/935 [01:35<01:10,  5.69it/s]\n",
      " 57%|#####7    | 533/935 [01:35<01:10,  5.69it/s]\n",
      " 57%|#####7    | 534/935 [01:35<01:10,  5.65it/s]\n",
      " 57%|#####7    | 535/935 [01:35<01:10,  5.65it/s]\n",
      " 57%|#####7    | 536/935 [01:35<01:10,  5.64it/s]\n",
      " 57%|#####7    | 537/935 [01:36<01:10,  5.65it/s]\n",
      " 58%|#####7    | 538/935 [01:36<01:10,  5.64it/s]\n",
      " 58%|#####7    | 539/935 [01:36<01:09,  5.67it/s]\n",
      " 58%|#####7    | 540/935 [01:36<01:09,  5.65it/s]\n",
      " 58%|#####7    | 541/935 [01:36<01:09,  5.66it/s]\n",
      " 58%|#####7    | 542/935 [01:36<01:09,  5.68it/s]\n",
      " 58%|#####8    | 543/935 [01:37<01:09,  5.66it/s]\n",
      " 58%|#####8    | 544/935 [01:37<01:08,  5.69it/s]\n",
      " 58%|#####8    | 545/935 [01:37<01:08,  5.70it/s]\n",
      " 58%|#####8    | 546/935 [01:37<01:08,  5.70it/s]\n",
      " 59%|#####8    | 547/935 [01:37<01:08,  5.67it/s]\n",
      " 59%|#####8    | 548/935 [01:37<01:08,  5.69it/s]\n",
      " 59%|#####8    | 549/935 [01:38<01:08,  5.67it/s]\n",
      " 59%|#####8    | 550/935 [01:38<01:07,  5.71it/s]\n",
      " 59%|#####8    | 551/935 [01:38<01:07,  5.66it/s]\n",
      " 59%|#####9    | 552/935 [01:38<01:07,  5.68it/s]\n",
      " 59%|#####9    | 553/935 [01:38<01:07,  5.65it/s]\n",
      " 59%|#####9    | 554/935 [01:39<01:07,  5.66it/s]\n",
      " 59%|#####9    | 555/935 [01:39<01:07,  5.66it/s]\n",
      " 59%|#####9    | 556/935 [01:39<01:06,  5.68it/s]\n",
      " 60%|#####9    | 557/935 [01:39<01:06,  5.64it/s]\n",
      " 60%|#####9    | 558/935 [01:39<01:06,  5.65it/s]\n",
      " 60%|#####9    | 559/935 [01:39<01:06,  5.62it/s]\n",
      " 60%|#####9    | 560/935 [01:40<01:06,  5.63it/s]\n",
      " 60%|######    | 562/935 [01:40<00:58,  6.39it/s]\n",
      " 60%|######    | 563/935 [01:40<01:00,  6.18it/s]\n",
      " 60%|######    | 564/935 [01:40<01:01,  6.05it/s]\n",
      " 60%|######    | 565/935 [01:40<01:02,  5.93it/s]\n",
      " 61%|######    | 566/935 [01:41<01:02,  5.87it/s]\n",
      " 61%|######    | 567/935 [01:41<01:03,  5.78it/s]\n",
      " 61%|######    | 568/935 [01:41<01:03,  5.76it/s]\n",
      " 61%|######    | 569/935 [01:41<01:04,  5.70it/s]\n",
      " 61%|######    | 570/935 [01:41<01:04,  5.68it/s]\n",
      " 61%|######1   | 571/935 [01:41<01:04,  5.66it/s]\n",
      " 61%|######1   | 572/935 [01:42<01:03,  5.68it/s]\n",
      " 61%|######1   | 573/935 [01:42<01:03,  5.68it/s]\n",
      " 61%|######1   | 574/935 [01:42<01:03,  5.68it/s]\n",
      " 61%|######1   | 575/935 [01:42<01:03,  5.68it/s]\n",
      " 62%|######1   | 576/935 [01:42<01:03,  5.67it/s]\n",
      " 62%|######1   | 577/935 [01:43<01:03,  5.67it/s]\n",
      " 62%|######1   | 578/935 [01:43<01:02,  5.70it/s]\n",
      " 62%|######1   | 579/935 [01:43<01:02,  5.71it/s]\n",
      " 62%|######2   | 580/935 [01:43<01:02,  5.68it/s]\n",
      " 62%|######2   | 581/935 [01:43<01:02,  5.71it/s]\n",
      " 62%|######2   | 582/935 [01:43<01:02,  5.69it/s]\n",
      " 62%|######2   | 583/935 [01:44<01:01,  5.70it/s]\n",
      " 62%|######2   | 584/935 [01:44<01:01,  5.67it/s]\n",
      " 63%|######2   | 585/935 [01:44<01:01,  5.68it/s]\n",
      " 63%|######2   | 586/935 [01:44<01:01,  5.65it/s]\n",
      " 63%|######2   | 587/935 [01:44<01:01,  5.67it/s]\n",
      " 63%|######2   | 588/935 [01:44<01:01,  5.66it/s]\n",
      " 63%|######2   | 589/935 [01:45<01:00,  5.67it/s]\n",
      " 63%|######3   | 590/935 [01:45<01:01,  5.64it/s]\n",
      " 63%|######3   | 591/935 [01:45<01:01,  5.63it/s]\n",
      " 63%|######3   | 592/935 [01:45<01:01,  5.58it/s]\n",
      " 63%|######3   | 593/935 [01:45<01:01,  5.59it/s]\n",
      " 64%|######3   | 594/935 [01:46<01:00,  5.59it/s]\n",
      " 64%|######3   | 595/935 [01:46<01:00,  5.61it/s]\n",
      " 64%|######3   | 596/935 [01:46<01:00,  5.65it/s]\n",
      " 64%|######3   | 597/935 [01:46<00:59,  5.67it/s]\n",
      " 64%|######3   | 598/935 [01:46<00:59,  5.70it/s]\n",
      " 64%|######4   | 599/935 [01:46<00:59,  5.67it/s]\n",
      " 64%|######4   | 600/935 [01:47<00:59,  5.67it/s]\n",
      " 64%|######4   | 601/935 [01:47<00:58,  5.67it/s]\n",
      " 64%|######4   | 602/935 [01:47<00:58,  5.69it/s]\n",
      " 64%|######4   | 603/935 [01:47<00:58,  5.70it/s]\n",
      " 65%|######4   | 604/935 [01:47<00:58,  5.70it/s]\n",
      " 65%|######4   | 605/935 [01:47<00:58,  5.68it/s]\n",
      " 65%|######4   | 606/935 [01:48<00:57,  5.70it/s]\n",
      " 65%|######4   | 607/935 [01:48<00:57,  5.69it/s]\n",
      " 65%|######5   | 608/935 [01:48<00:57,  5.69it/s]\n",
      " 65%|######5   | 609/935 [01:48<00:57,  5.66it/s]\n",
      " 65%|######5   | 610/935 [01:48<00:57,  5.65it/s]\n",
      " 65%|######5   | 611/935 [01:49<00:57,  5.64it/s]\n",
      " 65%|######5   | 612/935 [01:49<00:57,  5.65it/s]\n",
      " 66%|######5   | 613/935 [01:49<00:57,  5.63it/s]\n",
      " 66%|######5   | 614/935 [01:49<00:56,  5.65it/s]\n",
      " 66%|######5   | 615/935 [01:49<00:56,  5.66it/s]\n",
      " 66%|######5   | 616/935 [01:49<00:56,  5.65it/s]\n",
      " 66%|######5   | 617/935 [01:50<00:56,  5.67it/s]\n",
      " 66%|######6   | 618/935 [01:50<00:55,  5.66it/s]\n",
      " 66%|######6   | 619/935 [01:50<00:55,  5.68it/s]\n",
      " 66%|######6   | 620/935 [01:50<00:55,  5.69it/s]\n",
      " 66%|######6   | 621/935 [01:50<00:55,  5.70it/s]\n",
      " 67%|######6   | 622/935 [01:50<00:55,  5.68it/s]\n",
      " 67%|######6   | 623/935 [01:51<00:54,  5.68it/s]\n",
      " 67%|######6   | 624/935 [01:51<00:54,  5.69it/s]\n",
      " 67%|######6   | 625/935 [01:51<00:54,  5.70it/s]\n",
      " 67%|######6   | 626/935 [01:51<00:54,  5.65it/s]\n",
      " 67%|######7   | 627/935 [01:51<00:54,  5.68it/s]\n",
      " 67%|######7   | 628/935 [01:52<00:54,  5.65it/s]\n",
      " 67%|######7   | 629/935 [01:52<00:53,  5.68it/s]\n",
      " 67%|######7   | 630/935 [01:52<00:53,  5.66it/s]\n",
      " 67%|######7   | 631/935 [01:52<00:53,  5.68it/s]\n",
      " 68%|######7   | 632/935 [01:52<00:53,  5.65it/s]\n",
      " 68%|######7   | 633/935 [01:52<00:53,  5.66it/s]\n",
      " 68%|######7   | 634/935 [01:53<00:53,  5.63it/s]\n",
      " 68%|######7   | 635/935 [01:53<00:53,  5.64it/s]\n",
      " 68%|######8   | 636/935 [01:53<00:52,  5.64it/s]\n",
      " 68%|######8   | 637/935 [01:53<00:52,  5.67it/s]\n",
      " 68%|######8   | 638/935 [01:53<00:52,  5.65it/s]\n",
      " 68%|######8   | 639/935 [01:53<00:52,  5.65it/s]\n",
      " 68%|######8   | 640/935 [01:54<00:52,  5.66it/s]\n",
      " 69%|######8   | 641/935 [01:54<00:51,  5.67it/s]\n",
      " 69%|######8   | 642/935 [01:54<00:51,  5.69it/s]\n",
      " 69%|######8   | 643/935 [01:54<00:51,  5.69it/s]\n",
      " 69%|######8   | 644/935 [01:54<00:51,  5.70it/s]\n",
      " 69%|######8   | 645/935 [01:55<00:51,  5.67it/s]\n",
      " 69%|######9   | 646/935 [01:55<00:50,  5.70it/s]\n",
      " 69%|######9   | 647/935 [01:55<00:50,  5.71it/s]\n",
      " 69%|######9   | 648/935 [01:55<00:50,  5.70it/s]\n",
      " 69%|######9   | 649/935 [01:55<00:50,  5.68it/s]\n",
      " 70%|######9   | 650/935 [01:55<00:50,  5.68it/s]\n",
      " 70%|######9   | 651/935 [01:56<00:50,  5.67it/s]\n",
      " 70%|######9   | 652/935 [01:56<00:49,  5.68it/s]\n",
      " 70%|######9   | 653/935 [01:56<00:49,  5.66it/s]\n",
      " 70%|######9   | 654/935 [01:56<00:49,  5.68it/s]\n",
      " 70%|#######   | 655/935 [01:56<00:49,  5.65it/s]\n",
      " 70%|#######   | 656/935 [01:56<00:49,  5.65it/s]\n",
      " 70%|#######   | 657/935 [01:57<00:49,  5.64it/s]\n",
      " 70%|#######   | 658/935 [01:57<00:48,  5.67it/s]\n",
      " 70%|#######   | 659/935 [01:57<00:48,  5.64it/s]\n",
      " 71%|#######   | 660/935 [01:57<00:48,  5.65it/s]\n",
      " 71%|#######   | 661/935 [01:57<00:48,  5.67it/s]\n",
      " 71%|#######   | 662/935 [01:58<00:48,  5.67it/s]\n",
      " 71%|#######   | 663/935 [01:58<00:47,  5.68it/s]\n",
      " 71%|#######1  | 664/935 [01:58<00:47,  5.67it/s]\n",
      " 71%|#######1  | 665/935 [01:58<00:47,  5.70it/s]\n",
      " 71%|#######1  | 666/935 [01:58<00:47,  5.69it/s]\n",
      " 71%|#######1  | 667/935 [01:58<00:47,  5.68it/s]\n",
      " 71%|#######1  | 668/935 [01:59<00:46,  5.68it/s]\n",
      " 72%|#######1  | 669/935 [01:59<00:46,  5.69it/s]\n",
      " 72%|#######1  | 670/935 [01:59<00:46,  5.68it/s]\n",
      " 72%|#######1  | 671/935 [01:59<00:46,  5.68it/s]\n",
      " 72%|#######1  | 672/935 [01:59<00:46,  5.67it/s]\n",
      " 72%|#######1  | 673/935 [01:59<00:46,  5.69it/s]\n",
      " 72%|#######2  | 674/935 [02:00<00:46,  5.67it/s]\n",
      " 72%|#######2  | 675/935 [02:00<00:45,  5.68it/s]\n",
      " 72%|#######2  | 676/935 [02:00<00:46,  5.61it/s]\n",
      " 72%|#######2  | 677/935 [02:00<00:45,  5.64it/s]\n",
      " 73%|#######2  | 678/935 [02:00<00:45,  5.60it/s]\n",
      " 73%|#######2  | 679/935 [02:01<00:45,  5.62it/s]\n",
      " 73%|#######2  | 680/935 [02:01<00:45,  5.61it/s]\n",
      " 73%|#######2  | 681/935 [02:01<00:45,  5.63it/s]\n",
      " 73%|#######2  | 682/935 [02:01<00:44,  5.64it/s]\n",
      " 73%|#######3  | 683/935 [02:01<00:44,  5.66it/s]\n",
      " 73%|#######3  | 684/935 [02:01<00:44,  5.68it/s]\n",
      " 73%|#######3  | 685/935 [02:02<00:44,  5.66it/s]\n",
      " 73%|#######3  | 686/935 [02:02<00:43,  5.68it/s]\n",
      " 73%|#######3  | 687/935 [02:02<00:43,  5.69it/s]\n",
      " 74%|#######3  | 688/935 [02:02<00:43,  5.72it/s]\n",
      " 74%|#######3  | 689/935 [02:02<00:43,  5.68it/s]\n",
      " 74%|#######3  | 690/935 [02:02<00:43,  5.69it/s]\n",
      " 74%|#######3  | 691/935 [02:03<00:43,  5.66it/s]\n",
      " 74%|#######4  | 692/935 [02:03<00:42,  5.67it/s]\n",
      " 74%|#######4  | 693/935 [02:03<00:42,  5.65it/s]\n",
      " 74%|#######4  | 694/935 [02:03<00:42,  5.67it/s]\n",
      " 74%|#######4  | 695/935 [02:03<00:42,  5.65it/s]\n",
      " 74%|#######4  | 696/935 [02:04<00:42,  5.66it/s]\n",
      " 75%|#######4  | 697/935 [02:04<00:42,  5.64it/s]\n",
      " 75%|#######4  | 698/935 [02:04<00:41,  5.66it/s]\n",
      " 75%|#######4  | 699/935 [02:04<00:41,  5.65it/s]\n",
      " 75%|#######4  | 700/935 [02:04<00:41,  5.66it/s]\n",
      " 75%|#######4  | 701/935 [02:04<00:41,  5.64it/s]\n",
      " 75%|#######5  | 702/935 [02:05<00:41,  5.63it/s]\n",
      " 75%|#######5  | 703/935 [02:05<00:41,  5.63it/s]\n",
      " 75%|#######5  | 704/935 [02:05<00:41,  5.63it/s]\n",
      " 75%|#######5  | 705/935 [02:05<00:40,  5.67it/s]\n",
      " 76%|#######5  | 706/935 [02:05<00:40,  5.67it/s]\n",
      " 76%|#######5  | 707/935 [02:05<00:40,  5.69it/s]\n",
      " 76%|#######5  | 708/935 [02:06<00:40,  5.67it/s]\n",
      " 76%|#######5  | 709/935 [02:06<00:39,  5.67it/s]\n",
      " 76%|#######5  | 710/935 [02:06<00:39,  5.66it/s]\n",
      " 76%|#######6  | 711/935 [02:06<00:39,  5.69it/s]\n",
      " 76%|#######6  | 712/935 [02:06<00:39,  5.68it/s]\n",
      " 76%|#######6  | 713/935 [02:07<00:38,  5.69it/s]\n",
      " 76%|#######6  | 714/935 [02:07<00:38,  5.68it/s]\n",
      " 76%|#######6  | 715/935 [02:07<00:38,  5.68it/s]\n",
      " 77%|#######6  | 716/935 [02:07<00:38,  5.67it/s]\n",
      " 77%|#######6  | 717/935 [02:07<00:38,  5.68it/s]\n",
      " 77%|#######6  | 718/935 [02:07<00:38,  5.66it/s]\n",
      " 77%|#######6  | 719/935 [02:08<00:38,  5.64it/s]\n",
      " 77%|#######7  | 720/935 [02:08<00:38,  5.61it/s]\n",
      " 77%|#######7  | 721/935 [02:08<00:38,  5.63it/s]\n",
      " 77%|#######7  | 722/935 [02:08<00:37,  5.63it/s]\n",
      " 77%|#######7  | 723/935 [02:08<00:37,  5.62it/s]\n",
      " 77%|#######7  | 724/935 [02:08<00:37,  5.66it/s]\n",
      " 78%|#######7  | 725/935 [02:09<00:37,  5.68it/s]\n",
      " 78%|#######7  | 726/935 [02:09<00:36,  5.68it/s]\n",
      " 78%|#######7  | 727/935 [02:09<00:36,  5.68it/s]\n",
      " 78%|#######7  | 728/935 [02:09<00:36,  5.67it/s]\n",
      " 78%|#######7  | 729/935 [02:09<00:36,  5.66it/s]\n",
      " 78%|#######8  | 730/935 [02:10<00:36,  5.68it/s]\n",
      " 78%|#######8  | 731/935 [02:10<00:35,  5.68it/s]\n",
      " 78%|#######8  | 732/935 [02:10<00:35,  5.69it/s]\n",
      " 78%|#######8  | 733/935 [02:10<00:35,  5.66it/s]\n",
      " 79%|#######8  | 734/935 [02:10<00:35,  5.69it/s]\n",
      " 79%|#######8  | 735/935 [02:10<00:35,  5.68it/s]\n",
      " 79%|#######8  | 736/935 [02:11<00:34,  5.69it/s]\n",
      " 79%|#######8  | 737/935 [02:11<00:35,  5.65it/s]\n",
      " 79%|#######8  | 738/935 [02:11<00:34,  5.67it/s]\n",
      " 79%|#######9  | 739/935 [02:11<00:34,  5.64it/s]\n",
      " 79%|#######9  | 740/935 [02:11<00:34,  5.65it/s]\n",
      " 79%|#######9  | 741/935 [02:11<00:34,  5.64it/s]\n",
      " 79%|#######9  | 742/935 [02:12<00:34,  5.64it/s]\n",
      " 79%|#######9  | 743/935 [02:12<00:33,  5.66it/s]\n",
      " 80%|#######9  | 744/935 [02:12<00:33,  5.66it/s]\n",
      " 80%|#######9  | 745/935 [02:12<00:33,  5.62it/s]\n",
      " 80%|#######9  | 746/935 [02:12<00:33,  5.63it/s]\n",
      " 80%|#######9  | 747/935 [02:13<00:33,  5.65it/s]\n",
      " 80%|########  | 749/935 [02:13<00:29,  6.36it/s]\n",
      " 80%|########  | 750/935 [02:13<00:29,  6.18it/s]\n",
      " 80%|########  | 751/935 [02:13<00:30,  6.00it/s]\n",
      " 80%|########  | 752/935 [02:13<00:31,  5.90it/s]\n",
      " 81%|########  | 753/935 [02:14<00:31,  5.82it/s]\n",
      " 81%|########  | 754/935 [02:14<00:31,  5.79it/s]\n",
      " 81%|########  | 755/935 [02:14<00:31,  5.75it/s]\n",
      " 81%|########  | 756/935 [02:14<00:31,  5.73it/s]\n",
      " 81%|########  | 757/935 [02:14<00:31,  5.72it/s]\n",
      " 81%|########1 | 758/935 [02:14<00:31,  5.69it/s]\n",
      " 81%|########1 | 759/935 [02:15<00:30,  5.68it/s]\n",
      " 81%|########1 | 760/935 [02:15<00:30,  5.68it/s]\n",
      " 81%|########1 | 761/935 [02:15<00:30,  5.71it/s]\n",
      " 81%|########1 | 762/935 [02:15<00:30,  5.68it/s]\n",
      " 82%|########1 | 763/935 [02:15<00:30,  5.69it/s]\n",
      " 82%|########1 | 764/935 [02:15<00:30,  5.67it/s]\n",
      " 82%|########1 | 765/935 [02:16<00:29,  5.69it/s]\n",
      " 82%|########1 | 766/935 [02:16<00:29,  5.68it/s]\n",
      " 82%|########2 | 767/935 [02:16<00:29,  5.68it/s]\n",
      " 82%|########2 | 768/935 [02:16<00:29,  5.65it/s]\n",
      " 82%|########2 | 769/935 [02:16<00:29,  5.66it/s]\n",
      " 82%|########2 | 770/935 [02:17<00:29,  5.64it/s]\n",
      " 82%|########2 | 771/935 [02:17<00:28,  5.67it/s]\n",
      " 83%|########2 | 772/935 [02:17<00:28,  5.65it/s]\n",
      " 83%|########2 | 773/935 [02:17<00:28,  5.65it/s]\n",
      " 83%|########2 | 774/935 [02:17<00:28,  5.60it/s]\n",
      " 83%|########2 | 775/935 [02:17<00:28,  5.62it/s]\n",
      " 83%|########2 | 776/935 [02:18<00:28,  5.63it/s]\n",
      " 83%|########3 | 777/935 [02:18<00:28,  5.63it/s]\n",
      " 83%|########3 | 778/935 [02:18<00:27,  5.66it/s]\n",
      " 83%|########3 | 779/935 [02:18<00:27,  5.68it/s]\n",
      " 83%|########3 | 780/935 [02:18<00:27,  5.68it/s]\n",
      " 84%|########3 | 781/935 [02:18<00:27,  5.67it/s]\n",
      " 84%|########3 | 782/935 [02:19<00:26,  5.67it/s]\n",
      " 84%|########3 | 783/935 [02:19<00:26,  5.66it/s]\n",
      " 84%|########3 | 784/935 [02:19<00:26,  5.68it/s]\n",
      " 84%|########3 | 785/935 [02:19<00:26,  5.67it/s]\n",
      " 84%|########4 | 786/935 [02:19<00:26,  5.69it/s]\n",
      " 84%|########4 | 787/935 [02:20<00:26,  5.67it/s]\n",
      " 84%|########4 | 788/935 [02:20<00:25,  5.69it/s]\n",
      " 84%|########4 | 789/935 [02:20<00:25,  5.67it/s]\n",
      " 84%|########4 | 790/935 [02:20<00:25,  5.69it/s]\n",
      " 85%|########4 | 791/935 [02:20<00:25,  5.66it/s]\n",
      " 85%|########4 | 792/935 [02:20<00:25,  5.66it/s]\n",
      " 85%|########4 | 793/935 [02:21<00:25,  5.62it/s]\n",
      " 85%|########4 | 794/935 [02:21<00:25,  5.63it/s]\n",
      " 85%|########5 | 795/935 [02:21<00:24,  5.63it/s]\n",
      " 85%|########5 | 796/935 [02:21<00:24,  5.64it/s]\n",
      " 85%|########5 | 797/935 [02:21<00:24,  5.68it/s]\n",
      " 85%|########5 | 798/935 [02:21<00:24,  5.68it/s]\n",
      " 85%|########5 | 799/935 [02:22<00:23,  5.69it/s]\n",
      " 86%|########5 | 800/935 [02:22<00:23,  5.67it/s]\n",
      " 86%|########5 | 801/935 [02:22<00:23,  5.68it/s]\n",
      " 86%|########5 | 802/935 [02:22<00:23,  5.69it/s]\n",
      " 86%|########5 | 803/935 [02:22<00:23,  5.69it/s]\n",
      " 86%|########5 | 804/935 [02:23<00:23,  5.67it/s]\n",
      " 86%|########6 | 805/935 [02:23<00:22,  5.67it/s]\n",
      " 86%|########6 | 806/935 [02:23<00:22,  5.65it/s]\n",
      " 86%|########6 | 807/935 [02:23<00:22,  5.67it/s]\n",
      " 86%|########6 | 808/935 [02:23<00:22,  5.68it/s]\n",
      " 87%|########6 | 809/935 [02:23<00:22,  5.69it/s]\n",
      " 87%|########6 | 810/935 [02:24<00:22,  5.64it/s]\n",
      " 87%|########6 | 811/935 [02:24<00:21,  5.66it/s]\n",
      " 87%|########6 | 812/935 [02:24<00:21,  5.64it/s]\n",
      " 87%|########6 | 813/935 [02:24<00:21,  5.67it/s]\n",
      " 87%|########7 | 814/935 [02:24<00:21,  5.65it/s]\n",
      " 87%|########7 | 815/935 [02:24<00:21,  5.67it/s]\n",
      " 87%|########7 | 816/935 [02:25<00:21,  5.60it/s]\n",
      " 87%|########7 | 817/935 [02:25<00:20,  5.64it/s]\n",
      " 87%|########7 | 818/935 [02:25<00:20,  5.60it/s]\n",
      " 88%|########7 | 819/935 [02:25<00:20,  5.61it/s]\n",
      " 88%|########7 | 820/935 [02:25<00:20,  5.64it/s]\n",
      " 88%|########7 | 821/935 [02:26<00:20,  5.64it/s]\n",
      " 88%|########7 | 822/935 [02:26<00:19,  5.67it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|########8 | 823/935 [02:26<00:19,  5.67it/s]\n",
      " 88%|########8 | 824/935 [02:26<00:19,  5.68it/s]\n",
      " 88%|########8 | 825/935 [02:26<00:19,  5.67it/s]\n",
      " 88%|########8 | 826/935 [02:26<00:19,  5.70it/s]\n",
      " 88%|########8 | 827/935 [02:27<00:19,  5.68it/s]\n",
      " 89%|########8 | 828/935 [02:27<00:18,  5.69it/s]\n",
      " 89%|########8 | 829/935 [02:27<00:18,  5.66it/s]\n",
      " 89%|########8 | 830/935 [02:27<00:18,  5.66it/s]\n",
      " 89%|########8 | 831/935 [02:27<00:18,  5.64it/s]\n",
      " 89%|########8 | 832/935 [02:27<00:18,  5.66it/s]\n",
      " 89%|########9 | 833/935 [02:28<00:18,  5.64it/s]\n",
      " 89%|########9 | 834/935 [02:28<00:17,  5.67it/s]\n",
      " 89%|########9 | 835/935 [02:28<00:17,  5.63it/s]\n",
      " 89%|########9 | 836/935 [02:28<00:17,  5.64it/s]\n",
      " 90%|########9 | 837/935 [02:28<00:17,  5.61it/s]\n",
      " 90%|########9 | 838/935 [02:29<00:17,  5.63it/s]\n",
      " 90%|########9 | 839/935 [02:29<00:16,  5.66it/s]\n",
      " 90%|########9 | 840/935 [02:29<00:16,  5.68it/s]\n",
      " 90%|########9 | 841/935 [02:29<00:16,  5.69it/s]\n",
      " 90%|######### | 842/935 [02:29<00:16,  5.68it/s]\n",
      " 90%|######### | 843/935 [02:29<00:16,  5.69it/s]\n",
      " 90%|######### | 844/935 [02:30<00:16,  5.66it/s]\n",
      " 90%|######### | 845/935 [02:30<00:15,  5.67it/s]\n",
      " 90%|######### | 846/935 [02:30<00:15,  5.68it/s]\n",
      " 91%|######### | 847/935 [02:30<00:15,  5.70it/s]\n",
      " 91%|######### | 848/935 [02:30<00:15,  5.68it/s]\n",
      " 91%|######### | 849/935 [02:30<00:15,  5.70it/s]\n",
      " 91%|######### | 850/935 [02:31<00:14,  5.67it/s]\n",
      " 91%|#########1| 851/935 [02:31<00:14,  5.68it/s]\n",
      " 91%|#########1| 852/935 [02:31<00:14,  5.65it/s]\n",
      " 91%|#########1| 853/935 [02:31<00:14,  5.65it/s]\n",
      " 91%|#########1| 854/935 [02:31<00:14,  5.58it/s]\n",
      " 91%|#########1| 855/935 [02:32<00:14,  5.63it/s]\n",
      " 92%|#########1| 856/935 [02:32<00:14,  5.61it/s]\n",
      " 92%|#########1| 857/935 [02:32<00:13,  5.62it/s]\n",
      " 92%|#########1| 858/935 [02:32<00:13,  5.61it/s]\n",
      " 92%|#########1| 859/935 [02:32<00:13,  5.64it/s]\n",
      " 92%|#########1| 860/935 [02:32<00:13,  5.64it/s]\n",
      " 92%|#########2| 861/935 [02:33<00:13,  5.66it/s]\n",
      " 92%|#########2| 862/935 [02:33<00:12,  5.67it/s]\n",
      " 92%|#########2| 863/935 [02:33<00:12,  5.65it/s]\n",
      " 92%|#########2| 864/935 [02:33<00:12,  5.68it/s]\n",
      " 93%|#########2| 865/935 [02:33<00:12,  5.68it/s]\n",
      " 93%|#########2| 866/935 [02:33<00:12,  5.70it/s]\n",
      " 93%|#########2| 867/935 [02:34<00:11,  5.67it/s]\n",
      " 93%|#########2| 868/935 [02:34<00:11,  5.68it/s]\n",
      " 93%|#########2| 869/935 [02:34<00:11,  5.68it/s]\n",
      " 93%|#########3| 870/935 [02:34<00:11,  5.69it/s]\n",
      " 93%|#########3| 871/935 [02:34<00:11,  5.67it/s]\n",
      " 93%|#########3| 872/935 [02:35<00:11,  5.68it/s]\n",
      " 93%|#########3| 873/935 [02:35<00:10,  5.66it/s]\n",
      " 93%|#########3| 874/935 [02:35<00:10,  5.68it/s]\n",
      " 94%|#########3| 875/935 [02:35<00:10,  5.67it/s]\n",
      " 94%|#########3| 876/935 [02:35<00:10,  5.68it/s]\n",
      " 94%|#########3| 877/935 [02:35<00:10,  5.63it/s]\n",
      " 94%|#########3| 878/935 [02:36<00:10,  5.65it/s]\n",
      " 94%|#########4| 879/935 [02:36<00:10,  5.59it/s]\n",
      " 94%|#########4| 880/935 [02:36<00:09,  5.60it/s]\n",
      " 94%|#########4| 881/935 [02:36<00:09,  5.63it/s]\n",
      " 94%|#########4| 882/935 [02:36<00:09,  5.63it/s]\n",
      " 94%|#########4| 883/935 [02:36<00:09,  5.66it/s]\n",
      " 95%|#########4| 884/935 [02:37<00:08,  5.67it/s]\n",
      " 95%|#########4| 885/935 [02:37<00:08,  5.69it/s]\n",
      " 95%|#########4| 886/935 [02:37<00:08,  5.67it/s]\n",
      " 95%|#########4| 887/935 [02:37<00:08,  5.67it/s]\n",
      " 95%|#########4| 888/935 [02:37<00:08,  5.66it/s]\n",
      " 95%|#########5| 889/935 [02:38<00:08,  5.67it/s]\n",
      " 95%|#########5| 890/935 [02:38<00:07,  5.68it/s]\n",
      " 95%|#########5| 891/935 [02:38<00:07,  5.68it/s]\n",
      " 95%|#########5| 892/935 [02:38<00:07,  5.66it/s]\n",
      " 96%|#########5| 893/935 [02:38<00:07,  5.68it/s]\n",
      " 96%|#########5| 894/935 [02:38<00:07,  5.65it/s]\n",
      " 96%|#########5| 895/935 [02:39<00:07,  5.67it/s]\n",
      " 96%|#########5| 896/935 [02:39<00:06,  5.64it/s]\n",
      " 96%|#########5| 897/935 [02:39<00:06,  5.66it/s]\n",
      " 96%|#########6| 898/935 [02:39<00:06,  5.61it/s]\n",
      " 96%|#########6| 899/935 [02:39<00:06,  5.63it/s]\n",
      " 96%|#########6| 900/935 [02:39<00:06,  5.61it/s]\n",
      " 96%|#########6| 901/935 [02:40<00:06,  5.63it/s]\n",
      " 96%|#########6| 902/935 [02:40<00:05,  5.67it/s]\n",
      " 97%|#########6| 903/935 [02:40<00:05,  5.67it/s]\n",
      " 97%|#########6| 904/935 [02:40<00:05,  5.67it/s]\n",
      " 97%|#########6| 905/935 [02:40<00:05,  5.67it/s]\n",
      " 97%|#########6| 906/935 [02:41<00:05,  5.66it/s]\n",
      " 97%|#########7| 907/935 [02:41<00:04,  5.68it/s]\n",
      " 97%|#########7| 908/935 [02:41<00:04,  5.71it/s]\n",
      " 97%|#########7| 909/935 [02:41<00:04,  5.67it/s]\n",
      " 97%|#########7| 910/935 [02:41<00:04,  5.68it/s]\n",
      " 97%|#########7| 911/935 [02:41<00:04,  5.67it/s]\n",
      " 98%|#########7| 912/935 [02:42<00:04,  5.70it/s]\n",
      " 98%|#########7| 913/935 [02:42<00:03,  5.68it/s]\n",
      " 98%|#########7| 914/935 [02:42<00:03,  5.69it/s]\n",
      " 98%|#########7| 915/935 [02:42<00:03,  5.66it/s]\n",
      " 98%|#########7| 916/935 [02:42<00:03,  5.68it/s]\n",
      " 98%|#########8| 917/935 [02:42<00:03,  5.65it/s]\n",
      " 98%|#########8| 918/935 [02:43<00:02,  5.67it/s]\n",
      " 98%|#########8| 919/935 [02:43<00:02,  5.63it/s]\n",
      " 98%|#########8| 920/935 [02:43<00:02,  5.66it/s]\n",
      " 99%|#########8| 921/935 [02:43<00:02,  5.62it/s]\n",
      " 99%|#########8| 922/935 [02:43<00:02,  5.64it/s]\n",
      " 99%|#########8| 923/935 [02:44<00:02,  5.56it/s]\n",
      " 99%|#########8| 924/935 [02:44<00:01,  5.60it/s]\n",
      " 99%|#########8| 925/935 [02:44<00:01,  5.64it/s]\n",
      " 99%|#########9| 926/935 [02:44<00:01,  5.63it/s]\n",
      " 99%|#########9| 927/935 [02:44<00:01,  5.64it/s]\n",
      " 99%|#########9| 928/935 [02:44<00:01,  5.65it/s]\n",
      " 99%|#########9| 929/935 [02:45<00:01,  5.67it/s]\n",
      " 99%|#########9| 930/935 [02:45<00:00,  5.66it/s]\n",
      "100%|#########9| 931/935 [02:45<00:00,  5.65it/s]\n",
      "100%|#########9| 932/935 [02:45<00:00,  5.64it/s]\n",
      "100%|#########9| 933/935 [02:45<00:00,  5.67it/s]\n",
      "100%|#########9| 934/935 [02:46<00:00,  5.66it/s][INFO|trainer.py:1852] 2022-10-30 04:30:15,209 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\n",
      "                                                 \n",
      "\n",
      "100%|##########| 935/935 [02:46<00:00,  5.66it/s]\n",
      "100%|##########| 935/935 [02:46<00:00,  5.63it/s]\n",
      "[INFO|trainer.py:2656] 2022-10-30 04:30:15,220 >> Saving model checkpoint to C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\n",
      "[INFO|configuration_utils.py:447] 2022-10-30 04:30:15,222 >> Configuration saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\\config.json\n",
      "[INFO|modeling_utils.py:1624] 2022-10-30 04:30:15,572 >> Model weights saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\\pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2022-10-30 04:30:15,573 >> tokenizer config file saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2022-10-30 04:30:15,573 >> Special tokens file saved in C:\\Users\\Thomas\\Downloads\\afrisent/afrisent-semeval-2023\\models\\yo_no_test\\special_tokens_map.json\n",
      "[INFO|trainer.py:725] 2022-10-30 04:30:15,604 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:30:15,605 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:30:15,605 >>   Num examples = 852\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:30:15,605 >>   Batch size = 8\n",
      "\n",
      "  0%|          | 0/107 [00:00<?, ?it/s]\n",
      "  7%|7         | 8/107 [00:00<00:01, 68.95it/s]\n",
      " 14%|#4        | 15/107 [00:00<00:01, 63.88it/s]\n",
      " 21%|##        | 22/107 [00:00<00:01, 62.26it/s]\n",
      " 27%|##7       | 29/107 [00:00<00:01, 61.29it/s]\n",
      " 34%|###3      | 36/107 [00:00<00:01, 61.13it/s]\n",
      " 40%|####      | 43/107 [00:00<00:01, 61.04it/s]\n",
      " 47%|####6     | 50/107 [00:00<00:00, 60.98it/s]\n",
      " 53%|#####3    | 57/107 [00:00<00:00, 61.00it/s]\n",
      " 60%|#####9    | 64/107 [00:01<00:00, 61.14it/s]\n",
      " 66%|######6   | 71/107 [00:01<00:00, 61.05it/s]\n",
      " 73%|#######2  | 78/107 [00:01<00:00, 61.33it/s]\n",
      " 79%|#######9  | 85/107 [00:01<00:00, 61.46it/s]\n",
      " 86%|########5 | 92/107 [00:01<00:00, 60.48it/s]\n",
      " 93%|#########2| 99/107 [00:01<00:00, 60.28it/s]\n",
      " 99%|#########9| 106/107 [00:01<00:00, 60.77it/s]\n",
      "100%|##########| 107/107 [00:01<00:00, 61.02it/s]\n",
      "[INFO|trainer.py:725] 2022-10-30 04:30:17,377 >> The following columns in the test set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2907] 2022-10-30 04:30:17,377 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2909] 2022-10-30 04:30:17,378 >>   Num examples = 852\n",
      "[INFO|trainer.py:2912] 2022-10-30 04:30:17,378 >>   Batch size = 8\n",
      "\n",
      "  0%|          | 0/107 [00:00<?, ?it/s]\n",
      "  7%|7         | 8/107 [00:00<00:01, 70.78it/s]\n",
      " 15%|#4        | 16/107 [00:00<00:01, 64.72it/s]\n",
      " 21%|##1       | 23/107 [00:00<00:01, 63.26it/s]\n",
      " 28%|##8       | 30/107 [00:00<00:01, 62.76it/s]\n",
      " 35%|###4      | 37/107 [00:00<00:01, 62.46it/s]\n",
      " 41%|####1     | 44/107 [00:00<00:01, 61.74it/s]\n",
      " 48%|####7     | 51/107 [00:00<00:00, 61.45it/s]\n",
      " 54%|#####4    | 58/107 [00:00<00:00, 61.43it/s]\n",
      " 61%|######    | 65/107 [00:01<00:00, 61.25it/s]\n",
      " 67%|######7   | 72/107 [00:01<00:00, 60.48it/s]\n",
      " 74%|#######3  | 79/107 [00:01<00:00, 60.75it/s]\n",
      " 80%|########  | 86/107 [00:01<00:00, 61.11it/s]\n",
      " 87%|########6 | 93/107 [00:01<00:00, 61.35it/s]\n",
      " 93%|#########3| 100/107 [00:01<00:00, 61.53it/s]\n",
      "100%|##########| 107/107 [00:01<00:00, 62.48it/s]\n",
      "100%|##########| 107/107 [00:01<00:00, 61.80it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = os.path.join(TRAINING_DATA_DIR, 'splitted-train-dev-test', LANGUAGE_CODE)\n",
    "OUTPUT_DIR = os.path.join(PROJECT_DIR, 'models', LANGUAGE_CODE + '_no_test')\n",
    "kinya = 'jean-paul/KinyaBERT-small'\n",
    "\n",
    "!python starter_kit/run_textclass.py \\\n",
    "  --model_name_or_path {kinya} \\\n",
    "  --data_dir {DATA_DIR} \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size {BATCH_SIZE} \\\n",
    "  --learning_rate {MAXIMUM_SEQUENCE_LENGTH} \\\n",
    "  --num_train_epochs {NUMBER_OF_TRAINING_EPOCHS} \\\n",
    "  --max_seq_length {MAXIMUM_SEQUENCE_LENGTH} \\\n",
    "  --output_dir {OUTPUT_DIR} \\\n",
    "  --save_steps {SAVE_STEPS} \\\n",
    "  --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fi_pJl3N9RcT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "",
   "toc_visible": true,
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d203a7fbe37afbb990fedfc21c321928443618f3d7b991e0237ff71906aa031f"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "008a7cb40017467b8d4450e52e0c3ec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2691cee6ebda4659afdc7afc3d7c3ccf",
      "max": 10,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d96193cb0ef436987f37d611e0c3c23",
      "value": 9
     }
    },
    "01bbb58eeced4ed4920756d513870c5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08e3f965a5ea4b338aba411dbb969ec6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c8da5d1f22f4f2990196f1d1164dc72": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1193886e44d94cc9927d05680e26348d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_edd1fc29c36a4beda3528a5b962d5d4e",
      "placeholder": "",
      "style": "IPY_MODEL_508778f87d1a4733a86bc7466ce6c689",
      "value": " 9/10 [00:18&lt;00:01,  1.89s/ba]"
     }
    },
    "1d96193cb0ef436987f37d611e0c3c23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2048f8794fef499cb0757fad16437a92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c8da5d1f22f4f2990196f1d1164dc72",
      "placeholder": "",
      "style": "IPY_MODEL_47c11996d2f142779998e1ce52519e62",
      "value": " 1/2 [00:02&lt;00:01,  1.86s/ba]"
     }
    },
    "2691cee6ebda4659afdc7afc3d7c3ccf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "426e41f07cb145408a12146dff293ad6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b31b9c1562a4505bd84039e344eb03a",
      "placeholder": "",
      "style": "IPY_MODEL_ac7711f32d374b74b6f7949db6d2cb94",
      "value": "Running tokenizer on validation dataset:  50%"
     }
    },
    "47c11996d2f142779998e1ce52519e62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b31b9c1562a4505bd84039e344eb03a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e6edbccc9224ba89714e6d0e67fe76e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aafb2c69887549648e9c7044d6a344f3",
       "IPY_MODEL_008a7cb40017467b8d4450e52e0c3ec5",
       "IPY_MODEL_1193886e44d94cc9927d05680e26348d"
      ],
      "layout": "IPY_MODEL_01bbb58eeced4ed4920756d513870c5f"
     }
    },
    "508778f87d1a4733a86bc7466ce6c689": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e01536f748a4689b0c1ab32258b0161": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "865bc4814f3742cdb1d2230f141a8e72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e01536f748a4689b0c1ab32258b0161",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f6cc64f17d074ff68cced49559981438",
      "value": 1
     }
    },
    "877d1854199e4362877dd8774f10bc23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_426e41f07cb145408a12146dff293ad6",
       "IPY_MODEL_865bc4814f3742cdb1d2230f141a8e72",
       "IPY_MODEL_2048f8794fef499cb0757fad16437a92"
      ],
      "layout": "IPY_MODEL_92b9d2b9889a4d92a3907c5791df3091"
     }
    },
    "92b9d2b9889a4d92a3907c5791df3091": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aafb2c69887549648e9c7044d6a344f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08e3f965a5ea4b338aba411dbb969ec6",
      "placeholder": "",
      "style": "IPY_MODEL_b4943d26f11e46358a64ca534efade84",
      "value": "Running tokenizer on train dataset:  90%"
     }
    },
    "ac7711f32d374b74b6f7949db6d2cb94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b4943d26f11e46358a64ca534efade84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "edd1fc29c36a4beda3528a5b962d5d4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6cc64f17d074ff68cced49559981438": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
