{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d24085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "multiprocess 0.70.13 requires dill>=0.3.5.1, but you have dill 0.3.1.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install -Uq apache_beam mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44970d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c6374e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang_code = am\n",
      "source_lang_code = en\n",
      "base_model = xlm-roberta-base\n",
      "adapter_type = pfeiffer\n",
      "tmp_folder = tmp/\n",
      "show_bar = False\n",
      "use_en_train = False\n",
      "translate_en_to_lang = False\n",
      "freeze_head = False\n",
      "translate_low_resource = False\n",
      "train_da = False\n",
      "da_method = dann\n",
      "da_Ch = 256\n",
      "da_Dh = 256\n",
      "da_lr = 0.001\n",
      "finetune_style = stack\n",
      "train_lm = False\n",
      "lm_zero_shot = False\n",
      "lm_src_data = (None, None)\n",
      "lm_tgt_data = ('am', '20221120')\n",
      "lm_lr = 0.0001\n",
      "lm_epochs = 10\n",
      "lm_gradient_accumulation_steps = 4\n",
      "lm_per_device_batch_size = 2\n",
      "mlm_probability = 0.15\n",
      "lm_zero_src_lm_adapter = en/wiki@ukp\n",
      "lm_zero_tgt_lm_adapter = train\n",
      "lr = 0.0001\n",
      "train_epochs = 6\n",
      "per_device_batch_size = 32\n",
      "gradient_accumulation_steps = 1\n",
      "seed = 42069\n",
      "\n",
      " ================================================== \n",
      "\n",
      "loaded 4188 source train samples and 1796 source test samples\n",
      "4188 training samples, 1796 test samples\n",
      "Final evaluation data\n",
      "eval_loss : 1.0185946226119995\n",
      "eval_am_dev_acc : 0.52728285077951\n",
      "eval_am_dev_weighted_f1 : 0.36408083097936267\n",
      "eval_am_dev_balanced_accurancy : 0.3333333333333333\n",
      "eval_runtime : 6.0037\n",
      "eval_samples_per_second : 299.15\n",
      "eval_steps_per_second : 9.494\n",
      "epoch : 6.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaAdapterModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4188\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 786\n",
      "Saving model checkpoint to tmp/checkpoint-500\n",
      "Configuration saved in tmp/checkpoint-500\\sa\\adapter_config.json\n",
      "Module weights saved in tmp/checkpoint-500\\sa\\pytorch_adapter.bin\n",
      "Configuration saved in tmp/checkpoint-500\\sa\\head_config.json\n",
      "Module weights saved in tmp/checkpoint-500\\sa\\pytorch_model_head.bin\n",
      "Configuration saved in tmp/checkpoint-500\\sa\\head_config.json\n",
      "Module weights saved in tmp/checkpoint-500\\sa\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "!stdbuf -oL -eL python run_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94459e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!stdbuf -oL -eL python run_train.py --freeze_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872c9ae1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!stdbuf -oL -eL python run_train.py --use_en_train --translate_low_resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246c9e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!stdbuf -oL -eL python run_train.py --translate_en_to_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d001f485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!stdbuf -oL -eL python run_train.py --train_lm --lm_zero_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27430d2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!stdbuf -oL -eL python run_train.py --use_en_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cdcd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!stdbuf -oL -eL python run_train.py --lang_code pcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1985130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!stdbuf -oL -eL python run_train.py --lang_code pcm --use_en_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fce680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
