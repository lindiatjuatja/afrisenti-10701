{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "G7_P79Ut8wW2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoAdapterModel, AdapterConfig, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = pd.read_csv('translated_train_all.csv')\n",
    "en_test = pd.read_csv('translated_test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy birthday Roman Abramovich. Treat yoursel...</td>\n",
       "      <td>positive</td>\n",
       "      <td>መልካም ልደት የሮማውያን የሮማውያን አብርሃም.በጥር ወር ውስጥ ጥሩ ወዳጃ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tampa Bay Buccaneers: Game-by-Game Predictions...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ታምፓ ቤተር ቡካነሮች-የጨዋታ-ጨዋታ-ጨዋታ ትንበያ ለ 2 ኛው ግማሽ ጊዜ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Interesting: Sacha Baron Cohen to play  Freddi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>የሚስብ: SACHA ባሮን ኮሮን በኋለኛው ባዮቲክ ውስጥ ፍሬድዲ ሜርኬሪ P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DealBook: Clearwire Is Sought by Sprint for Sp...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Selegbobe: ማፅዳት ለትርፍ ይፈለጋል-ሐሙስ በሚሽከረከርበት ጊዜ ሐሙ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BREAKING NEWS...........Man utd have placed Ho...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ሰበር ዜና ......... የሰው ልጅ UTD አዲስ ፈራጅ ማርክ ክሬቴንትበ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9578</th>\n",
       "      <td>Good night out with the boys,party bus down to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>ጥሩ ምሽት ከወንዶቹ, ነገ ከከተማው ጋር ነገ ከከተማይቱ (ኦስ ኦስ ቅዳሜ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9579</th>\n",
       "      <td>I wanna go crazy with Zayn till we see the sun...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ፀሐይን እስክናይ ድረስ ከጽን ጋር እብድ እፈልጋለሁ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9580</th>\n",
       "      <td>@Pike_JSpell are y'all going to come jam with ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@Pike_jesplownsss ነገ ነገ ኮንትሮባንድ ጋር መምጣት የሚሄዱት?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9581</th>\n",
       "      <td>@Nessaa456 the 6th chapter talks about malcolm...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@ Sensao456 የ 6 ኛ ምዕራፍ ስለ ማልኮም ኤክስ ነው እና እንደማስ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9582</th>\n",
       "      <td>March Fourth Marching Band is sound checking a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>መጋቢት አራተኛ መጋረጫ ባንድ የድምፅ ማጣሪያ እና ጥሩ ይመስላል.ዛሬ ማታ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9583 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    labels  \\\n",
       "0     Happy birthday Roman Abramovich. Treat yoursel...  positive   \n",
       "1     Tampa Bay Buccaneers: Game-by-Game Predictions...   neutral   \n",
       "2     Interesting: Sacha Baron Cohen to play  Freddi...  positive   \n",
       "3     DealBook: Clearwire Is Sought by Sprint for Sp...   neutral   \n",
       "4     BREAKING NEWS...........Man utd have placed Ho...   neutral   \n",
       "...                                                 ...       ...   \n",
       "9578  Good night out with the boys,party bus down to...  positive   \n",
       "9579  I wanna go crazy with Zayn till we see the sun...   neutral   \n",
       "9580  @Pike_JSpell are y'all going to come jam with ...   neutral   \n",
       "9581  @Nessaa456 the 6th chapter talks about malcolm...   neutral   \n",
       "9582  March Fourth Marching Band is sound checking a...  positive   \n",
       "\n",
       "                                             translated  \n",
       "0     መልካም ልደት የሮማውያን የሮማውያን አብርሃም.በጥር ወር ውስጥ ጥሩ ወዳጃ...  \n",
       "1     ታምፓ ቤተር ቡካነሮች-የጨዋታ-ጨዋታ-ጨዋታ ትንበያ ለ 2 ኛው ግማሽ ጊዜ ...  \n",
       "2     የሚስብ: SACHA ባሮን ኮሮን በኋለኛው ባዮቲክ ውስጥ ፍሬድዲ ሜርኬሪ P...  \n",
       "3     Selegbobe: ማፅዳት ለትርፍ ይፈለጋል-ሐሙስ በሚሽከረከርበት ጊዜ ሐሙ...  \n",
       "4     ሰበር ዜና ......... የሰው ልጅ UTD አዲስ ፈራጅ ማርክ ክሬቴንትበ...  \n",
       "...                                                 ...  \n",
       "9578  ጥሩ ምሽት ከወንዶቹ, ነገ ከከተማው ጋር ነገ ከከተማይቱ (ኦስ ኦስ ቅዳሜ...  \n",
       "9579               ፀሐይን እስክናይ ድረስ ከጽን ጋር እብድ እፈልጋለሁ ...  \n",
       "9580  @Pike_jesplownsss ነገ ነገ ኮንትሮባንድ ጋር መምጣት የሚሄዱት?...  \n",
       "9581  @ Sensao456 የ 6 ኛ ምዕራፍ ስለ ማልኮም ኤክስ ነው እና እንደማስ...  \n",
       "9582  መጋቢት አራተኛ መጋረጫ ባንድ የድምፅ ማጣሪያ እና ጥሩ ይመስላል.ዛሬ ማታ...  \n",
       "\n",
       "[9583 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>eng_translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>am_train_01074</td>\n",
       "      <td>ለትራምፕ ስንሳቀቅ ህውሀት ከመቀሌ ደሞ አልወሀድም ብላለች  እና አሁን ህ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Habitheu says that when we laughed for the gru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>am_train_05469</td>\n",
       "      <td>ለኔ ታላቅ ክብር ነው! እነበመባሌ የተሰማኝን ትልቅ ክብር በቃላት ልገልፀ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>It is a great glory for me!I can't express it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5306</th>\n",
       "      <td>am_train_05307</td>\n",
       "      <td>በስህተት ወደ አካውንታቸው የገባውን አንድ ሚልዮን ብር በቅንነት የመለሱት...</td>\n",
       "      <td>positive</td>\n",
       "      <td>In an error, the individual who first came to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>am_train_01322</td>\n",
       "      <td>የሴቶች ደህንነት እንደ ዜጋ ቅድሚያ ይልተሰጠ... የችግኝን ያህል ግድ ያ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Women's Safety as a Citizen ... What is the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>am_train_04845</td>\n",
       "      <td>ነገሮችን እንደአመጣጣቸው መቀበል የግድ ነው  በዛው እሳቤ ኑሮ ትቂት ጊዜ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>It is a necessity that we are coming as their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>am_train_05701</td>\n",
       "      <td>ጀግና ኢትዮጵያዊ ሁሌም ከእናንተ ጋር ነን</td>\n",
       "      <td>positive</td>\n",
       "      <td>Hero Ethiopian are always with you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5129</th>\n",
       "      <td>am_train_05130</td>\n",
       "      <td>ለመልካም ዕድልሲል በአውሮፕላን ሞተር ውስጥ ሳንቲም የወረወረው ግለሰብ ተ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>The penny in a plane engine for fine lines!For...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>am_train_03250</td>\n",
       "      <td>ድርጊቱ መፈፀሙ ያልተገባ ቢሆንም ልዩ ልዩ ትንኮሳዎች ቢፈታተኑንም ቅሉ ዳ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Although the action is not unworthy, the struc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>am_train_01209</td>\n",
       "      <td>ትዕግስት ማለት ትዕግስት ማለት ብቻ አይደለም</td>\n",
       "      <td>negative</td>\n",
       "      <td>Trudy is not just Trudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>am_train_03869</td>\n",
       "      <td>በተግባር ድርጅትህ የሚያደርገው አንተም ዛሬ የደረከው ነው፣አዲስ ነገር አ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>In practice, your organization does not do it ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4188 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID                                              tweet  \\\n",
       "1073  am_train_01074  ለትራምፕ ስንሳቀቅ ህውሀት ከመቀሌ ደሞ አልወሀድም ብላለች  እና አሁን ህ...   \n",
       "5468  am_train_05469  ለኔ ታላቅ ክብር ነው! እነበመባሌ የተሰማኝን ትልቅ ክብር በቃላት ልገልፀ...   \n",
       "5306  am_train_05307  በስህተት ወደ አካውንታቸው የገባውን አንድ ሚልዮን ብር በቅንነት የመለሱት...   \n",
       "1321  am_train_01322  የሴቶች ደህንነት እንደ ዜጋ ቅድሚያ ይልተሰጠ... የችግኝን ያህል ግድ ያ...   \n",
       "4844  am_train_04845  ነገሮችን እንደአመጣጣቸው መቀበል የግድ ነው  በዛው እሳቤ ኑሮ ትቂት ጊዜ...   \n",
       "...              ...                                                ...   \n",
       "5700  am_train_05701                         ጀግና ኢትዮጵያዊ ሁሌም ከእናንተ ጋር ነን   \n",
       "5129  am_train_05130  ለመልካም ዕድልሲል በአውሮፕላን ሞተር ውስጥ ሳንቲም የወረወረው ግለሰብ ተ...   \n",
       "3249  am_train_03250  ድርጊቱ መፈፀሙ ያልተገባ ቢሆንም ልዩ ልዩ ትንኮሳዎች ቢፈታተኑንም ቅሉ ዳ...   \n",
       "1208  am_train_01209                       ትዕግስት ማለት ትዕግስት ማለት ብቻ አይደለም   \n",
       "3868  am_train_03869  በተግባር ድርጅትህ የሚያደርገው አንተም ዛሬ የደረከው ነው፣አዲስ ነገር አ...   \n",
       "\n",
       "         label                                     eng_translated  \n",
       "1073  negative  Habitheu says that when we laughed for the gru...  \n",
       "5468  positive  It is a great glory for me!I can't express it ...  \n",
       "5306  positive  In an error, the individual who first came to ...  \n",
       "1321  negative  Women's Safety as a Citizen ... What is the pr...  \n",
       "4844  positive  It is a necessity that we are coming as their ...  \n",
       "...        ...                                                ...  \n",
       "5700  positive                 Hero Ethiopian are always with you  \n",
       "5129  positive  The penny in a plane engine for fine lines!For...  \n",
       "3249   neutral  Although the action is not unworthy, the struc...  \n",
       "1208  negative                            Trudy is not just Trudy  \n",
       "3868   neutral  In practice, your organization does not do it ...  \n",
       "\n",
       "[4188 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "am_train = pd.read_csv('am_train_translated.csv')\n",
    "am_train, am_dev, am_test = np.split(\n",
    "    am_train.sample(frac=1, random_state=42), [int(.7*len(am_train)), int(.8*len(am_train))])\n",
    "\n",
    "am_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train = pd.concat([\n",
    "    am_train[['eng_translated', 'label', 'tweet']].rename(columns={\n",
    "        'eng_translated':'en', 'label':'labels', 'tweet':'am'\n",
    "    }),\n",
    "    en_train[['text', 'labels', 'translated']].rename(columns={\n",
    "        'text':'en', 'translated':'am'\n",
    "    })\n",
    "])\n",
    "\n",
    "combined_test = pd.concat([\n",
    "    am_test[['eng_translated', 'label', 'tweet']].rename(columns={\n",
    "        'eng_translated':'en', 'label':'labels', 'tweet':'am'\n",
    "    }),\n",
    "    am_dev[['eng_translated', 'label', 'tweet']].rename(columns={\n",
    "        'eng_translated':'en', 'label':'labels', 'tweet':'am'\n",
    "    }),\n",
    "    en_test[['text', 'labels', 'translated']].rename(columns={\n",
    "        'text':'en', 'translated':'am'\n",
    "    })\n",
    "])\n",
    "test_split_lengths = [('am_test', len(am_test)), ('am_dev', len(am_dev)), ('en_test', len(en_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "X_9irGwG8wXB",
    "outputId": "129bb934-2713-4d49-b138-f7a78a181d14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13771"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label2id = {\"positive\":0, \"neutral\":1, 'negative':2}\n",
    "id2label = {0:\"positive\", 1:\"neutral\", 2:'negative'}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def encode_batch(row):\n",
    "    full_dict = dict()\n",
    "    for name in ['en', 'am']:\n",
    "        text = ' '.join(filter(lambda x:x[0]!='@', row[name].split() if type(row[name])==str else [' ']))\n",
    "        out = tokenizer(text, max_length=100, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "        out['labels'] = torch.LongTensor([label2id[row.labels]])[0]\n",
    "        for key, value in out.items():\n",
    "            full_dict[f'{name}_{key}'] = value\n",
    "    return full_dict\n",
    "\n",
    "train = combined_train.apply(encode_batch, axis=1).reset_index()[0]\n",
    "\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "22-fOD6t8wXE",
    "outputId": "edaffbd5-38fb-49d0-b83d-be8cb0579a05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4934"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = combined_test.apply(encode_batch, axis=1).reset_index()[0]\n",
    "\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaAdapterModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "en_model = AutoAdapterModel.from_pretrained('xlm-roberta-base')\n",
    "en_model.add_adapter(\"sa\")\n",
    "en_model.train_adapter(\"sa\")\n",
    "en_model.add_classification_head(\"sa\", num_labels=3)\n",
    "en_model.set_active_adapters(\"sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "9hD4HpNV8wXQ",
    "outputId": "96a4151d-fc28-486f-dbbe-413a6acf0595"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaAdapterModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "am_model = AutoAdapterModel.from_pretrained('xlm-roberta-base')\n",
    "am_model.add_adapter(\"sa\")\n",
    "am_model.train_adapter(\"sa\")\n",
    "# am_model.add_classification_head(\"sa\", num_labels=3)\n",
    "am_model.set_active_adapters(\"sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationHead(\n",
       "  (0): Dropout(p=0.1, inplace=False)\n",
       "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (2): Activation_Function_Class(\n",
       "    (f): Tanh()\n",
       "  )\n",
       "  (3): Dropout(p=0.1, inplace=False)\n",
       "  (4): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = en_model.heads['sa']\n",
    "en_model.delete_head('sa')\n",
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.8255e-02,  2.6466e-01,  1.4748e-01,  4.9844e-01, -2.2852e-02,\n",
       "          3.3957e-01,  4.1229e-01, -4.4294e-01,  1.2508e-01, -1.7262e-01,\n",
       "          1.1965e-01,  1.2945e-01,  3.8452e-01,  3.4946e-01, -2.1315e-01,\n",
       "         -1.9194e-01,  1.9861e-01,  4.1296e-01, -3.8165e-02, -2.4928e-01,\n",
       "         -2.7022e-01,  3.2548e-01, -6.6433e-01, -5.3539e-01, -2.2196e-01,\n",
       "          5.8153e-01,  1.2145e-01, -3.1469e-01, -1.4031e-01,  6.4794e-01,\n",
       "          1.4161e-01,  4.7685e-01, -2.5686e-01,  9.2925e-02,  9.6632e-02,\n",
       "         -1.7666e-01,  3.9255e-01,  2.1039e-01,  4.7253e-01,  3.1692e-01,\n",
       "          1.1767e-01, -1.3166e-01, -1.1009e-01,  2.4308e-01,  2.2894e-01,\n",
       "         -2.5825e-01,  1.8567e-01, -5.4735e-02, -2.2145e-01,  3.8256e-01,\n",
       "          5.7466e-01, -2.5924e-01,  4.5987e-02,  4.0382e-02,  1.6140e-01,\n",
       "          1.7561e-01,  3.2409e-01, -2.9713e-01, -2.5705e-01, -5.0585e-01,\n",
       "          2.1902e-02,  6.0602e-01,  4.0996e-01,  3.3709e-01,  3.1988e-01,\n",
       "         -5.7198e-01,  1.1909e-01, -2.0756e-02, -6.2340e-01,  1.9347e-01,\n",
       "          5.3089e-02, -3.3533e-01,  3.2284e-01,  5.6416e-02,  1.4626e-01,\n",
       "          4.5925e-02,  3.8278e-01,  1.4781e-01,  3.8781e-01,  2.9401e-01,\n",
       "          3.0616e-01, -3.5158e-01,  1.1528e-01, -2.3857e-01,  1.0281e-01,\n",
       "          6.6138e-01, -3.9020e-01,  7.5808e-01,  3.0480e-01,  2.9584e-01,\n",
       "         -1.8896e-01, -2.2616e-02, -2.9407e-01,  3.0040e-01, -3.7175e-01,\n",
       "          3.5479e-01, -3.4791e-01, -5.7905e-01,  1.3161e-01, -2.6847e-02,\n",
       "         -2.9182e-01, -4.0102e-01,  2.4389e-01, -4.6808e-01, -9.9898e-02,\n",
       "         -7.4028e-01,  5.0585e-01,  4.6028e-01, -1.9662e-01,  3.9187e-01,\n",
       "          1.9136e-01,  1.4670e-01,  3.2229e-01, -6.6229e-01,  8.7772e-02,\n",
       "          3.4196e-01,  5.3006e-01,  1.4219e-01,  3.3562e-02, -2.6140e-01,\n",
       "         -2.5041e-01,  5.4796e-01, -3.5712e-01, -1.4757e-01,  3.6175e-01,\n",
       "          9.3314e-02, -2.9771e-01,  6.4366e-01,  3.0835e-01, -7.9207e-02,\n",
       "          4.6256e-01, -3.1402e-01,  1.7569e-01, -5.6844e-01, -4.6885e-02,\n",
       "         -2.5434e-01,  5.0339e-01, -3.8451e-01, -3.4014e-02, -5.5003e-01,\n",
       "          3.5715e-01, -6.3842e-02, -1.7751e-01,  4.5712e-01, -2.0136e-01,\n",
       "          4.9119e-01,  5.0848e-01,  1.2156e-01, -2.8825e-01, -2.3991e-03,\n",
       "          6.2156e-01, -2.9874e-01,  1.8853e-01, -1.6426e-01,  1.4411e-01,\n",
       "          4.9188e-01, -4.8532e-02,  1.9506e-01, -1.9925e-01,  1.5389e-01,\n",
       "         -1.2836e-01,  3.0775e-01, -1.7311e-01, -3.3449e-01, -4.3438e-01,\n",
       "          2.3206e-01,  1.8088e-01,  7.5625e-01, -1.2763e-01,  2.8595e-01,\n",
       "         -2.6222e-01,  4.2754e-02,  4.8836e-01, -1.3573e-01, -2.4850e-01,\n",
       "         -5.2952e-01, -1.2992e-01, -3.6429e-01,  3.5228e-01, -7.4800e-03,\n",
       "         -3.3252e-01, -2.7488e-01,  7.2503e-01,  2.9890e-01, -1.1820e-01,\n",
       "          9.4450e-02, -2.0517e-01, -3.7535e-01, -2.9126e-02, -5.2902e-02,\n",
       "         -3.8468e-01, -1.4956e-01,  2.9140e-01, -5.0222e-01, -7.9570e-02,\n",
       "         -3.4624e-01, -8.8894e-02, -2.0701e-01, -3.0470e-01,  4.9956e-01,\n",
       "          3.6825e-01,  6.2466e-02, -3.9145e-01, -1.2993e-01, -6.1981e-03,\n",
       "          3.7645e-02, -2.7553e-01,  3.7439e-01, -1.8422e-02,  1.4969e-01,\n",
       "          3.5532e-02,  2.8233e-01, -5.7477e-02, -6.6085e-01, -3.8188e-01,\n",
       "          2.0731e-01, -7.4888e-01,  3.9472e-01, -2.5543e-01,  3.0673e-01,\n",
       "         -5.2231e-01,  3.8835e-01, -9.8597e-02, -5.5799e-01,  2.7893e-01,\n",
       "          5.8391e-01,  5.6069e-01, -5.1582e-01,  1.8394e-01, -7.4540e-02,\n",
       "          6.7691e-01,  2.9776e-03, -2.6878e-01, -1.9665e-01, -8.8822e-02,\n",
       "         -4.9314e-01,  3.7277e-01,  3.7347e-01, -5.1698e-01,  4.4996e-01,\n",
       "         -2.3053e-01,  6.6104e-02,  3.1577e-01,  1.0886e-01, -5.1560e-03,\n",
       "         -4.9759e-01,  3.0057e-01, -4.7136e-01,  3.0183e-01, -9.6967e-03,\n",
       "         -7.4988e-01, -2.9362e-01,  7.0503e-02,  6.2067e-01, -3.6698e-01,\n",
       "         -4.2264e-01, -3.1177e-02,  4.2185e-01, -5.3207e-01, -2.1914e-01,\n",
       "          1.1786e-01,  7.6132e-02, -1.0340e-01, -6.0035e-01, -4.6313e-01,\n",
       "          8.4666e-02, -1.8374e-02,  2.0442e-01, -2.1545e-01, -1.5563e-01,\n",
       "          4.8779e-01, -2.1057e-02,  2.2906e-01,  1.1783e-01, -3.5553e-01,\n",
       "          3.8340e-02,  1.0277e-01, -5.7221e-01,  2.5996e-01,  6.0625e-02,\n",
       "          7.1114e-01, -3.6555e-01,  3.5446e-01, -1.8411e-01,  1.3256e-01,\n",
       "         -6.7039e-02,  3.9715e-01,  6.7916e-02, -2.1067e-01,  4.7394e-01,\n",
       "         -5.3045e-01, -1.7001e-01,  1.5942e-01,  1.5139e-01, -3.2298e-01,\n",
       "          2.7515e-01, -2.8136e-01, -7.0407e-01,  1.7958e-01, -6.3006e-02,\n",
       "         -7.5662e-01, -7.6804e-01, -2.6694e-01, -2.7986e-01,  5.2543e-01,\n",
       "          2.4963e-01, -5.0996e-01,  1.2420e-02, -1.3536e-01, -1.3953e-02,\n",
       "          4.9825e-01,  2.5760e-01,  2.3486e-01,  3.1689e-02, -4.7941e-01,\n",
       "          3.6468e-01, -5.3499e-01,  3.6879e-02, -1.1532e-01,  2.9970e-01,\n",
       "          1.7638e-01,  1.9345e-01,  7.3248e-02,  6.9090e-01,  3.2639e-02,\n",
       "         -2.0664e-01, -4.2896e-01, -1.3336e-01, -2.4797e-02,  6.2922e-02,\n",
       "          4.3511e-01, -1.5586e-01, -3.7679e-01, -3.3647e-01,  1.4320e-01,\n",
       "          3.9980e-01,  5.5852e-01,  2.3138e-01, -3.1029e-01, -3.4178e-02,\n",
       "         -6.4118e-02, -4.8999e-01, -5.6391e-01,  3.3646e-01, -5.0711e-01,\n",
       "         -4.0177e-01,  2.2010e-01,  2.3932e-02, -4.3487e-01, -9.0357e-02,\n",
       "         -4.9196e-01,  4.2277e-01, -7.3617e-03, -4.6980e-01, -1.1249e-01,\n",
       "          2.0748e-01,  5.2659e-01,  1.4851e-02,  2.3416e-01,  1.1083e-01,\n",
       "         -2.7946e-02,  5.7774e-03,  1.5130e-01,  6.1371e-02,  1.2617e-01,\n",
       "         -5.8480e-01, -5.2241e-02, -2.1026e-01,  5.8773e-01,  9.3675e-02,\n",
       "          8.3827e-02,  7.7636e-02,  2.2017e-01, -3.1582e-01,  2.7397e-01,\n",
       "         -4.0620e-02, -3.5302e-01,  4.0951e-01, -2.1387e-01, -1.9173e-01,\n",
       "          5.0547e-01,  1.6922e-01, -9.5517e-02, -4.9260e-03,  1.8449e-01,\n",
       "          3.5951e-01,  5.4280e-01, -5.2095e-02,  6.6645e-01, -8.5393e-02,\n",
       "          4.9310e-01,  1.2365e-01, -1.5525e-01, -3.1333e-01,  3.0630e-02,\n",
       "         -5.8425e-01,  5.2525e-01, -4.7583e-01,  3.5425e-01, -4.7651e-02,\n",
       "         -3.9214e-01, -4.4626e-01,  6.3681e-01,  4.1661e-01,  3.9969e-01,\n",
       "          1.4765e-01,  1.1885e-01,  5.3274e-01, -6.6628e-01,  2.9521e-01,\n",
       "          3.1670e-01,  1.4241e-01,  1.7185e-01, -3.9729e-01, -1.6664e-01,\n",
       "         -1.5751e-01,  9.3766e-03, -1.9728e-01, -3.1919e-01, -1.4948e-01,\n",
       "          2.8798e-01, -5.7719e-01,  2.5035e-01,  7.4342e-01,  6.5845e-01,\n",
       "         -1.6863e-01,  1.1796e-01, -1.1420e-01, -2.4484e-01, -6.6227e-01,\n",
       "         -2.7018e-02, -1.1184e-01, -4.5389e-01,  1.5249e-01, -1.4391e-01,\n",
       "         -5.5359e-01,  2.1685e-02,  1.7410e-01, -3.8462e-02,  8.8196e-02,\n",
       "         -1.5238e-01,  1.9308e-01,  3.7697e-01, -2.6749e-01, -8.1745e-02,\n",
       "         -2.7246e-01,  1.4055e-01,  3.6785e-01, -1.1049e-01,  5.8568e-02,\n",
       "          2.3977e-01, -3.2357e-01,  5.6576e-01,  1.5569e-01,  6.0906e-01,\n",
       "          3.1277e-01,  3.1373e-02,  2.2844e-01,  1.2899e-01, -3.5527e-01,\n",
       "          3.0641e-01, -4.0706e-01, -8.9128e-02,  1.0639e-01, -4.3804e-01,\n",
       "          1.8667e-01, -2.9121e-01, -7.2233e-01,  2.4755e-01,  3.7237e-01,\n",
       "         -2.9084e-01, -4.3054e-01,  2.6321e-03,  5.9242e-02, -4.5038e-01,\n",
       "         -1.1667e-01,  2.2859e-01, -1.0228e-01,  6.0181e-02,  2.6591e-01,\n",
       "          2.4213e-01, -4.6669e-03, -3.3893e-01,  3.1965e-01, -3.6005e-01,\n",
       "          6.4996e-01, -5.4555e-02,  3.0108e-02, -1.3953e-01, -4.0535e-01,\n",
       "         -1.2637e-01,  6.9465e-02,  1.8277e-01,  6.4285e-01, -1.8491e-01,\n",
       "          2.8164e-01,  1.6396e-01, -3.5584e-01, -4.0770e-01, -5.9605e-01,\n",
       "         -1.2904e-01,  1.0763e-01, -3.3173e-01,  2.3767e-01,  4.1508e-01,\n",
       "         -9.3134e-02, -7.3293e-02, -1.1190e-01,  1.4167e-01,  7.2577e-02,\n",
       "          3.2049e-01,  1.2158e-01, -5.9056e-01, -3.1840e-01, -7.1807e-02,\n",
       "         -3.0406e-01,  5.2641e-01,  3.9025e-01,  4.7004e-02,  2.0468e-01,\n",
       "          5.4673e-01, -3.3281e-01, -2.2371e-02,  1.9492e-01, -1.4764e-01,\n",
       "         -1.8716e-01, -3.1283e-01, -9.8566e-03,  4.5222e-01,  1.5724e-01,\n",
       "         -3.9808e-01, -5.1704e-01,  3.3879e-01,  2.8778e-01, -2.8259e-01,\n",
       "         -2.2674e-01,  6.4647e-01,  1.9456e-01, -7.9308e-02,  1.6207e-01,\n",
       "          3.9248e-01,  8.9740e-03,  3.3044e-01,  2.4554e-01,  7.7190e-01,\n",
       "          1.1765e-01, -1.2171e-01,  2.6865e-01, -5.2802e-01, -3.7931e-01,\n",
       "         -3.2433e-01,  6.6202e-01,  1.1749e-01,  3.7449e-01,  1.2864e-01,\n",
       "          1.1255e-01,  1.9525e-01,  2.4573e-01, -3.1236e-01, -2.7771e-01,\n",
       "          1.2830e-01,  4.6162e-01, -3.0146e-03, -3.0003e-01, -2.2464e-01,\n",
       "          3.9361e-01,  1.3590e-01, -4.2619e-01, -4.1006e-01,  4.0471e-01,\n",
       "          7.2796e-02, -1.0631e-01, -2.5785e-01,  2.9490e-01,  1.7510e-01,\n",
       "          2.6513e-01, -4.2002e-01, -5.1730e-01,  2.9645e-01, -3.1568e-01,\n",
       "         -2.5146e-01, -9.0498e-05,  7.3874e-01,  1.8441e-01, -7.4710e-02,\n",
       "          3.5275e-03, -3.7372e-02, -4.7469e-01, -4.4042e-01,  1.7293e-01,\n",
       "         -7.3649e-01,  2.0492e-01,  4.9697e-01, -1.2138e-01, -2.4624e-01,\n",
       "         -2.9696e-01,  1.7536e-02, -4.7035e-01,  2.3704e-01, -1.3886e-01,\n",
       "         -4.4617e-01,  2.6815e-01, -3.1765e-01, -4.6656e-02, -3.1831e-01,\n",
       "         -1.8308e-01,  3.0838e-01,  7.0097e-01, -4.9116e-01, -2.6559e-01,\n",
       "         -5.9934e-01,  1.6277e-01, -1.4661e-01,  2.8467e-02, -6.2820e-01,\n",
       "         -7.0650e-01,  1.5269e-02,  1.0492e-01,  5.8226e-01, -1.4260e-01,\n",
       "          1.8038e-01,  4.7162e-02,  1.1521e-01, -2.8440e-01,  7.9433e-01,\n",
       "          4.6266e-01, -3.0328e-01, -3.5382e-01, -5.5231e-01, -2.5071e-01,\n",
       "         -3.4936e-01, -3.6975e-01, -1.5139e-01, -4.0274e-01, -6.7727e-02,\n",
       "         -1.1488e-01,  8.1831e-01, -2.8143e-01, -3.5948e-01, -7.6765e-03,\n",
       "         -3.2868e-02, -8.0075e-02,  8.5608e-02,  3.3852e-02, -2.1182e-01,\n",
       "         -2.1432e-01,  2.7977e-01,  6.7583e-02, -9.5459e-03, -3.5703e-01,\n",
       "         -2.8449e-01, -5.7847e-01,  7.9722e-02,  2.1131e-01, -1.9828e-01,\n",
       "         -4.5697e-01,  5.7256e-01,  1.2024e-01,  1.7086e-01, -4.8072e-01,\n",
       "          4.5266e-01, -3.6540e-01,  1.8666e-01, -2.5667e-01, -5.7416e-02,\n",
       "         -3.0348e-01, -2.0833e-01, -1.0609e-01,  1.4804e-01, -1.5682e-02,\n",
       "          4.3541e-01, -1.4736e-01,  4.4885e-01,  1.2303e-01,  2.2359e-03,\n",
       "          2.0351e-01,  4.9150e-01,  5.7040e-02, -6.4936e-01, -2.2066e-01,\n",
       "         -7.6217e-01,  9.9524e-02,  2.1448e-02,  4.4170e-01, -5.9887e-01,\n",
       "         -1.0642e-01,  4.0338e-01,  9.6000e-02, -2.1734e-01,  1.8460e-01,\n",
       "         -3.3635e-01,  6.3046e-01,  4.3923e-01,  2.9475e-01, -2.0527e-03,\n",
       "          3.6642e-01,  5.9967e-01,  1.2769e-01, -8.0613e-02, -6.5019e-01,\n",
       "          2.0648e-02,  2.5862e-01,  1.5650e-01, -1.3104e-01,  1.3319e-01,\n",
       "         -6.8480e-01, -3.5156e-01,  3.7474e-01, -5.1350e-01, -9.7412e-02,\n",
       "          3.3841e-01, -1.1283e-01,  3.0492e-01, -5.9979e-01, -3.9503e-01,\n",
       "         -3.7227e-01, -1.8983e-01,  2.1469e-01, -4.1975e-01,  6.5566e-02,\n",
       "         -4.4345e-01, -1.0916e-01, -3.0018e-02,  2.0200e-02, -2.7286e-01,\n",
       "         -2.2470e-01, -3.6743e-01,  2.6786e-01, -8.6958e-02, -1.8605e-01,\n",
       "          6.3977e-02, -9.5089e-02, -5.6595e-03, -3.1057e-01, -1.0503e-01,\n",
       "         -2.8277e-01, -3.6762e-01, -2.3331e-01, -1.2073e-01, -2.1392e-01,\n",
       "          3.4375e-01, -3.9688e-01,  1.0844e-02, -5.1992e-02,  1.5028e-01,\n",
       "         -3.0108e-01,  8.0516e-01,  5.0078e-02, -1.2858e-01, -5.0094e-01,\n",
       "          2.5981e-01, -1.1914e-02, -4.2741e-01, -7.1636e-02, -1.4350e-01,\n",
       "         -1.8278e-01,  3.7278e-01,  5.7567e-01,  6.0813e-01, -5.4614e-01,\n",
       "         -1.0869e-01,  6.0062e-01, -2.8299e-01,  4.8424e-01, -3.9559e-01,\n",
       "         -9.5814e-02, -5.5018e-02,  1.3458e-01]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum = train.iloc[0]\n",
    "am_model(datum['am_input_ids'], datum['am_attention_mask']).pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1630, -0.2444,  0.0464,  0.6477,  0.1834,  0.6088,  0.3035, -0.4116,\n",
       "          0.2670,  0.0207,  0.1809,  0.0638,  0.0525,  0.3862, -0.4753, -0.1986,\n",
       "          0.4439,  0.0101,  0.1767,  0.0474, -0.5748,  0.4201, -0.7491, -0.6483,\n",
       "         -0.2721,  0.5789, -0.0101, -0.0829,  0.0570,  0.5318,  0.1083,  0.3474,\n",
       "         -0.2996,  0.1450,  0.4583,  0.0035,  0.0954,  0.5226,  0.6545,  0.5353,\n",
       "          0.0276, -0.2630, -0.1655, -0.0813,  0.4258, -0.1560,  0.1858,  0.1726,\n",
       "         -0.2186,  0.2704,  0.4518,  0.2379, -0.0758,  0.1101,  0.1629,  0.3347,\n",
       "          0.3880, -0.4262,  0.0114, -0.3948, -0.2267,  0.4753,  0.1453,  0.4607,\n",
       "          0.2998, -0.5394,  0.1787, -0.1700, -0.6143,  0.3009,  0.0306, -0.1799,\n",
       "          0.6475, -0.1543,  0.1878, -0.0900,  0.4939,  0.2990, -0.1266,  0.1899,\n",
       "         -0.2604,  0.0056, -0.0219,  0.1530,  0.0889,  0.6042, -0.6138,  0.8332,\n",
       "          0.2683,  0.0294,  0.0723,  0.0958, -0.3228,  0.3668, -0.5065,  0.1948,\n",
       "         -0.2429, -0.5890, -0.4255,  0.2310, -0.3127, -0.2025,  0.0559, -0.4658,\n",
       "         -0.1217, -0.7868,  0.4695,  0.2447, -0.1295,  0.4212,  0.0200,  0.1843,\n",
       "          0.3149, -0.2886,  0.0933,  0.4978,  0.5515,  0.0483, -0.1188, -0.5043,\n",
       "          0.0467,  0.5121, -0.2164, -0.1600,  0.1886,  0.2961, -0.3249,  0.5601,\n",
       "          0.1329,  0.2274,  0.5593, -0.3520,  0.3647, -0.5846, -0.2182,  0.1715,\n",
       "          0.3976, -0.1204,  0.4366, -0.5008,  0.4892,  0.1651, -0.5048,  0.2112,\n",
       "         -0.1552,  0.6650,  0.5839,  0.0616, -0.1252,  0.1725,  0.2950, -0.3880,\n",
       "          0.1344, -0.4932,  0.0206,  0.5586, -0.0606,  0.5497, -0.3433,  0.2833,\n",
       "         -0.2814,  0.1695, -0.4617, -0.3119, -0.2708,  0.2098,  0.3717,  0.5638,\n",
       "         -0.1254, -0.1285, -0.2414,  0.2849,  0.5762,  0.2231, -0.3362, -0.5449,\n",
       "         -0.1753,  0.1094,  0.3003, -0.1206,  0.0892, -0.3939,  0.6648,  0.2532,\n",
       "         -0.2348, -0.2903,  0.0369, -0.2993,  0.1469, -0.1645, -0.2903, -0.3003,\n",
       "         -0.1412, -0.4545, -0.5314, -0.1912, -0.3672, -0.1178, -0.4323,  0.4645,\n",
       "          0.4349,  0.1599, -0.5937, -0.3185,  0.3095, -0.0366, -0.4104, -0.0525,\n",
       "         -0.0933,  0.1505, -0.2216,  0.4276, -0.1429, -0.5025, -0.4687,  0.2456,\n",
       "         -0.6524,  0.2448, -0.4092,  0.3402, -0.1495,  0.3249,  0.0836, -0.5485,\n",
       "          0.5217,  0.6753,  0.5113, -0.4161,  0.2124,  0.0587,  0.5796,  0.0603,\n",
       "          0.0824,  0.0769,  0.2984, -0.5427,  0.2187,  0.3229, -0.5459,  0.5642,\n",
       "         -0.2340,  0.2930, -0.0418, -0.1179,  0.0495, -0.4311, -0.1144, -0.4026,\n",
       "          0.0425,  0.1578, -0.5623,  0.1417, -0.0498,  0.7090, -0.1751, -0.2833,\n",
       "          0.0275,  0.4708, -0.6493,  0.0274,  0.0488,  0.0616,  0.0016, -0.2320,\n",
       "         -0.3334,  0.2428, -0.0725,  0.0269, -0.1794, -0.1791,  0.4434,  0.2380,\n",
       "          0.3293, -0.2353, -0.4737, -0.0079,  0.1258, -0.1731,  0.0870,  0.2325,\n",
       "          0.6761, -0.0116,  0.2567, -0.1684,  0.0594, -0.1182,  0.3871,  0.0072,\n",
       "          0.1733,  0.3104, -0.4375, -0.4391,  0.2203,  0.3213, -0.2414,  0.3148,\n",
       "         -0.2945, -0.6589,  0.4557, -0.0432, -0.6869, -0.7975, -0.3192, -0.6469,\n",
       "          0.5363,  0.4820, -0.6762,  0.1097, -0.2434,  0.4570,  0.4950,  0.2168,\n",
       "          0.2119, -0.0334, -0.6596, -0.0526, -0.4252,  0.0124, -0.2942,  0.0702,\n",
       "          0.2240,  0.0054,  0.3377,  0.7120, -0.0642, -0.2327, -0.0827,  0.0830,\n",
       "         -0.4393,  0.0969,  0.3314,  0.0804, -0.3982, -0.0409,  0.0016, -0.3942,\n",
       "          0.1198,  0.5263, -0.0713, -0.1781, -0.0584, -0.4527, -0.2475,  0.0452,\n",
       "         -0.5668, -0.4136, -0.1103, -0.0139, -0.2586, -0.2528, -0.2397,  0.4751,\n",
       "          0.0018, -0.5806, -0.1763,  0.1438,  0.3962,  0.2583,  0.3470, -0.1418,\n",
       "          0.2350, -0.0110,  0.0862, -0.0545,  0.0038, -0.5528,  0.2216, -0.4674,\n",
       "          0.4944,  0.2590,  0.4100, -0.1062, -0.4713, -0.3785,  0.3203,  0.1480,\n",
       "         -0.3110,  0.4395, -0.0401, -0.1022,  0.2752,  0.1136, -0.3462, -0.0393,\n",
       "         -0.0888,  0.0938,  0.5148,  0.0879,  0.5811, -0.2321,  0.6483,  0.3637,\n",
       "         -0.1543,  0.0616,  0.1349, -0.6317,  0.5943, -0.3860,  0.3068, -0.2202,\n",
       "         -0.2747, -0.1239,  0.5483,  0.6803,  0.5374,  0.4036, -0.0230,  0.4356,\n",
       "         -0.6951,  0.3914,  0.5399,  0.1129,  0.1923, -0.1178, -0.2589, -0.1402,\n",
       "         -0.0147, -0.3129, -0.2273, -0.2108,  0.0400, -0.6167,  0.1613,  0.4948,\n",
       "          0.4912,  0.0551,  0.5162, -0.3410, -0.2217, -0.3991,  0.4252, -0.1907,\n",
       "         -0.5223,  0.0467, -0.1497, -0.1557,  0.3063,  0.3446,  0.5390,  0.1833,\n",
       "         -0.1578, -0.1515,  0.5377, -0.2130, -0.4324,  0.1916,  0.3082,  0.2588,\n",
       "         -0.3033,  0.3093,  0.4625, -0.3721,  0.5117,  0.0176,  0.5144,  0.1319,\n",
       "         -0.2114, -0.0321,  0.0402, -0.3160,  0.2474, -0.3813, -0.1184, -0.1681,\n",
       "         -0.5559,  0.1954, -0.2879, -0.7198,  0.4378,  0.1276, -0.3136, -0.4977,\n",
       "          0.1508,  0.1916, -0.6359,  0.0575, -0.0179, -0.0861, -0.2158,  0.0791,\n",
       "         -0.1233,  0.0027, -0.2115,  0.1671, -0.5133,  0.6271, -0.3143,  0.0033,\n",
       "          0.0501, -0.6012, -0.3833,  0.0971,  0.0831,  0.6462, -0.1531,  0.1111,\n",
       "          0.1476, -0.2799, -0.2309, -0.6141,  0.0222,  0.0982, -0.2295,  0.0372,\n",
       "          0.2829,  0.0390, -0.1231, -0.0502,  0.1372, -0.2039,  0.3957, -0.1013,\n",
       "         -0.3879, -0.2620,  0.0491,  0.2183,  0.5105,  0.4691,  0.0014,  0.1342,\n",
       "          0.4044, -0.4827,  0.0177,  0.3930, -0.0657,  0.0393,  0.0350,  0.0319,\n",
       "          0.6135,  0.2349, -0.2734, -0.2840,  0.2261,  0.1954,  0.0228, -0.3385,\n",
       "          0.4546, -0.0811, -0.1933,  0.4216,  0.3052, -0.0547,  0.4031,  0.0889,\n",
       "          0.7354,  0.1396, -0.3201, -0.0280, -0.5758, -0.2885, -0.4938,  0.5558,\n",
       "          0.2574,  0.4888,  0.3010,  0.0333,  0.1745,  0.0762, -0.2124, -0.2731,\n",
       "          0.0313,  0.5334,  0.1584, -0.2698,  0.0055,  0.4102,  0.0662, -0.5130,\n",
       "         -0.3557,  0.5506, -0.1233, -0.0277, -0.3263,  0.2213,  0.3730,  0.1029,\n",
       "         -0.4736, -0.3630,  0.3466, -0.2770,  0.1475, -0.1170,  0.6525,  0.1240,\n",
       "         -0.0875,  0.2110, -0.0331, -0.4377, -0.2792, -0.2508, -0.5425, -0.2408,\n",
       "          0.5766, -0.0940, -0.1385, -0.4082, -0.1655, -0.2792, -0.0206,  0.1938,\n",
       "         -0.5527,  0.4523, -0.4086, -0.0428, -0.4098, -0.1895,  0.4861,  0.7460,\n",
       "         -0.2033, -0.1238, -0.3787, -0.1920,  0.3709, -0.0848, -0.4261, -0.7301,\n",
       "          0.1481,  0.3559,  0.3277, -0.1460, -0.0580, -0.1410,  0.1000, -0.1149,\n",
       "          0.6020,  0.4612, -0.3167,  0.0702, -0.2610, -0.0684, -0.5842, -0.3775,\n",
       "         -0.2454, -0.1305, -0.2596, -0.2344,  0.7649, -0.4762, -0.3818, -0.0354,\n",
       "         -0.2782,  0.1146, -0.2961, -0.1599, -0.4379,  0.1579,  0.4217,  0.0172,\n",
       "         -0.1071, -0.4907, -0.3103, -0.3003,  0.0964, -0.1157, -0.4571, -0.1812,\n",
       "          0.5955, -0.1566,  0.2861, -0.3730,  0.4470, -0.4699,  0.2934, -0.0523,\n",
       "          0.0086, -0.1370, -0.1785, -0.3702,  0.2301, -0.0052,  0.4549, -0.2037,\n",
       "          0.4466,  0.0577, -0.0539,  0.2868,  0.6766,  0.1835, -0.7509, -0.2759,\n",
       "         -0.7722,  0.0329, -0.0092,  0.1962, -0.6206,  0.1650,  0.3884,  0.0803,\n",
       "         -0.4118,  0.2800, -0.4936,  0.5148,  0.4913,  0.4934, -0.0487,  0.0829,\n",
       "          0.4523,  0.1173, -0.1987, -0.6637, -0.1406,  0.2513,  0.3098,  0.0035,\n",
       "          0.2753, -0.2877, -0.4853,  0.5780, -0.1501, -0.1344,  0.4688, -0.0203,\n",
       "          0.1684, -0.5522, -0.5643, -0.2928, -0.3912, -0.0077, -0.2879, -0.0989,\n",
       "         -0.3910, -0.2007,  0.3013,  0.0081,  0.0924, -0.2286, -0.1872,  0.3056,\n",
       "         -0.3270, -0.2583, -0.2993, -0.1946,  0.5116, -0.1647, -0.3795, -0.3509,\n",
       "         -0.2727, -0.1275,  0.2581,  0.3986,  0.0553, -0.2251,  0.0818,  0.0509,\n",
       "          0.0396, -0.2271,  0.5598, -0.2639, -0.1965, -0.3595,  0.1025, -0.2396,\n",
       "         -0.6886, -0.1404, -0.0239, -0.2077,  0.1078,  0.4523,  0.3343, -0.5524,\n",
       "         -0.2319,  0.7513, -0.1780,  0.5882, -0.5883,  0.0225, -0.1807,  0.4530]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "en_model(datum['en_input_ids'], datum['en_attention_mask']).pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7869e-02,  2.6767e-01,  1.4659e-01,  5.1228e-01,  2.1222e-03,\n",
       "          3.4991e-01,  4.1621e-01, -4.3170e-01,  1.3669e-01, -1.5643e-01,\n",
       "          1.3210e-01,  1.2155e-01,  3.8029e-01,  3.4419e-01, -2.1720e-01,\n",
       "         -1.8610e-01,  1.9871e-01,  4.0466e-01, -5.3914e-02, -2.4750e-01,\n",
       "         -2.7676e-01,  3.2171e-01, -6.6809e-01, -5.3984e-01, -2.2919e-01,\n",
       "          5.7757e-01,  1.4443e-01, -3.1282e-01, -1.5114e-01,  6.3906e-01,\n",
       "          1.3372e-01,  4.5950e-01, -2.6668e-01,  8.4528e-02,  8.0833e-02,\n",
       "         -1.6383e-01,  3.7891e-01,  2.2333e-01,  4.7694e-01,  3.0504e-01,\n",
       "          1.0771e-01, -1.1021e-01, -1.3105e-01,  2.4067e-01,  2.1850e-01,\n",
       "         -2.6004e-01,  1.7227e-01, -5.1929e-02, -2.2440e-01,  3.9848e-01,\n",
       "          5.6024e-01, -2.5976e-01,  5.5520e-02,  5.4747e-02,  1.7706e-01,\n",
       "          1.7315e-01,  2.9831e-01, -2.9641e-01, -2.6570e-01, -5.2391e-01,\n",
       "          2.4735e-02,  6.0531e-01,  4.2172e-01,  3.3452e-01,  3.2311e-01,\n",
       "         -5.5445e-01,  1.3097e-01, -2.8100e-02, -6.1212e-01,  2.0409e-01,\n",
       "          7.6012e-02, -3.2322e-01,  3.2035e-01,  6.4612e-02,  1.5774e-01,\n",
       "          5.6877e-02,  3.9746e-01,  1.6383e-01,  3.7650e-01,  2.8774e-01,\n",
       "          2.9103e-01, -3.5942e-01,  1.2123e-01, -2.3669e-01,  1.0067e-01,\n",
       "          6.7186e-01, -3.9182e-01,  7.5831e-01,  3.1487e-01,  2.9120e-01,\n",
       "         -1.9240e-01, -2.2264e-02, -3.0582e-01,  2.8244e-01, -3.7116e-01,\n",
       "          3.3828e-01, -3.4953e-01, -5.8674e-01,  1.2287e-01, -2.8002e-02,\n",
       "         -2.8887e-01, -3.8783e-01,  2.4462e-01, -4.7696e-01, -1.0930e-01,\n",
       "         -7.4584e-01,  5.0941e-01,  4.7417e-01, -1.8398e-01,  3.9544e-01,\n",
       "          1.8274e-01,  1.4759e-01,  3.2841e-01, -6.5981e-01,  9.6917e-02,\n",
       "          3.6187e-01,  5.2917e-01,  1.3716e-01,  3.5728e-02, -2.7205e-01,\n",
       "         -2.5824e-01,  5.3473e-01, -3.5306e-01, -1.4744e-01,  3.5880e-01,\n",
       "          7.9285e-02, -3.3829e-01,  6.3514e-01,  3.2582e-01, -4.7288e-02,\n",
       "          4.7564e-01, -3.3282e-01,  1.6990e-01, -5.6332e-01, -3.6656e-02,\n",
       "         -2.6081e-01,  5.1446e-01, -3.7311e-01, -8.4416e-03, -5.6180e-01,\n",
       "          3.6669e-01, -5.7389e-02, -1.4751e-01,  4.6007e-01, -2.0319e-01,\n",
       "          4.9002e-01,  5.0643e-01,  1.1453e-01, -2.8469e-01,  1.2467e-02,\n",
       "          6.1185e-01, -2.8373e-01,  1.5659e-01, -1.6270e-01,  1.3413e-01,\n",
       "          4.8531e-01, -4.6305e-02,  1.8302e-01, -1.9592e-01,  1.4850e-01,\n",
       "         -1.1701e-01,  3.1298e-01, -1.6596e-01, -3.3245e-01, -4.4689e-01,\n",
       "          2.3778e-01,  1.6312e-01,  7.5482e-01, -1.2341e-01,  2.7213e-01,\n",
       "         -2.5778e-01,  3.0583e-02,  4.8718e-01, -1.4814e-01, -2.3509e-01,\n",
       "         -5.1553e-01, -1.1979e-01, -3.7336e-01,  3.4974e-01, -5.7468e-03,\n",
       "         -3.2303e-01, -2.8952e-01,  7.2869e-01,  2.9959e-01, -1.2892e-01,\n",
       "          6.9122e-02, -2.1588e-01, -3.7952e-01, -1.8583e-02, -3.3665e-02,\n",
       "         -3.8722e-01, -1.6145e-01,  2.8255e-01, -5.0532e-01, -7.5605e-02,\n",
       "         -3.6046e-01, -1.0160e-01, -1.8516e-01, -3.0233e-01,  4.9813e-01,\n",
       "          3.6470e-01,  7.4300e-02, -3.8705e-01, -1.4256e-01, -3.2754e-02,\n",
       "          4.8556e-02, -2.7366e-01,  3.6900e-01, -1.3406e-02,  1.3594e-01,\n",
       "          2.6645e-02,  2.9408e-01, -5.3734e-02, -6.7040e-01, -3.8655e-01,\n",
       "          1.9495e-01, -7.4892e-01,  4.0003e-01, -2.5373e-01,  2.9763e-01,\n",
       "         -5.2456e-01,  3.7738e-01, -1.0743e-01, -5.6491e-01,  2.6717e-01,\n",
       "          5.9128e-01,  5.7170e-01, -5.1301e-01,  1.9152e-01, -8.1275e-02,\n",
       "          6.7401e-01,  1.5309e-02, -2.6499e-01, -1.9883e-01, -9.1001e-02,\n",
       "         -4.8678e-01,  3.6694e-01,  3.7931e-01, -5.1414e-01,  4.5687e-01,\n",
       "         -2.2563e-01,  6.4121e-02,  3.2498e-01,  9.8138e-02, -5.3615e-03,\n",
       "         -4.9963e-01,  3.0819e-01, -4.8419e-01,  3.2113e-01, -1.3824e-02,\n",
       "         -7.5402e-01, -3.0426e-01,  8.2952e-02,  6.3184e-01, -3.8108e-01,\n",
       "         -4.3131e-01, -2.6421e-02,  4.2526e-01, -5.3043e-01, -2.2153e-01,\n",
       "          1.1608e-01,  5.9889e-02, -9.2555e-02, -6.0165e-01, -4.5672e-01,\n",
       "          8.2900e-02, -1.3211e-02,  1.9906e-01, -2.3117e-01, -1.5425e-01,\n",
       "          4.9370e-01, -3.9634e-02,  2.2984e-01,  9.7483e-02, -3.4932e-01,\n",
       "          3.1679e-02,  1.0463e-01, -5.7734e-01,  2.5821e-01,  4.6680e-02,\n",
       "          7.2032e-01, -3.8083e-01,  3.4764e-01, -1.8516e-01,  1.3291e-01,\n",
       "         -7.2593e-02,  3.8276e-01,  6.2898e-02, -1.8457e-01,  4.7518e-01,\n",
       "         -5.1012e-01, -1.7282e-01,  1.3144e-01,  1.5487e-01, -3.1973e-01,\n",
       "          2.7571e-01, -3.0109e-01, -7.0246e-01,  1.8039e-01, -9.2196e-02,\n",
       "         -7.5553e-01, -7.6907e-01, -2.7229e-01, -2.8243e-01,  5.2300e-01,\n",
       "          2.3410e-01, -5.0923e-01,  1.2743e-02, -1.3953e-01, -3.4427e-03,\n",
       "          5.0137e-01,  2.4615e-01,  2.1490e-01,  1.6128e-02, -4.8373e-01,\n",
       "          3.6524e-01, -5.4931e-01,  4.0816e-02, -1.0027e-01,  2.9119e-01,\n",
       "          1.5606e-01,  1.9536e-01,  6.9885e-02,  6.9201e-01,  3.0606e-02,\n",
       "         -2.1509e-01, -4.3958e-01, -1.2401e-01, -2.4677e-04,  5.6067e-02,\n",
       "          4.1214e-01, -1.6158e-01, -3.9344e-01, -3.3596e-01,  1.5915e-01,\n",
       "          3.9143e-01,  5.5431e-01,  2.3367e-01, -3.1332e-01, -2.3369e-02,\n",
       "         -6.6215e-02, -4.9224e-01, -5.8158e-01,  3.3536e-01, -5.1901e-01,\n",
       "         -3.7740e-01,  2.2368e-01,  3.2814e-02, -4.2468e-01, -8.8323e-02,\n",
       "         -4.8621e-01,  4.1602e-01, -7.9928e-03, -4.7582e-01, -1.2408e-01,\n",
       "          2.1147e-01,  5.2544e-01,  2.4861e-02,  2.1236e-01,  1.2817e-01,\n",
       "         -3.8290e-02,  2.6337e-02,  1.5921e-01,  6.0552e-02,  1.1944e-01,\n",
       "         -5.8997e-01, -4.2063e-02, -2.0300e-01,  5.7996e-01,  1.0455e-01,\n",
       "          8.7537e-02,  5.9988e-02,  2.1130e-01, -3.1772e-01,  2.7342e-01,\n",
       "         -7.5674e-02, -3.5860e-01,  4.2155e-01, -2.3617e-01, -1.9367e-01,\n",
       "          5.1570e-01,  1.8831e-01, -1.0004e-01, -9.6276e-03,  1.6829e-01,\n",
       "          3.4689e-01,  5.4315e-01, -7.6510e-02,  6.5907e-01, -9.5081e-02,\n",
       "          5.0505e-01,  1.1640e-01, -1.6045e-01, -2.9546e-01,  4.5278e-02,\n",
       "         -5.7040e-01,  5.2459e-01, -4.6762e-01,  3.4170e-01, -3.5203e-02,\n",
       "         -3.9646e-01, -4.5560e-01,  6.4028e-01,  4.1662e-01,  3.9849e-01,\n",
       "          1.4738e-01,  1.1670e-01,  5.1634e-01, -6.6220e-01,  3.1469e-01,\n",
       "          3.1908e-01,  1.6702e-01,  1.9817e-01, -4.1223e-01, -1.6431e-01,\n",
       "         -1.5819e-01, -1.5345e-02, -1.9681e-01, -3.2180e-01, -1.3760e-01,\n",
       "          2.9862e-01, -5.6220e-01,  2.5161e-01,  7.4455e-01,  6.6280e-01,\n",
       "         -1.6362e-01,  9.7005e-02, -1.0728e-01, -2.6440e-01, -6.7029e-01,\n",
       "         -3.4202e-02, -9.0782e-02, -4.5403e-01,  1.7307e-01, -1.4565e-01,\n",
       "         -5.4697e-01,  2.1585e-02,  1.8612e-01, -4.1009e-02,  9.5534e-02,\n",
       "         -1.5811e-01,  1.9092e-01,  4.0621e-01, -2.4146e-01, -4.9558e-02,\n",
       "         -2.6874e-01,  1.6003e-01,  3.6549e-01, -1.0768e-01,  4.7513e-02,\n",
       "          2.4547e-01, -3.2679e-01,  5.6675e-01,  1.4417e-01,  6.0912e-01,\n",
       "          3.1481e-01,  2.3024e-02,  2.2064e-01,  1.4235e-01, -3.4035e-01,\n",
       "          3.1506e-01, -4.1320e-01, -8.1552e-02,  1.0013e-01, -4.2830e-01,\n",
       "          1.5747e-01, -2.7474e-01, -7.1233e-01,  2.4795e-01,  3.6297e-01,\n",
       "         -2.7966e-01, -4.1004e-01,  3.4476e-02,  4.1559e-02, -4.4760e-01,\n",
       "         -1.4250e-01,  2.2132e-01, -9.6683e-02,  6.4473e-02,  2.5594e-01,\n",
       "          2.3163e-01,  1.3046e-03, -3.4839e-01,  3.0097e-01, -3.5487e-01,\n",
       "          6.5517e-01, -5.0178e-02,  2.2524e-02, -1.3120e-01, -4.1315e-01,\n",
       "         -1.0494e-01,  7.1364e-02,  2.0935e-01,  6.4548e-01, -1.9248e-01,\n",
       "          2.8485e-01,  1.4454e-01, -3.5460e-01, -3.9682e-01, -5.9187e-01,\n",
       "         -1.4531e-01,  1.0695e-01, -3.5457e-01,  2.4050e-01,  4.2691e-01,\n",
       "         -1.0711e-01, -7.4667e-02, -1.1833e-01,  1.5516e-01,  8.7939e-02,\n",
       "          2.9437e-01,  1.2866e-01, -5.8491e-01, -3.2911e-01, -7.1475e-02,\n",
       "         -2.9976e-01,  5.1838e-01,  3.8846e-01,  8.4094e-02,  1.9016e-01,\n",
       "          5.5691e-01, -3.3238e-01, -2.2299e-02,  2.0205e-01, -1.5444e-01,\n",
       "         -1.9212e-01, -3.2145e-01,  1.4075e-04,  4.6620e-01,  1.5400e-01,\n",
       "         -3.9963e-01, -5.2390e-01,  3.3559e-01,  3.0030e-01, -2.7399e-01,\n",
       "         -2.2863e-01,  6.4974e-01,  1.9431e-01, -9.7352e-02,  1.6691e-01,\n",
       "          3.9410e-01,  2.4532e-02,  3.1695e-01,  2.3996e-01,  7.6616e-01,\n",
       "          1.1643e-01, -1.1565e-01,  2.7519e-01, -5.2933e-01, -3.8796e-01,\n",
       "         -3.2044e-01,  6.7012e-01,  1.2257e-01,  3.8204e-01,  1.2392e-01,\n",
       "          1.2135e-01,  2.0478e-01,  2.2987e-01, -3.1103e-01, -2.6979e-01,\n",
       "          1.3677e-01,  4.7603e-01,  2.3636e-03, -3.0597e-01, -2.0747e-01,\n",
       "          4.0166e-01,  1.4210e-01, -4.1794e-01, -3.9878e-01,  4.0123e-01,\n",
       "          8.0136e-02, -1.0101e-01, -2.5293e-01,  3.0491e-01,  1.8033e-01,\n",
       "          2.6683e-01, -4.1892e-01, -5.1538e-01,  3.1771e-01, -3.2240e-01,\n",
       "         -2.1541e-01, -1.1696e-03,  7.4168e-01,  1.7181e-01, -6.3219e-02,\n",
       "          1.4317e-02, -3.5332e-02, -4.8264e-01, -4.3772e-01,  1.5430e-01,\n",
       "         -7.3777e-01,  2.0662e-01,  4.9081e-01, -1.2063e-01, -2.6255e-01,\n",
       "         -3.0286e-01,  1.7659e-02, -4.8011e-01,  2.2282e-01, -1.3909e-01,\n",
       "         -4.5442e-01,  3.0536e-01, -3.1482e-01, -3.3154e-02, -3.0903e-01,\n",
       "         -1.8530e-01,  2.9289e-01,  7.1124e-01, -4.8868e-01, -2.7179e-01,\n",
       "         -5.9228e-01,  1.5997e-01, -1.4635e-01,  2.5110e-02, -6.2214e-01,\n",
       "         -7.0928e-01,  1.8241e-02,  1.1895e-01,  5.9392e-01, -1.6025e-01,\n",
       "          1.8346e-01,  3.9990e-02,  1.2062e-01, -2.8431e-01,  7.9914e-01,\n",
       "          4.5426e-01, -3.0205e-01, -3.4626e-01, -5.5598e-01, -2.4399e-01,\n",
       "         -3.3137e-01, -3.8663e-01, -1.1923e-01, -3.8010e-01, -3.0772e-02,\n",
       "         -1.1146e-01,  8.1966e-01, -2.6529e-01, -3.6950e-01, -3.6876e-03,\n",
       "         -1.4493e-02, -8.9636e-02,  8.9745e-02,  3.1780e-02, -2.1611e-01,\n",
       "         -2.0406e-01,  2.7256e-01,  4.8614e-02, -1.9234e-02, -3.5180e-01,\n",
       "         -2.6599e-01, -5.8319e-01,  9.7617e-02,  2.2431e-01, -2.0299e-01,\n",
       "         -4.5673e-01,  5.8180e-01,  1.2733e-01,  1.4700e-01, -4.7567e-01,\n",
       "          4.4919e-01, -3.8154e-01,  1.6763e-01, -2.5206e-01, -2.9824e-02,\n",
       "         -3.0352e-01, -2.2776e-01, -1.0165e-01,  1.4171e-01,  1.1825e-03,\n",
       "          4.4744e-01, -1.3871e-01,  4.5439e-01,  1.1172e-01,  2.9759e-02,\n",
       "          2.0787e-01,  4.8613e-01,  4.4900e-02, -6.6492e-01, -2.1083e-01,\n",
       "         -7.6373e-01,  9.2703e-02,  1.8491e-02,  4.2347e-01, -5.9417e-01,\n",
       "         -8.2182e-02,  3.8870e-01,  9.7981e-02, -2.1116e-01,  1.8371e-01,\n",
       "         -3.4863e-01,  6.3771e-01,  4.3087e-01,  2.9596e-01,  4.3834e-04,\n",
       "          3.6925e-01,  6.1113e-01,  1.2037e-01, -9.8355e-02, -6.4637e-01,\n",
       "          3.5912e-02,  2.5091e-01,  1.5185e-01, -1.2905e-01,  1.4517e-01,\n",
       "         -6.7786e-01, -3.5118e-01,  3.7696e-01, -5.0491e-01, -1.0360e-01,\n",
       "          3.7045e-01, -1.2593e-01,  3.1526e-01, -5.9105e-01, -3.7486e-01,\n",
       "         -3.5101e-01, -1.9550e-01,  2.1887e-01, -4.1358e-01,  6.4036e-02,\n",
       "         -4.5978e-01, -1.1896e-01, -4.9858e-02,  2.7524e-02, -2.6648e-01,\n",
       "         -2.1919e-01, -3.6956e-01,  2.7353e-01, -1.0351e-01, -1.8181e-01,\n",
       "          7.4609e-02, -1.0331e-01, -2.3909e-02, -2.7105e-01, -1.2541e-01,\n",
       "         -2.9877e-01, -3.7990e-01, -2.2199e-01, -9.4330e-02, -2.2905e-01,\n",
       "          3.4268e-01, -3.8806e-01,  7.4037e-03, -6.0688e-02,  1.6136e-01,\n",
       "         -3.0266e-01,  8.0417e-01,  6.1895e-02, -1.2532e-01, -4.9987e-01,\n",
       "          2.5203e-01, -7.7982e-03, -4.4022e-01, -8.0328e-02, -1.5520e-01,\n",
       "         -1.7494e-01,  3.5957e-01,  5.7478e-01,  6.1169e-01, -5.3589e-01,\n",
       "         -1.2224e-01,  6.0053e-01, -2.9177e-01,  5.0134e-01, -3.8810e-01,\n",
       "         -9.8340e-02, -5.3059e-02,  1.4232e-01]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(am_model(datum['am_input_ids'], datum['am_attention_mask']))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en_input_ids': tensor([[     0,  33124,  10902,     34,  17378,    450,   3229,    642,  94518,\n",
       "             297,    100,     70,  23322,   2676,     53,    136,   5036,     70,\n",
       "          133836,     83,    959,    142,  53702,    450,     83,    959,  78737,\n",
       "            2481,      4,    136,    442,     83,   2373,     73,   3674,    678,\n",
       "              70,  14537,    111,     70,  25443,     32,      2,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1]]),\n",
       " 'en_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0]]),\n",
       " 'en_labels': tensor(2),\n",
       " 'am_input_ids': tensor([[     0,      6,  21619,   3851,    816,  32014,  65169,   4047, 160807,\n",
       "           43743,   1178,  21608,   1437,  27217,   4656,  10824,  21368,  13357,\n",
       "           16241,   5698,  21608,  85632,  14615,   4712,  72570,   2302,  26261,\n",
       "           43743,   1178,  21608,   1437,      6,  94532,  21090,   4799,    946,\n",
       "          219702,   5519,  44020,  53153,  33659,   5519,  23985,  64408,    623,\n",
       "           82234,  64580,    548, 144172,   2237,  94532,  21090,   4799,    946,\n",
       "           10822,   3008,   6550,  72570, 168624,  10822,  64019,   4974,   6550,\n",
       "           72570,     32,      2,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
       "               1]]),\n",
       " 'am_attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0]]),\n",
       " 'am_labels': tensor(2)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5uob8B8n8wXS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, AdapterTrainer, EvalPrediction\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True\n",
    "#     # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#     remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "def compute_scores(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    i, output = 0, dict()\n",
    "    for name, split_length in test_split_lengths:\n",
    "        s = np.s_[i:i+split_length]\n",
    "        split_preds = preds[s]\n",
    "        split_labels = p.label_ids[s]\n",
    "        output[f'{name}_acc'] = (split_preds==split_labels).mean()\n",
    "        output[f'{name}_weighted_f1'] = f1_score(split_labels, split_preds, average='weighted')\n",
    "        i += split_length\n",
    "    return output\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    compute_metrics=compute_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQ2EpQQa8wXV",
    "outputId": "97b0481f-4ed7-4cbd-fba3-0f074514f128",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9583\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  22/1800 07:42 < 11:25:56, 0.04 it/s, Epoch 0.07/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dh4bkdtt8wXW",
    "outputId": "1b5c34a2-7810-46db-d1f4-1d498ad2143f"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
