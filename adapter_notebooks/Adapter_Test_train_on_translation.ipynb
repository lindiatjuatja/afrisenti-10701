{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "G7_P79Ut8wW2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoAdapterModel, AdapterConfig, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = pd.read_csv('translated_train_all.csv')\n",
    "en_test = pd.read_csv('translated_test_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Happy birthday Roman Abramovich. Treat yoursel...</td>\n",
       "      <td>positive</td>\n",
       "      <td>መልካም ልደት የሮማውያን የሮማውያን አብርሃም.በጥር ወር ውስጥ ጥሩ ወዳጃ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tampa Bay Buccaneers: Game-by-Game Predictions...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ታምፓ ቤተር ቡካነሮች-የጨዋታ-ጨዋታ-ጨዋታ ትንበያ ለ 2 ኛው ግማሽ ጊዜ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Interesting: Sacha Baron Cohen to play  Freddi...</td>\n",
       "      <td>positive</td>\n",
       "      <td>የሚስብ: SACHA ባሮን ኮሮን በኋለኛው ባዮቲክ ውስጥ ፍሬድዲ ሜርኬሪ P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DealBook: Clearwire Is Sought by Sprint for Sp...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Selegbobe: ማፅዳት ለትርፍ ይፈለጋል-ሐሙስ በሚሽከረከርበት ጊዜ ሐሙ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BREAKING NEWS...........Man utd have placed Ho...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ሰበር ዜና ......... የሰው ልጅ UTD አዲስ ፈራጅ ማርክ ክሬቴንትበ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9578</th>\n",
       "      <td>Good night out with the boys,party bus down to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>ጥሩ ምሽት ከወንዶቹ, ነገ ከከተማው ጋር ነገ ከከተማይቱ (ኦስ ኦስ ቅዳሜ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9579</th>\n",
       "      <td>I wanna go crazy with Zayn till we see the sun...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ፀሐይን እስክናይ ድረስ ከጽን ጋር እብድ እፈልጋለሁ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9580</th>\n",
       "      <td>@Pike_JSpell are y'all going to come jam with ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@Pike_jesplownsss ነገ ነገ ኮንትሮባንድ ጋር መምጣት የሚሄዱት?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9581</th>\n",
       "      <td>@Nessaa456 the 6th chapter talks about malcolm...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@ Sensao456 የ 6 ኛ ምዕራፍ ስለ ማልኮም ኤክስ ነው እና እንደማስ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9582</th>\n",
       "      <td>March Fourth Marching Band is sound checking a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>መጋቢት አራተኛ መጋረጫ ባንድ የድምፅ ማጣሪያ እና ጥሩ ይመስላል.ዛሬ ማታ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9583 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    labels  \\\n",
       "0     Happy birthday Roman Abramovich. Treat yoursel...  positive   \n",
       "1     Tampa Bay Buccaneers: Game-by-Game Predictions...   neutral   \n",
       "2     Interesting: Sacha Baron Cohen to play  Freddi...  positive   \n",
       "3     DealBook: Clearwire Is Sought by Sprint for Sp...   neutral   \n",
       "4     BREAKING NEWS...........Man utd have placed Ho...   neutral   \n",
       "...                                                 ...       ...   \n",
       "9578  Good night out with the boys,party bus down to...  positive   \n",
       "9579  I wanna go crazy with Zayn till we see the sun...   neutral   \n",
       "9580  @Pike_JSpell are y'all going to come jam with ...   neutral   \n",
       "9581  @Nessaa456 the 6th chapter talks about malcolm...   neutral   \n",
       "9582  March Fourth Marching Band is sound checking a...  positive   \n",
       "\n",
       "                                             translated  \n",
       "0     መልካም ልደት የሮማውያን የሮማውያን አብርሃም.በጥር ወር ውስጥ ጥሩ ወዳጃ...  \n",
       "1     ታምፓ ቤተር ቡካነሮች-የጨዋታ-ጨዋታ-ጨዋታ ትንበያ ለ 2 ኛው ግማሽ ጊዜ ...  \n",
       "2     የሚስብ: SACHA ባሮን ኮሮን በኋለኛው ባዮቲክ ውስጥ ፍሬድዲ ሜርኬሪ P...  \n",
       "3     Selegbobe: ማፅዳት ለትርፍ ይፈለጋል-ሐሙስ በሚሽከረከርበት ጊዜ ሐሙ...  \n",
       "4     ሰበር ዜና ......... የሰው ልጅ UTD አዲስ ፈራጅ ማርክ ክሬቴንትበ...  \n",
       "...                                                 ...  \n",
       "9578  ጥሩ ምሽት ከወንዶቹ, ነገ ከከተማው ጋር ነገ ከከተማይቱ (ኦስ ኦስ ቅዳሜ...  \n",
       "9579               ፀሐይን እስክናይ ድረስ ከጽን ጋር እብድ እፈልጋለሁ ...  \n",
       "9580  @Pike_jesplownsss ነገ ነገ ኮንትሮባንድ ጋር መምጣት የሚሄዱት?...  \n",
       "9581  @ Sensao456 የ 6 ኛ ምዕራፍ ስለ ማልኮም ኤክስ ነው እና እንደማስ...  \n",
       "9582  መጋቢት አራተኛ መጋረጫ ባንድ የድምፅ ማጣሪያ እና ጥሩ ይመስላል.ዛሬ ማታ...  \n",
       "\n",
       "[9583 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>eng_translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>am_train_01074</td>\n",
       "      <td>ለትራምፕ ስንሳቀቅ ህውሀት ከመቀሌ ደሞ አልወሀድም ብላለች  እና አሁን ህ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Habitheu says that when we laughed for the gru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5468</th>\n",
       "      <td>am_train_05469</td>\n",
       "      <td>ለኔ ታላቅ ክብር ነው! እነበመባሌ የተሰማኝን ትልቅ ክብር በቃላት ልገልፀ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>It is a great glory for me!I can't express it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5306</th>\n",
       "      <td>am_train_05307</td>\n",
       "      <td>በስህተት ወደ አካውንታቸው የገባውን አንድ ሚልዮን ብር በቅንነት የመለሱት...</td>\n",
       "      <td>positive</td>\n",
       "      <td>In an error, the individual who first came to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>am_train_01322</td>\n",
       "      <td>የሴቶች ደህንነት እንደ ዜጋ ቅድሚያ ይልተሰጠ... የችግኝን ያህል ግድ ያ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Women's Safety as a Citizen ... What is the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>am_train_04845</td>\n",
       "      <td>ነገሮችን እንደአመጣጣቸው መቀበል የግድ ነው  በዛው እሳቤ ኑሮ ትቂት ጊዜ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>It is a necessity that we are coming as their ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>am_train_05701</td>\n",
       "      <td>ጀግና ኢትዮጵያዊ ሁሌም ከእናንተ ጋር ነን</td>\n",
       "      <td>positive</td>\n",
       "      <td>Hero Ethiopian are always with you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5129</th>\n",
       "      <td>am_train_05130</td>\n",
       "      <td>ለመልካም ዕድልሲል በአውሮፕላን ሞተር ውስጥ ሳንቲም የወረወረው ግለሰብ ተ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>The penny in a plane engine for fine lines!For...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>am_train_03250</td>\n",
       "      <td>ድርጊቱ መፈፀሙ ያልተገባ ቢሆንም ልዩ ልዩ ትንኮሳዎች ቢፈታተኑንም ቅሉ ዳ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Although the action is not unworthy, the struc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>am_train_01209</td>\n",
       "      <td>ትዕግስት ማለት ትዕግስት ማለት ብቻ አይደለም</td>\n",
       "      <td>negative</td>\n",
       "      <td>Trudy is not just Trudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>am_train_03869</td>\n",
       "      <td>በተግባር ድርጅትህ የሚያደርገው አንተም ዛሬ የደረከው ነው፣አዲስ ነገር አ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>In practice, your organization does not do it ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4188 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID                                              tweet  \\\n",
       "1073  am_train_01074  ለትራምፕ ስንሳቀቅ ህውሀት ከመቀሌ ደሞ አልወሀድም ብላለች  እና አሁን ህ...   \n",
       "5468  am_train_05469  ለኔ ታላቅ ክብር ነው! እነበመባሌ የተሰማኝን ትልቅ ክብር በቃላት ልገልፀ...   \n",
       "5306  am_train_05307  በስህተት ወደ አካውንታቸው የገባውን አንድ ሚልዮን ብር በቅንነት የመለሱት...   \n",
       "1321  am_train_01322  የሴቶች ደህንነት እንደ ዜጋ ቅድሚያ ይልተሰጠ... የችግኝን ያህል ግድ ያ...   \n",
       "4844  am_train_04845  ነገሮችን እንደአመጣጣቸው መቀበል የግድ ነው  በዛው እሳቤ ኑሮ ትቂት ጊዜ...   \n",
       "...              ...                                                ...   \n",
       "5700  am_train_05701                         ጀግና ኢትዮጵያዊ ሁሌም ከእናንተ ጋር ነን   \n",
       "5129  am_train_05130  ለመልካም ዕድልሲል በአውሮፕላን ሞተር ውስጥ ሳንቲም የወረወረው ግለሰብ ተ...   \n",
       "3249  am_train_03250  ድርጊቱ መፈፀሙ ያልተገባ ቢሆንም ልዩ ልዩ ትንኮሳዎች ቢፈታተኑንም ቅሉ ዳ...   \n",
       "1208  am_train_01209                       ትዕግስት ማለት ትዕግስት ማለት ብቻ አይደለም   \n",
       "3868  am_train_03869  በተግባር ድርጅትህ የሚያደርገው አንተም ዛሬ የደረከው ነው፣አዲስ ነገር አ...   \n",
       "\n",
       "         label                                     eng_translated  \n",
       "1073  negative  Habitheu says that when we laughed for the gru...  \n",
       "5468  positive  It is a great glory for me!I can't express it ...  \n",
       "5306  positive  In an error, the individual who first came to ...  \n",
       "1321  negative  Women's Safety as a Citizen ... What is the pr...  \n",
       "4844  positive  It is a necessity that we are coming as their ...  \n",
       "...        ...                                                ...  \n",
       "5700  positive                 Hero Ethiopian are always with you  \n",
       "5129  positive  The penny in a plane engine for fine lines!For...  \n",
       "3249   neutral  Although the action is not unworthy, the struc...  \n",
       "1208  negative                            Trudy is not just Trudy  \n",
       "3868   neutral  In practice, your organization does not do it ...  \n",
       "\n",
       "[4188 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "am_train = pd.read_csv('am_train_translated.csv')\n",
    "am_train, am_dev, am_test = np.split(\n",
    "    am_train.sample(frac=1, random_state=42), [int(.7*len(am_train)), int(.8*len(am_train))])\n",
    "\n",
    "am_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train = pd.concat([\n",
    "#     am_train[['tweet', 'label']].rename(columns={'tweet':'text', 'label':'labels'}),\n",
    "    en_train[['translated', 'labels']].rename(columns={'translated':'text'})\n",
    "])\n",
    "\n",
    "combined_test = pd.concat([\n",
    "    am_test[['tweet', 'label']].rename(columns={'tweet':'text', 'label':'labels'}),\n",
    "    am_dev[['tweet', 'label']].rename(columns={'tweet':'text', 'label':'labels'}),\n",
    "    en_test[['translated', 'labels']].rename(columns={'translated':'text'})\n",
    "])\n",
    "test_split_lengths = [('am_test', len(am_test)), ('am_dev', len(am_dev)), ('en_test', len(en_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "X_9irGwG8wXB",
    "outputId": "129bb934-2713-4d49-b138-f7a78a181d14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /Users/thomas/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /Users/thomas/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at /Users/thomas/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /Users/thomas/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13771"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label2id = {\"positive\":0, \"neutral\":1, 'negative':2}\n",
    "id2label = {0:\"positive\", 1:\"neutral\", 2:'negative'}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def encode_batch(row):\n",
    "    text = ' '.join(filter(lambda x:x[0]!='@', row.text.split() if type(row.text)==str else []))\n",
    "    out = tokenizer(text, max_length=100, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "    out['labels'] = torch.LongTensor([label2id[row.labels]])[0]\n",
    "    return out\n",
    "\n",
    "train = combined_train.apply(encode_batch, axis=1).reset_index()[0]\n",
    "# train = train[train != '']\n",
    "\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "22-fOD6t8wXE",
    "outputId": "edaffbd5-38fb-49d0-b83d-be8cb0579a05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4934"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = combined_test.apply(encode_batch, axis=1).reset_index()[0]\n",
    "\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9hD4HpNV8wXQ",
    "outputId": "96a4151d-fc28-486f-dbbe-413a6acf0595"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /Users/thomas/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /Users/thomas/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaAdapterModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Adding adapter 'sa'.\n",
      "Adding head 'sa' with config {'head_type': 'classification', 'num_labels': 3, 'layers': 2, 'activation_function': 'tanh', 'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}, 'use_pooler': False, 'bias': True}.\n"
     ]
    }
   ],
   "source": [
    "model = AutoAdapterModel.from_pretrained('xlm-roberta-base')\n",
    "model.add_adapter(\"sa\")\n",
    "model.train_adapter(\"sa\")\n",
    "model.add_classification_head(\"sa\", num_labels=3)\n",
    "model.set_active_adapters(\"sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5uob8B8n8wXS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, AdapterTrainer, EvalPrediction\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True\n",
    "#     # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#     remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "def compute_scores(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    i, output = 0, dict()\n",
    "    for name, split_length in test_split_lengths:\n",
    "        s = np.s_[i:i+split_length]\n",
    "        split_preds = preds[s]\n",
    "        split_labels = p.label_ids[s]\n",
    "        output[f'{name}_acc'] = (split_preds==split_labels).mean()\n",
    "        output[f'{name}_weighted_f1'] = f1_score(split_labels, split_preds, average='weighted')\n",
    "        i += split_length\n",
    "    return output\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    compute_metrics=compute_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQ2EpQQa8wXV",
    "outputId": "97b0481f-4ed7-4cbd-fba3-0f074514f128",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 13771\n",
      "  Num Epochs = 6\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2586\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2287' max='2586' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2287/2586 10:19:21 < 1:21:02, 0.06 it/s, Epoch 5.30/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.931100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.873700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.864500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.851900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.825200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.823600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.814800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.800200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.786900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./training_output/checkpoint-500\n",
      "Configuration saved in ./training_output/checkpoint-500/sa/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-500/sa/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-500/sa/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-500/sa/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-500/sa/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-500/sa/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-1000\n",
      "Configuration saved in ./training_output/checkpoint-1000/sa/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1000/sa/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-1000/sa/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1000/sa/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-1000/sa/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1000/sa/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-1500\n",
      "Configuration saved in ./training_output/checkpoint-1500/sa/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1500/sa/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-1500/sa/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1500/sa/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-1500/sa/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-1500/sa/pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output/checkpoint-2000\n",
      "Configuration saved in ./training_output/checkpoint-2000/sa/adapter_config.json\n",
      "Module weights saved in ./training_output/checkpoint-2000/sa/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output/checkpoint-2000/sa/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-2000/sa/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output/checkpoint-2000/sa/head_config.json\n",
      "Module weights saved in ./training_output/checkpoint-2000/sa/pytorch_model_head.bin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dh4bkdtt8wXW",
    "outputId": "1b5c34a2-7810-46db-d1f4-1d498ad2143f"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
