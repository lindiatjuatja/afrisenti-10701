{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-adapt\n",
      "  Downloading pytorch_adapt-0.0.81-py3-none-any.whl (157 kB)\n",
      "Collecting pytorch-metric-learning>=1.5.2\n",
      "  Downloading pytorch_metric_learning-1.6.3-py3-none-any.whl (111 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from pytorch-adapt) (1.12.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from pytorch-adapt) (1.21.5)\n",
      "Requirement already satisfied: torchvision in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from pytorch-adapt) (0.13.1)\n",
      "Collecting torchmetrics>=0.9.3\n",
      "  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from pytorch-metric-learning>=1.5.2->pytorch-adapt) (1.0.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from pytorch-metric-learning>=1.5.2->pytorch-adapt) (4.64.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from torch->pytorch-adapt) (4.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from torchmetrics>=0.9.3->pytorch-adapt) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from packaging->torchmetrics>=0.9.3->pytorch-adapt) (3.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from scikit-learn->pytorch-metric-learning>=1.5.2->pytorch-adapt) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from scikit-learn->pytorch-metric-learning>=1.5.2->pytorch-adapt) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from scikit-learn->pytorch-metric-learning>=1.5.2->pytorch-adapt) (1.7.3)\n",
      "Requirement already satisfied: requests in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from torchvision->pytorch-adapt) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from torchvision->pytorch-adapt) (9.0.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->torchvision->pytorch-adapt) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->torchvision->pytorch-adapt) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->torchvision->pytorch-adapt) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from requests->torchvision->pytorch-adapt) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\thomas\\anaconda3\\lib\\site-packages (from tqdm->pytorch-metric-learning>=1.5.2->pytorch-adapt) (0.4.4)\n",
      "Installing collected packages: torchmetrics, pytorch-metric-learning, pytorch-adapt\n",
      "Successfully installed pytorch-adapt-0.0.81 pytorch-metric-learning-1.6.3 torchmetrics-0.10.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G7_P79Ut8wW2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoAdapterModel, AdapterConfig, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_train = pd.read_csv(\"data/en_all.csv\")\n",
    "en_train, en_test = np.split(en_train.sample(frac=1, random_state=42), [int(.8*len(en_train))])\n",
    "\n",
    "am_train = pd.read_csv('data/am/am_train_translated.csv').rename(columns={'tweet':'text', 'label':'labels'})\n",
    "am_train, am_dev = np.split(\n",
    "    am_train.sample(frac=1, random_state=42), [int(.7*len(am_train))])\n",
    "\n",
    "# combined_train = pd.concat([\n",
    "#     en_train[['ID', 'text', 'labels']],\n",
    "#     am_train[['ID', 'tweet', 'label']].rename(columns={'tweet':'text', 'label':'labels'}),\n",
    "# ])\n",
    "\n",
    "# combined_test = pd.concat([\n",
    "#     am_dev[['ID', 'tweet', 'label']].rename(columns={'tweet':'text', 'label':'labels'}),\n",
    "#     en_test[['ID', 'text', 'labels']]\n",
    "# ])\n",
    "# test_split_lengths = [('am_test', len(am_test)), ('am_dev', len(am_dev)), ('en_test', len(en_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 0, 'am': 1}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_domain = lambda t: t.split('_')[0] if type(t)==str else 'en'\n",
    "domains = combined_train['ID'].apply(get_domain).unique()\n",
    "domains\n",
    "domain2id = {d: i for i, d in enumerate(domains)}\n",
    "id2domain = {i: d for d, i in domain2id.items()}\n",
    "domain2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "X_9irGwG8wXB",
    "outputId": "129bb934-2713-4d49-b138-f7a78a181d14"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.dtype' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [91]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     out \u001b[38;5;241m=\u001b[39m encode_batch(row)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mvstack([out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]]),\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mint(out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mint(domain2id[get_domain(row\u001b[38;5;241m.\u001b[39mID)])\n\u001b[0;32m     21\u001b[0m     }\n\u001b[1;32m---> 23\u001b[0m en_train \u001b[38;5;241m=\u001b[39m \u001b[43men_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapt_encode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m en_test \u001b[38;5;241m=\u001b[39m en_test\u001b[38;5;241m.\u001b[39mapply(adapt_encode, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m am_train \u001b[38;5;241m=\u001b[39m am_train\u001b[38;5;241m.\u001b[39mapply(adapt_encode, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:8839\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8828\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   8830\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   8831\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8832\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8837\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   8838\u001b[0m )\n\u001b[1;32m-> 8839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:727\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:851\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:867\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    869\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    870\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    871\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[1;32mIn [91]\u001b[0m, in \u001b[0;36madapt_encode\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     15\u001b[0m real_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m     16\u001b[0m out \u001b[38;5;241m=\u001b[39m encode_batch(row)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mvstack([out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]]),\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mint(domain2id[get_domain(row\u001b[38;5;241m.\u001b[39mID)])\n\u001b[0;32m     21\u001b[0m }\n",
      "\u001b[1;31mTypeError\u001b[0m: 'torch.dtype' object is not callable"
     ]
    }
   ],
   "source": [
    "\n",
    "label2id = {\"positive\":0, \"neutral\":1, 'negative':2}\n",
    "id2label = {0:\"positive\", 1:\"neutral\", 2:'negative'}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "do_domain = True\n",
    "\n",
    "def encode_batch(row):\n",
    "    text = ' '.join(filter(lambda x:x[0]!='@', row.text.split() if type(row.text)==str else []))\n",
    "    out = tokenizer(text, max_length=100, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "    out['labels'] = torch.LongTensor([label2id[row.labels]])[0]\n",
    "    return out\n",
    "\n",
    "def adapt_encode(row):\n",
    "    real_out = dict()\n",
    "    out = encode_batch(row)\n",
    "    return {\n",
    "        'imgs': torch.vstack([out['input_ids'], out['attention_mask']]),\n",
    "        'labels': torch.int(out['labels']),\n",
    "        'domain': torch.int(domain2id[get_domain(row.ID)])\n",
    "    }\n",
    "\n",
    "en_train = en_train.apply(adapt_encode, axis=1)\n",
    "en_test = en_test.apply(adapt_encode, axis=1)\n",
    "am_train = am_train.apply(adapt_encode, axis=1)\n",
    "am_dev = am_dev.apply(adapt_encode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSourceAndTargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, s, t):\n",
    "        self.s = s\n",
    "        self.t = t\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.t)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tgt = self.t.iloc[idx]\n",
    "        src = self.s.iloc[self.get_random_src_idx()]\n",
    "        return {\n",
    "            'src_imgs': torch.Tensor(src['imgs']),\n",
    "            'src_labels': torch.LongTensor(src['labels']),\n",
    "            'src_domain': torch.LongTensor(src['domain']),\n",
    "            'target_imgs': torch.Tensor(tgt['imgs']),\n",
    "            'target_labels': torch.LongTensor(tgt['labels']),\n",
    "            'target_domain': torch.LongTensor(tgt['domain']),\n",
    "        }\n",
    "    \n",
    "    def get_random_src_idx(self):\n",
    "        return np.random.choice(len(self.s))\n",
    "    \n",
    "class SimpleTargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, t):\n",
    "        self.t = t\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.t)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tgt = self.t.iloc[idx]\n",
    "        return {\n",
    "            'target_imgs': torch.FloatTensor(tgt['imgs']),\n",
    "            'target_labels': torch.LongTensor(tgt['labels']),\n",
    "            'target_domain': torch.LongTensor(tgt['domain']),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=__int64, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m SimpleSourceAndTargetDataset(en_train, am_train)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n",
      "Input \u001b[1;32mIn [83]\u001b[0m, in \u001b[0;36mSimpleSourceAndTargetDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     10\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[0;32m     11\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_random_src_idx()]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_imgs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimgs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_labels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mLongTensor(src[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_domain\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mLongTensor(src[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_imgs\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mFloatTensor(tgt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_labels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mLongTensor(tgt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_domain\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mLongTensor(tgt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     19\u001b[0m }\n",
      "\u001b[1;31mTypeError\u001b[0m: expected TensorOptions(dtype=float, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)) (got TensorOptions(dtype=__int64, device=cpu, layout=Strided, requires_grad=false (default), pinned_memory=false (default), memory_format=(nullopt)))"
     ]
    }
   ],
   "source": [
    "train_data = SimpleSourceAndTargetDataset(en_train, am_train)\n",
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        model = AutoAdapterModel.from_pretrained('xlm-roberta-base')\n",
    "        model.add_adapter(\"sa\")\n",
    "        model.train_adapter(\"sa\")\n",
    "        model.set_active_adapters(\"sa\")\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.model(x[:, 0], x[:, 1]).pooler_output\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaAdapterModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "from pytorch_adapt.containers import LRSchedulers\n",
    "from pytorch_adapt.containers import Optimizers\n",
    "from pytorch_adapt.adapters import DANN\n",
    "from pytorch_adapt.containers import Models\n",
    "\n",
    "G = Generator()\n",
    "C = torch.nn.Linear(768, 10)\n",
    "D = torch.nn.Sequential(torch.nn.Linear(768, 1), torch.nn.Flatten(start_dim=0))\n",
    "models = Models({\"G\": G, \"C\": C, \"D\": D})\n",
    "\n",
    "\n",
    "optimizers = Optimizers((torch.optim.Adam, {\"lr\": 1}))\n",
    "lr_schedulers = LRSchedulers(\n",
    "    {\n",
    "        \"G\": (torch.optim.lr_scheduler.ExponentialLR, {\"gamma\": 0.99}),\n",
    "        \"C\": (torch.optim.lr_scheduler.StepLR, {\"step_size\": 2}),\n",
    "    },\n",
    "    scheduler_types={\"per_step\": [\"G\"], \"per_epoch\": [\"C\"]},\n",
    ")\n",
    "adapter = DANN(models=models, optimizers=optimizers, lr_schedulers=lr_schedulers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in DANNHook: __call__\nin ChainHook: __call__\nin OptimizerHook: __call__\nin ChainHook: __call__\nin AssertHook: __call__\nin OnlyNewOutputsHook: __call__\nin ChainHook: __call__\nin DomainLossHook: __call__\nDomainLossHook: Expecting 'src_domain' and 'target_domain' in inputs\nDomainLossHook: Computing loss for src domain\n'int' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m loss\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\adapters\\base_adapter.py:106\u001b[0m, in \u001b[0;36mBaseAdapter.training_step\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03mCalls the wrapped hook at each iteration during training.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m        - the inner level contains the loss components.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    103\u001b[0m combined \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39massert_dicts_are_disjoint(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmisc, with_opt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers), batch, kwargs\n\u001b[0;32m    105\u001b[0m )\n\u001b[1;32m--> 106\u001b[0m _, losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\base.py:52\u001b[0m, in \u001b[0;36mBaseHook.__call__\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mmap_keys(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_map)\n\u001b[1;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool_)):\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\base.py:194\u001b[0m, in \u001b[0;36mBaseWrapperHook.call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\base.py:52\u001b[0m, in \u001b[0;36mBaseHook.__call__\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mmap_keys(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_map)\n\u001b[1;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool_)):\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\utils.py:109\u001b[0m, in \u001b[0;36mChainHook.call\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m    107\u001b[0m all_losses \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_losses, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprev_losses}\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditions[i](all_inputs, all_losses):\n\u001b[1;32m--> 109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malts[i](all_inputs, all_losses)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\base.py:52\u001b[0m, in \u001b[0;36mBaseHook.__call__\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mmap_keys(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_map)\n\u001b[1;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool_)):\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\optimizer.py:51\u001b[0m, in \u001b[0;36mOptimizerHook.call\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, losses):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     outputs, losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     combined \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39massert_dicts_are_disjoint(inputs, outputs)\n\u001b[0;32m     53\u001b[0m     new_outputs, losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreducer(combined, losses)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\base.py:52\u001b[0m, in \u001b[0;36mBaseHook.__call__\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mmap_keys(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_map)\n\u001b[1;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool_)):\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\utils.py:109\u001b[0m, in \u001b[0;36mChainHook.call\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m    107\u001b[0m all_losses \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_losses, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprev_losses}\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditions[i](all_inputs, all_losses):\n\u001b[1;32m--> 109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malts[i](all_inputs, all_losses)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\base.py:52\u001b[0m, in \u001b[0;36mBaseHook.__call__\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mmap_keys(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_map)\n\u001b[1;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool_)):\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\utils.py:318\u001b[0m, in \u001b[0;36mAssertHook.call\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, losses):\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m     outputs, losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39massert_fn(outputs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs, losses\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\base.py:52\u001b[0m, in \u001b[0;36mBaseHook.__call__\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mmap_keys(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_map)\n\u001b[1;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool_)):\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\utils.py:220\u001b[0m, in \u001b[0;36mOnlyNewOutputsHook.call\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, losses):\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m     outputs, losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m {k: outputs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m (outputs\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m-\u001b[39m inputs\u001b[38;5;241m.\u001b[39mkeys())}\n\u001b[0;32m    222\u001b[0m     c_f\u001b[38;5;241m.\u001b[39massert_dicts_are_disjoint(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\base.py:52\u001b[0m, in \u001b[0;36mBaseHook.__call__\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mmap_keys(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_map)\n\u001b[1;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool_)):\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\utils.py:109\u001b[0m, in \u001b[0;36mChainHook.call\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m    107\u001b[0m all_losses \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_losses, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprev_losses}\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditions[i](all_inputs, all_losses):\n\u001b[1;32m--> 109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malts[i](all_inputs, all_losses)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\base.py:52\u001b[0m, in \u001b[0;36mBaseHook.__call__\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mmap_keys(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_map)\n\u001b[1;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool_)):\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_adapt\\hooks\\domain.py:110\u001b[0m, in \u001b[0;36mDomainLossHook.call\u001b[1;34m(self, inputs, losses)\u001b[0m\n\u001b[0;32m    108\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m    111\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_loss_fn(dlogits, labels)\n\u001b[0;32m    112\u001b[0m losses[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdomain_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_domain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loss\n",
      "\u001b[1;31mAttributeError\u001b[0m: in DANNHook: __call__\nin ChainHook: __call__\nin OptimizerHook: __call__\nin ChainHook: __call__\nin AssertHook: __call__\nin OnlyNewOutputsHook: __call__\nin ChainHook: __call__\nin DomainLossHook: __call__\nDomainLossHook: Expecting 'src_domain' and 'target_domain' in inputs\nDomainLossHook: Computing loss for src domain\n'int' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "loss = adapter.training_step(train_data[0])\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data[0]['src_imgs'][0], data[0]['src_imgs'][1]).pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [input_ids, attention_mask, labels]\n",
       "1        [input_ids, attention_mask, labels]\n",
       "2        [input_ids, attention_mask, labels]\n",
       "3        [input_ids, attention_mask, labels]\n",
       "4        [input_ids, attention_mask, labels]\n",
       "                        ...                 \n",
       "10171    [input_ids, attention_mask, labels]\n",
       "10172    [input_ids, attention_mask, labels]\n",
       "10173    [input_ids, attention_mask, labels]\n",
       "10174    [input_ids, attention_mask, labels]\n",
       "10175    [input_ids, attention_mask, labels]\n",
       "Name: 0, Length: 10176, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.x_data[idx].to(device)\n",
    "        y = self.y_data[idx].to(device) \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9hD4HpNV8wXQ",
    "outputId": "96a4151d-fc28-486f-dbbe-413a6acf0595"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaAdapterModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoAdapterModel.from_pretrained('xlm-roberta-base')\n",
    "model.add_adapter(\"sa\")\n",
    "model.train_adapter(\"sa\")\n",
    "model.add_classification_head(\"sa\", num_labels=3)\n",
    "model.set_active_adapters(\"sa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hidden_sizes=(4096, 4096), dropout=0.5, lr=1e-3, warmup_steps=100, num_epochs=15, updates_per_epoch=4, title='', leave=False):\n",
    "    \n",
    "    show = False\n",
    "    losses = []\n",
    "        \n",
    "    net = ViTAdapterModel.from_pretrained(\n",
    "                model_name_or_path\n",
    "    )\n",
    "    net.add_adapter(\"microstructures\")\n",
    "    net.register_custom_head(\"my_custom_head\", CustomHead)\n",
    "    net.add_custom_head(head_type=\"my_custom_head\", head_name=\"custom_head\", **{'hidden_sizes': hidden_sizes, 'dropout':dropout})\n",
    "    net.train_adapter(\"microstructures\")\n",
    "    net = net.to(device)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(fold['train'], batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(fold['valid'], batch_size=batch_size, shuffle=False)\n",
    "    valid2_dataloader = torch.utils.data.DataLoader(fold['valid2'], batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(fold['test'], batch_size=batch_size, shuffle=False)\n",
    "    test2_dataloader = torch.utils.data.DataLoader(fold['test2'], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_batches = len(train_dataloader)\n",
    "    num_valid_batches = len(valid_dataloader)\n",
    "    num_test_batches = len(test_dataloader)\n",
    "\n",
    "    num_valid2_batches = len(valid2_dataloader)\n",
    "    num_test2_batches = len(test2_dataloader)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, num_batches * num_epochs)\n",
    "\n",
    "    train_losses, valid_losses, test_losses = [], [], []\n",
    "    valid2_losses, test2_losses = [], []\n",
    "\n",
    "    update_idxs = set([i * (num_batches // updates_per_epoch) \n",
    "        for i in range(1, updates_per_epoch)] + [num_batches])\n",
    "\n",
    "    best_losses = []\n",
    "    best_valid2 = 1e9\n",
    "\n",
    "    for epoch in range(1, 1+num_epochs):\n",
    "        total_loss = 0.0 \n",
    "        net.train()\n",
    "\n",
    "        pbar = tqdm(train_dataloader, desc=f\"Fold {fold_num}, Epoch {epoch}\", leave=leave)\n",
    "        for idx, (X, y) in enumerate(pbar, start=1):\n",
    "            optimizer.zero_grad()\n",
    "            out = net(X)\n",
    "            loss_val = criterion(out, y)\n",
    "            total_loss += loss_val.item()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if idx in update_idxs:\n",
    "                avg_valid_loss = 0.0\n",
    "                avg_test_loss = 0.0\n",
    "                avg_valid2_loss = 0.0\n",
    "                avg_test2_loss = 0.0\n",
    "                net.eval()\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    for X, y in valid_dataloader:\n",
    "                        out = net(X)\n",
    "                        loss_val = criterion(out, y)\n",
    "                        avg_valid_loss += loss_val.item() / num_valid_batches\n",
    "                    valid_losses.append(avg_valid_loss)\n",
    "\n",
    "                    for X, y in test_dataloader:\n",
    "                        out = net(X)\n",
    "                        loss_val = criterion(out, y)\n",
    "                        avg_test_loss += loss_val.item() / num_test_batches\n",
    "                    test_losses.append(avg_test_loss)\n",
    "\n",
    "                    for X, y in valid2_dataloader:\n",
    "                        out = net(X)\n",
    "                        loss_val = criterion(out, y)\n",
    "                        avg_valid2_loss += loss_val.item() / num_valid2_batches\n",
    "                    valid2_losses.append(avg_valid2_loss)\n",
    "\n",
    "                    for X, y in test2_dataloader:\n",
    "                        out = net(X)\n",
    "                        loss_val = criterion(out, y)\n",
    "                        avg_test2_loss += loss_val.item() / num_test_batches\n",
    "                    test2_losses.append(avg_test2_loss)\n",
    "\n",
    "                    if best_valid2 > avg_valid_loss + avg_valid2_loss:\n",
    "                        best_valid2 = avg_valid_loss + avg_valid2_loss\n",
    "                        best_losses = [total_loss / idx, avg_valid_loss, avg_valid2_loss,\n",
    "                                      avg_test_loss, avg_test2_loss]\n",
    "\n",
    "\n",
    "                pbar.set_description(f\"Fold {fold_num}, Epoch {epoch} | tr {total_loss / idx:.2f}\" + \\\n",
    "                                    f\" | v1 {avg_valid_loss:.2f}  | v2 {avg_valid2_loss:.2f}\" + \\\n",
    "                                    f\"| t1 {avg_test_loss:.2f} | t2 {avg_test2_loss:.2f}\")\n",
    "                train_losses.append(total_loss / idx)\n",
    "        losses.append(best_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5uob8B8n8wXS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, AdapterTrainer, EvalPrediction\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=200,\n",
    "    output_dir=\"./training_output\",\n",
    "    overwrite_output_dir=True\n",
    "#     # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#     remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "def compute_scores(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    i, output = 0, dict()\n",
    "    for name, split_length in test_split_lengths:\n",
    "        s = np.s_[i:i+split_length]\n",
    "        split_preds = preds[s]\n",
    "        split_labels = p.label_ids[s]\n",
    "        output[f'{name}_acc'] = (split_preds==split_labels).mean()\n",
    "        output[f'{name}_weighted_f1'] = f1_score(split_preds, split_labels, average='weighted')\n",
    "        output[f'{name}_balanced_accurancy'] = balanced_accuracy_score(split_preds, split_labels)\n",
    "        i += split_length\n",
    "    return output\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    compute_metrics=compute_scores,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mQ2EpQQa8wXV",
    "outputId": "97b0481f-4ed7-4cbd-fba3-0f074514f128",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13771\n",
      "  Num Epochs = 7\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3017\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3017' max='3017' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3017/3017 09:51, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.891700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.844100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.839100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.830700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.806800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.800500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.795600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.773200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.774800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.777900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./training_output\\checkpoint-500\n",
      "Configuration saved in ./training_output\\checkpoint-500\\sa\\adapter_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-500\\sa\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output\\checkpoint-500\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-500\\sa\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output\\checkpoint-500\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-500\\sa\\pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output\\checkpoint-1000\n",
      "Configuration saved in ./training_output\\checkpoint-1000\\sa\\adapter_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-1000\\sa\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output\\checkpoint-1000\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-1000\\sa\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output\\checkpoint-1000\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-1000\\sa\\pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output\\checkpoint-1500\n",
      "Configuration saved in ./training_output\\checkpoint-1500\\sa\\adapter_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-1500\\sa\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output\\checkpoint-1500\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-1500\\sa\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output\\checkpoint-1500\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-1500\\sa\\pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output\\checkpoint-2000\n",
      "Configuration saved in ./training_output\\checkpoint-2000\\sa\\adapter_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-2000\\sa\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output\\checkpoint-2000\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-2000\\sa\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output\\checkpoint-2000\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-2000\\sa\\pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output\\checkpoint-2500\n",
      "Configuration saved in ./training_output\\checkpoint-2500\\sa\\adapter_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-2500\\sa\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output\\checkpoint-2500\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-2500\\sa\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output\\checkpoint-2500\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-2500\\sa\\pytorch_model_head.bin\n",
      "Saving model checkpoint to ./training_output\\checkpoint-3000\n",
      "Configuration saved in ./training_output\\checkpoint-3000\\sa\\adapter_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-3000\\sa\\pytorch_adapter.bin\n",
      "Configuration saved in ./training_output\\checkpoint-3000\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-3000\\sa\\pytorch_model_head.bin\n",
      "Configuration saved in ./training_output\\checkpoint-3000\\sa\\head_config.json\n",
      "Module weights saved in ./training_output\\checkpoint-3000\\sa\\pytorch_model_head.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3017, training_loss=0.8335713129546178, metrics={'train_runtime': 593.918, 'train_samples_per_second': 162.307, 'train_steps_per_second': 5.08, 'total_flos': 5039674817725800.0, 'train_loss': 0.8335713129546178, 'epoch': 7.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Dh4bkdtt8wXW",
    "outputId": "1b5c34a2-7810-46db-d1f4-1d498ad2143f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4934\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='155' max='155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [155/155 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.809025228023529,\n",
       " 'eval_am_test_acc': 0.6006683375104428,\n",
       " 'eval_am_test_weighted_f1': 0.6024775787444283,\n",
       " 'eval_am_test_balanced_accurancy': 0.5770568131477881,\n",
       " 'eval_am_dev_acc': 0.5976627712854758,\n",
       " 'eval_am_dev_weighted_f1': 0.5989814862583934,\n",
       " 'eval_am_dev_balanced_accurancy': 0.578623886704277,\n",
       " 'eval_en_test_acc': 0.6593371574251116,\n",
       " 'eval_en_test_weighted_f1': 0.670796609889722,\n",
       " 'eval_en_test_balanced_accurancy': 0.6550661143187694,\n",
       " 'eval_runtime': 14.8375,\n",
       " 'eval_samples_per_second': 332.535,\n",
       " 'eval_steps_per_second': 10.446,\n",
       " 'epoch': 7.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
