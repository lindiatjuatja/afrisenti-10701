{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq pytorch-adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoAdapterModel, AdapterConfig, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_adapt.containers import Optimizers, LRSchedulers\n",
    "from pytorch_adapt.hooks import DANNHook\n",
    "from pytorch_adapt.containers import Models\n",
    "from pytorch_adapt.models import Discriminator, Classifier\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "en_train = pd.read_csv(\"data/en_all.csv\")\n",
    "\n",
    "am_train = pd.read_csv('data/am/am_train_translated.csv').rename(columns={'tweet':'text', 'label':'labels'})\n",
    "am_train, am_dev, am_test = np.split(am_train.sample(frac=1, random_state=42), [int(.7*len(am_train)), int(.8*len(am_train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 0, 'am': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_domain = lambda t: t.split('_')[0] if type(t)==str else 'en'\n",
    "\n",
    "domains = []\n",
    "for d in en_train.ID:\n",
    "    d = get_domain(d)\n",
    "    if d not in domains:\n",
    "        domains.append(d)\n",
    "for d in am_train.ID:\n",
    "    d = get_domain(d)\n",
    "    if d not in domains:\n",
    "        domains.append(d)\n",
    "domain2id = {d: i for i, d in enumerate(domains)}\n",
    "id2domain = {i: d for d, i in domain2id.items()}\n",
    "domain2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X_9irGwG8wXB",
    "outputId": "129bb934-2713-4d49-b138-f7a78a181d14"
   },
   "outputs": [],
   "source": [
    "\n",
    "label2id = {\"positive\":0, \"neutral\":1, 'negative':2}\n",
    "id2label = {0:\"positive\", 1:\"neutral\", 2:'negative'}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "do_domain = True\n",
    "\n",
    "def encode_batch(row):\n",
    "    text = ' '.join(filter(lambda x:x[0]!='@', row.text.split() if type(row.text)==str else []))\n",
    "    out = tokenizer(text, max_length=100, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "    out['labels'] = torch.LongTensor([label2id[row.labels]])[0]\n",
    "    return out\n",
    "\n",
    "def adapt_encode(row):\n",
    "    real_out = dict()\n",
    "    out = encode_batch(row)\n",
    "    return {\n",
    "        'imgs': torch.vstack([out['input_ids'], out['attention_mask']]),\n",
    "        'labels': torch.LongTensor([out['labels']])[0],\n",
    "        'domain': torch.LongTensor([domain2id[get_domain(row.ID)]])[0]\n",
    "    }\n",
    "\n",
    "en_train = en_train.apply(adapt_encode, axis=1)\n",
    "                          \n",
    "am_train = am_train.apply(adapt_encode, axis=1)\n",
    "am_dev = am_dev.apply(adapt_encode, axis=1)\n",
    "am_test = am_test.apply(adapt_encode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSourceAndTargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, s, t):\n",
    "        self.s = s\n",
    "        self.t = t\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.t)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tgt = self.t.iloc[idx]\n",
    "        src = self.s.iloc[self.get_random_src_idx()]\n",
    "        return {\n",
    "            'src_imgs': src['imgs'].to(device),\n",
    "            'src_labels': src['labels'].to(device),\n",
    "            'src_domain': src['domain'].to(device),\n",
    "            'target_imgs': tgt['imgs'].to(device),\n",
    "#             'target_labels': tgt['labels'].to(device),\n",
    "            'target_domain': tgt['domain'].to(device),\n",
    "        }\n",
    "    \n",
    "    def get_random_src_idx(self):\n",
    "        return np.random.choice(len(self.s))\n",
    "    \n",
    "class SimpleTargetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, t):\n",
    "        self.t = t\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.t)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tgt = self.t.iloc[idx]\n",
    "        return {\n",
    "            'target_imgs': tgt['imgs'].to(device),\n",
    "            'target_labels': tgt['labels'].to(device),\n",
    "            'target_domain': tgt['domain'].to(device),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = SimpleSourceAndTargetDataset(en_train, am_train)\n",
    "valid_data = SimpleTargetDataset(am_dev)\n",
    "test_data = SimpleTargetDataset(am_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        model = AutoAdapterModel.from_pretrained('xlm-roberta-base')\n",
    "        model.add_adapter(\"sa\")\n",
    "        model.train_adapter(\"sa\")\n",
    "        model.set_active_adapters(\"sa\")\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        a = self.model(x[:, 0], x[:, 1]).pooler_output\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    C_h=256,\n",
    "    D_h=256,\n",
    "    lr=1e-4,\n",
    "    classifier_lr=1e-3,\n",
    "    num_epochs=20, \n",
    "    batch_size=16,\n",
    "    updates_per_epoch=4, \n",
    "    leave=False):\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_batches = len(train_dataloader)\n",
    "    num_valid_batches = len(valid_dataloader)\n",
    "    num_test_batches = len(test_dataloader)\n",
    "    \n",
    "    G = Generator().to(device)\n",
    "    C = Classifier(3, in_size=768, h=C_h).to(device)\n",
    "    D = Discriminator(in_size=768, h=D_h).to(device)\n",
    "    \n",
    "    models = Models({\"G\": G, \"C\": C, \"D\": D})\n",
    "    optimizers = Optimizers(\n",
    "        {\"G\": (torch.optim.AdamW, {\"lr\": lr}),\n",
    "         \"C\": (torch.optim.AdamW, {\"lr\": classifier_lr}),\n",
    "         \"D\": (torch.optim.AdamW, {\"lr\": classifier_lr})}\n",
    "    )\n",
    "    optimizers.create_with(models)\n",
    "    optimizers = list(optimizers.values())\n",
    "    \n",
    "#     lr_schedulers = LRSchedulers(\n",
    "#         {\n",
    "#             \"G\": (torch.optim.lr_scheduler.ExponentialLR, {\"gamma\": 0.99}),\n",
    "#             \"C\": (torch.optim.lr_scheduler.ExponentialLR, {\"gamma\": 0.99}),\n",
    "#         },\n",
    "#         scheduler_types={\"per_step\": [\"G\", \"C\"]},\n",
    "#     )\n",
    "    \n",
    "    hook = DANNHook(optimizers)\n",
    "\n",
    "    update_idxs = set([i * (num_batches // updates_per_epoch) \n",
    "        for i in range(1, updates_per_epoch)] + [num_batches])\n",
    "\n",
    "    best_losses = dict()\n",
    "    best_valid = -1\n",
    "    best_test = -1\n",
    "\n",
    "    for epoch in range(1, 1+num_epochs):\n",
    "        total_loss = 0.0 \n",
    "\n",
    "        pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch}\", leave=leave)\n",
    "        for idx, data in enumerate(pbar, start=1):\n",
    "            models.train()\n",
    "            _, loss = hook({**models, **data})\n",
    "            \n",
    "            total_loss += loss['total_loss']['total']\n",
    "\n",
    "            if idx in update_idxs:\n",
    "                \n",
    "                models.eval()\n",
    "                with torch.no_grad():\n",
    "                    logits = []\n",
    "                    ans = []\n",
    "                    for data in valid_dataloader:\n",
    "                        logits.append(C(G(data[\"target_imgs\"])))\n",
    "                        ans.append(data[\"target_labels\"])\n",
    "                    valid_preds = torch.cat(logits, dim=0).argmax(-1).cpu().numpy()\n",
    "                    valid_ans = torch.cat(ans, dim=0).cpu().numpy()\n",
    "                    valid_bal_acc = balanced_accuracy_score(valid_preds, valid_ans)\n",
    "                    \n",
    "                    if valid_bal_acc > best_valid:\n",
    "                        best_valid = valid_bal_acc\n",
    "                        best_losses = dict()\n",
    "                        \n",
    "                        best_losses['dev_balanced_accuracy'] = valid_bal_acc\n",
    "                        best_losses['dev_f1'] = f1_score(valid_preds, valid_ans, average='weighted')\n",
    "                        \n",
    "                        logits = []\n",
    "                        ans = []\n",
    "                        for data in test_dataloader:\n",
    "                            logits.append(C(G(data[\"target_imgs\"])))\n",
    "                            ans.append(data[\"target_labels\"])\n",
    "                        test_preds = torch.cat(logits, dim=0).argmax(-1).cpu().numpy()\n",
    "                        test_ans = torch.cat(ans, dim=0).cpu().numpy()\n",
    "                        \n",
    "                        best_losses['test_balanced_accuracy'] = balanced_accuracy_score(test_preds, test_ans)\n",
    "                        best_losses['test_f1'] = f1_score(test_preds, test_ans, average='weighted')\n",
    "\n",
    "\n",
    "                pbar.set_description(f\" Epoch {epoch} | tr {total_loss / idx:.3f}\" + \\\n",
    "                                    f\" | valid bal_acc {valid_bal_acc:.2f} | test bal_acc {best_losses['test_balanced_accuracy']:.2f}\")\n",
    "#                 train_losses.append(total_loss / idx)\n",
    "    return best_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaAdapterModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " Epoch 1 | tr 0.795 | valid bal_acc 0.51 | test bal_acc 0.53: 100%|██████████████████| 262/262 [01:05<00:00,  4.00it/s]\n",
      " Epoch 2 | tr 0.798 | valid bal_acc 0.51 | test bal_acc 0.53: 100%|██████████████████| 262/262 [01:02<00:00,  4.22it/s]\n",
      " Epoch 3 | tr 0.806 | valid bal_acc 0.51 | test bal_acc 0.53: 100%|██████████████████| 262/262 [01:02<00:00,  4.21it/s]\n",
      " Epoch 4 | tr 0.791 | valid bal_acc 0.51 | test bal_acc 0.53: 100%|██████████████████| 262/262 [01:02<00:00,  4.22it/s]\n",
      " Epoch 5 | tr 0.806 | valid bal_acc 0.51 | test bal_acc 0.53:  80%|██████████████▍   | 210/262 [00:49<00:12,  4.25it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_experiment(C_h=256,\n",
    "    D_h=256,\n",
    "    lr=1e-5,\n",
    "    classifier_lr=1e-4,\n",
    "    num_epochs=20, leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
