{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisenti-10701\\baselines\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get the data\n",
    "\n",
    "import os\n",
    "colab = False\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    proj_folder = '/content/drive/MyDrive'\n",
    "else:\n",
    "    proj_folder = os.getcwd()\n",
    "\n",
    "%cd {proj_folder}\n",
    "\n",
    "\n",
    "PROJECT_DIR = f'{proj_folder}/afrisent-semeval-2023'\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  %run Make_Datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments on codes: 'am', 'dz', 'ha', 'ig', 'ma', 'pcm', 'pt', 'sw', 'yo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Language Code:  am\n",
      "['neutral', 'positive', 'negative']\n",
      "balanced acc: 0.404, f1: 0.469, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.001}\n",
      "balanced acc: 0.407, f1: 0.471, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n",
      "Best Weighted F1: 0.47129786740805657\n",
      "Best Balanced Accuracy: 0.4074563515343887\n",
      "Params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 1/5 [00:01<00:07,  1.84s/ba]\n",
      "Running tokenizer on train dataset:  40%|####      | 2/5 [00:03<00:05,  1.82s/ba]\n",
      "Running tokenizer on train dataset:  60%|######    | 3/5 [00:05<00:03,  1.80s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.81s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.90s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:01<?, ?ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.80s/ba]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:02<00:02,  2.16s/ba]\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-LSTM.py --lang_code am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Language Code:  am\n",
      "['neutral', 'positive', 'negative']\n",
      "balanced acc: 0.404, f1: 0.469, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.001}\n",
      "balanced acc: 0.407, f1: 0.471, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n",
      "Best Weighted F1: 0.47129786740805657\n",
      "Best Balanced Accuracy: 0.4074563515343887\n",
      "Params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 1/5 [00:02<00:08,  2.00s/ba]\n",
      "Running tokenizer on train dataset:  40%|####      | 2/5 [00:03<00:05,  1.98s/ba]\n",
      "Running tokenizer on train dataset:  60%|######    | 3/5 [00:05<00:03,  1.95s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.89s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:08<00:02,  2.01s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:01<?, ?ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.81s/ba]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:02<00:02,  2.18s/ba]\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-LSTM.py --lang_code dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Language Code:  am\n",
      "['neutral', 'positive', 'negative']\n",
      "balanced acc: 0.404, f1: 0.469, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.001}\n",
      "balanced acc: 0.407, f1: 0.471, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n",
      "Best Weighted F1: 0.47129786740805657\n",
      "Best Balanced Accuracy: 0.4074563515343887\n",
      "Params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 1/5 [00:01<00:07,  1.99s/ba]\n",
      "Running tokenizer on train dataset:  40%|####      | 2/5 [00:03<00:05,  1.98s/ba]\n",
      "Running tokenizer on train dataset:  60%|######    | 3/5 [00:05<00:03,  1.92s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.89s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:08<00:02,  2.00s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:01<?, ?ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.90s/ba]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:02<00:02,  2.26s/ba]\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-LSTM.py --lang_code ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Language Code:  am\n",
      "['neutral', 'positive', 'negative']\n",
      "balanced acc: 0.404, f1: 0.469, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.001}\n",
      "balanced acc: 0.407, f1: 0.471, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n",
      "Best Weighted F1: 0.47129786740805657\n",
      "Best Balanced Accuracy: 0.4074563515343887\n",
      "Params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 1/5 [00:01<00:07,  1.89s/ba]\n",
      "Running tokenizer on train dataset:  40%|####      | 2/5 [00:03<00:05,  1.87s/ba]\n",
      "Running tokenizer on train dataset:  60%|######    | 3/5 [00:05<00:03,  1.89s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.87s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.96s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:01<?, ?ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.89s/ba]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:02<00:02,  2.25s/ba]\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-LSTM.py --lang_code ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 1/5 [00:01<00:07,  1.94s/ba]\n",
      "Running tokenizer on train dataset:  40%|####      | 2/5 [00:03<00:05,  1.88s/ba]\n",
      "Running tokenizer on train dataset:  60%|######    | 3/5 [00:05<00:03,  1.86s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.86s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.96s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:01<?, ?ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.85s/ba]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:02<00:02,  2.21s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Language Code:  am\n",
      "['neutral', 'positive', 'negative']\n",
      "balanced acc: 0.404, f1: 0.469, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.001}\n",
      "balanced acc: 0.407, f1: 0.471, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n",
      "Best Weighted F1: 0.47129786740805657\n",
      "Best Balanced Accuracy: 0.4074563515343887\n",
      "Params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-LSTM.py --lang_code ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 1/5 [00:01<00:07,  1.91s/ba]\n",
      "Running tokenizer on train dataset:  40%|####      | 2/5 [00:03<00:05,  1.91s/ba]\n",
      "Running tokenizer on train dataset:  60%|######    | 3/5 [00:05<00:03,  1.86s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.88s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.97s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:01<?, ?ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.81s/ba]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:02<00:02,  2.17s/ba]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Language Code:  am\n",
      "['neutral', 'positive', 'negative']\n",
      "balanced acc: 0.404, f1: 0.469, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.001}\n",
      "balanced acc: 0.407, f1: 0.471, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n",
      "Best Weighted F1: 0.47129786740805657\n",
      "Best Balanced Accuracy: 0.4074563515343887\n",
      "Params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-LSTM.py --lang_code pcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Language Code:  am\n",
      "['neutral', 'positive', 'negative']\n",
      "balanced acc: 0.404, f1: 0.469, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.001}\n",
      "balanced acc: 0.407, f1: 0.471, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n",
      "Best Weighted F1: 0.47129786740805657\n",
      "Best Balanced Accuracy: 0.4074563515343887\n",
      "Params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 1/5 [00:01<00:07,  1.93s/ba]\n",
      "Running tokenizer on train dataset:  40%|####      | 2/5 [00:03<00:05,  1.87s/ba]\n",
      "Running tokenizer on train dataset:  60%|######    | 3/5 [00:05<00:03,  1.84s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.87s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.95s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:01<?, ?ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.85s/ba]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:02<00:02,  2.21s/ba]\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-LSTM.py --lang_code pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Language Code:  am\n",
      "['neutral', 'positive', 'negative']\n",
      "balanced acc: 0.404, f1: 0.469, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.001}\n",
      "balanced acc: 0.407, f1: 0.471, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n",
      "Best Weighted F1: 0.47129786740805657\n",
      "Best Balanced Accuracy: 0.4074563515343887\n",
      "Params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 1/5 [00:01<00:07,  1.94s/ba]\n",
      "Running tokenizer on train dataset:  40%|####      | 2/5 [00:03<00:05,  1.93s/ba]\n",
      "Running tokenizer on train dataset:  60%|######    | 3/5 [00:05<00:03,  1.93s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.90s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:08<00:02,  2.00s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:01<?, ?ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.83s/ba]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:02<00:02,  2.19s/ba]\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-LSTM.py --lang_code sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Language Code:  am\n",
      "['neutral', 'positive', 'negative']\n",
      "balanced acc: 0.404, f1: 0.469, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.001}\n",
      "balanced acc: 0.407, f1: 0.471, params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n",
      "Best Weighted F1: 0.47129786740805657\n",
      "Best Balanced Accuracy: 0.4074563515343887\n",
      "Params: {'hidden_dim': 300, 'emb_dim': 500, 'num_layers': 2, 'dropout': 0.0, 'lstm_dropout': 0.5, 'lr': 0.0001}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 1/5 [00:01<00:07,  1.96s/ba]\n",
      "Running tokenizer on train dataset:  40%|####      | 2/5 [00:03<00:05,  1.92s/ba]\n",
      "Running tokenizer on train dataset:  60%|######    | 3/5 [00:05<00:03,  1.87s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.85s/ba]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:07<00:01,  1.96s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:01<?, ?ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.86s/ba]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:02<00:02,  2.22s/ba]\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-LSTM.py --lang_code yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments using XLMR (Kinyabert experiment disabled for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  am\n",
      "['neutral', 'positive', 'negative']\n",
      "metrics on dev set\n",
      "eval_loss : 1.018392562866211\n",
      "eval_accuracy : 0.52728285077951\n",
      "eval_f1 : 0.36408083097936267\n",
      "eval_bal_acc : 0.3333333333333333\n",
      "eval_runtime : 7.1669\n",
      "eval_samples_per_second : 250.596\n",
      "eval_steps_per_second : 31.394\n",
      "epoch : 5.0\n",
      "eval_samples : 1796\n",
      "Testing on am dev + test set\n",
      "am results:      F1: 0.364     balanced accuracy: 0.333\n",
      "Testing on dz dev + test set\n",
      "dz results:      F1: 0.0714     balanced accuracy: 0.333\n",
      "Testing on ha dev + test set\n",
      "ha results:      F1: 0.176     balanced accuracy: 0.333\n",
      "Testing on ig dev + test set\n",
      "ig results:      F1: 0.266     balanced accuracy: 0.333\n",
      "Testing on ma dev + test set\n",
      "ma results:      F1: 0.228     balanced accuracy: 0.333\n",
      "Testing on pcm dev + test set\n",
      "pcm results:      F1: 0.000652     balanced accuracy: 0.333\n",
      "Testing on pt dev + test set\n",
      "pt results:      F1: 0.357     balanced accuracy: 0.333\n",
      "Testing on sw dev + test set\n",
      "sw results:      F1: 0.433     balanced accuracy: 0.333\n",
      "Testing on yo dev + test set\n",
      "yo results:      F1: 0.204     balanced accuracy: 0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 1/5 [00:00<00:03,  1.12ba/s]\n",
      "Running tokenizer on train dataset:  40%|####      | 2/5 [00:01<00:02,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  60%|######    | 3/5 [00:02<00:01,  1.18ba/s]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:03<00:00,  1.18ba/s]\n",
      "Running tokenizer on train dataset:  80%|########  | 4/5 [00:03<00:00,  1.12ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.19ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.50s/ba]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, tokenized. If text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4188\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 655\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.60s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 496\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  20%|##        | 1/5 [00:00<00:03,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  40%|####      | 2/5 [00:01<00:02,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  60%|######    | 3/5 [00:03<00:02,  1.21s/ba]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:04<00:01,  1.09s/ba]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:04<00:01,  1.13s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4252\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 1/4 [00:00<00:02,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 2/4 [00:01<00:01,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.10ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3058\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.49s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.36s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1537\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 919\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 543\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 1/3 [00:00<00:01,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:01<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:02<00:01,  1.16s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2557\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-XLMR.py --lang_code am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  dz\n",
      "['neutral', 'negative', 'positive']\n",
      "metrics on dev set\n",
      "eval_loss : 1.0223191976547241\n",
      "eval_accuracy : 0.5080645161290323\n",
      "eval_f1 : 0.34233224081421426\n",
      "eval_bal_acc : 0.3333333333333333\n",
      "eval_runtime : 1.981\n",
      "eval_samples_per_second : 250.379\n",
      "eval_steps_per_second : 31.297\n",
      "epoch : 4.99\n",
      "eval_samples : 496\n",
      "Testing on am dev + test set\n",
      "am results:      F1: 0.1     balanced accuracy: 0.333\n",
      "Testing on dz dev + test set\n",
      "dz results:      F1: 0.342     balanced accuracy: 0.333\n",
      "Testing on ha dev + test set\n",
      "ha results:      F1: 0.147     balanced accuracy: 0.333\n",
      "Testing on ig dev + test set\n",
      "ig results:      F1: 0.101     balanced accuracy: 0.333\n",
      "Testing on ma dev + test set\n",
      "ma results:      F1: 0.132     balanced accuracy: 0.333\n",
      "Testing on pcm dev + test set\n",
      "pcm results:      F1: 0.483     balanced accuracy: 0.333\n",
      "Testing on pt dev + test set\n",
      "pt results:      F1: 0.106     balanced accuracy: 0.333\n",
      "Testing on sw dev + test set\n",
      "sw results:      F1: 0.0167     balanced accuracy: 0.333\n",
      "Testing on yo dev + test set\n",
      "yo results:      F1: 0.0808     balanced accuracy: 0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  50%|#####     | 1/2 [00:00<00:00,  1.02ba/s]\n",
      "Running tokenizer on train dataset:  50%|#####     | 1/2 [00:01<00:01,  1.12s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1155\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 180\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 496\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.64s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 496\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  20%|##        | 1/5 [00:00<00:03,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  40%|####      | 2/5 [00:01<00:02,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  60%|######    | 3/5 [00:02<00:01,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.03ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4252\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 1/4 [00:00<00:02,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 2/4 [00:01<00:01,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.08ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3058\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.50s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.40s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1537\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 919\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 543\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 1/3 [00:00<00:01,  1.08ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:01<00:00,  1.07ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:02<00:01,  1.19s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2557\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-XLMR.py --lang_code dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  ha\n",
      "['positive', 'neutral', 'negative']\n",
      "metrics on dev set\n",
      "eval_loss : 0.6512125134468079\n",
      "eval_accuracy : 0.7476481655691439\n",
      "eval_f1 : 0.747890189417467\n",
      "eval_bal_acc : 0.7461729507681542\n",
      "eval_runtime : 16.9948\n",
      "eval_samples_per_second : 250.194\n",
      "eval_steps_per_second : 31.304\n",
      "epoch : 5.0\n",
      "eval_samples : 4252\n",
      "Testing on am dev + test set\n",
      "am results:      F1: 0.406     balanced accuracy: 0.438\n",
      "Testing on dz dev + test set\n",
      "dz results:      F1: 0.554     balanced accuracy: 0.539\n",
      "Testing on ha dev + test set\n",
      "ha results:      F1: 0.748     balanced accuracy: 0.746\n",
      "Testing on ig dev + test set\n",
      "ig results:      F1: 0.423     balanced accuracy: 0.402\n",
      "Testing on ma dev + test set\n",
      "ma results:      F1: 0.484     balanced accuracy: 0.493\n",
      "Testing on pcm dev + test set\n",
      "pcm results:      F1: 0.493     balanced accuracy: 0.384\n",
      "Testing on pt dev + test set\n",
      "pt results:      F1: 0.536     balanced accuracy: 0.542\n",
      "Testing on sw dev + test set\n",
      "sw results:      F1: 0.5     balanced accuracy: 0.536\n",
      "Testing on yo dev + test set\n",
      "yo results:      F1: 0.323     balanced accuracy: 0.385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/10 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  10%|#         | 1/10 [00:00<00:08,  1.01ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 2/10 [00:01<00:07,  1.09ba/s]\n",
      "Running tokenizer on train dataset:  30%|###       | 3/10 [00:02<00:06,  1.10ba/s]\n",
      "Running tokenizer on train dataset:  40%|####      | 4/10 [00:03<00:05,  1.12ba/s]\n",
      "Running tokenizer on train dataset:  50%|#####     | 5/10 [00:04<00:04,  1.14ba/s]\n",
      "Running tokenizer on train dataset:  60%|######    | 6/10 [00:05<00:03,  1.13ba/s]\n",
      "Running tokenizer on train dataset:  70%|#######   | 7/10 [00:06<00:02,  1.15ba/s]\n",
      "Running tokenizer on train dataset:  80%|########  | 8/10 [00:07<00:01,  1.12ba/s]\n",
      "Running tokenizer on train dataset:  90%|######### | 9/10 [00:08<00:00,  1.13ba/s]\n",
      "Running tokenizer on train dataset:  90%|######### | 9/10 [00:08<00:00,  1.02ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  20%|##        | 1/5 [00:00<00:03,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  40%|####      | 2/5 [00:01<00:02,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  60%|######    | 3/5 [00:02<00:01,  1.15ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.15ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.08ba/s]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, tokenized. If text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9920\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1550\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4252\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.65s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 496\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  20%|##        | 1/5 [00:00<00:03,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  40%|####      | 2/5 [00:01<00:02,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  60%|######    | 3/5 [00:02<00:01,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.04ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4252\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 1/4 [00:00<00:02,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 2/4 [00:01<00:01,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.08ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3058\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.51s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.40s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1537\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 919\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 543\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 1/3 [00:00<00:01,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:01<00:00,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:02<00:01,  1.18s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2557\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-XLMR.py --lang_code ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  ig"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/8 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  12%|#2        | 1/8 [00:00<00:06,  1.09ba/s]\n",
      "Running tokenizer on train dataset:  25%|##5       | 2/8 [00:01<00:05,  1.10ba/s]\n",
      "Running tokenizer on train dataset:  38%|###7      | 3/8 [00:02<00:04,  1.13ba/s]\n",
      "Running tokenizer on train dataset:  50%|#####     | 4/8 [00:03<00:03,  1.13ba/s]\n",
      "Running tokenizer on train dataset:  62%|######2   | 5/8 [00:04<00:02,  1.13ba/s]\n",
      "Running tokenizer on train dataset:  75%|#######5  | 6/8 [00:05<00:01,  1.13ba/s]\n",
      "Running tokenizer on train dataset:  88%|########7 | 7/8 [00:06<00:00,  1.15ba/s]\n",
      "Running tokenizer on train dataset:  88%|########7 | 7/8 [00:06<00:00,  1.11ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 1/4 [00:00<00:02,  1.07ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 2/4 [00:01<00:01,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.09ba/s]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7134\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1115\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3058\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.62s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 496\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  20%|##        | 1/5 [00:00<00:03,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  40%|####      | 2/5 [00:01<00:02,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  60%|######    | 3/5 [00:02<00:01,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.04ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4252\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 1/4 [00:00<00:02,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 2/4 [00:01<00:01,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.10ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3058\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.48s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.38s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1537\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 919\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 543\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 1/3 [00:00<00:01,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:01<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:02<00:01,  1.17s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2557\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['positive', 'neutral', 'negative']\n",
      "metrics on dev set\n",
      "eval_loss : 1.068713903427124\n",
      "eval_accuracy : 0.43721386527141926\n",
      "eval_f1 : 0.2660090729774232\n",
      "eval_bal_acc : 0.3333333333333333\n",
      "eval_runtime : 12.1557\n",
      "eval_samples_per_second : 251.568\n",
      "eval_steps_per_second : 31.508\n",
      "epoch : 5.0\n",
      "eval_samples : 3058\n",
      "Testing on am dev + test set\n",
      "am results:      F1: 0.364     balanced accuracy: 0.333\n",
      "Testing on dz dev + test set\n",
      "dz results:      F1: 0.0714     balanced accuracy: 0.333\n",
      "Testing on ha dev + test set\n",
      "ha results:      F1: 0.176     balanced accuracy: 0.333\n",
      "Testing on ig dev + test set\n",
      "ig results:      F1: 0.266     balanced accuracy: 0.333\n",
      "Testing on ma dev + test set\n",
      "ma results:      F1: 0.228     balanced accuracy: 0.333\n",
      "Testing on pcm dev + test set\n",
      "pcm results:      F1: 0.000652     balanced accuracy: 0.333\n",
      "Testing on pt dev + test set\n",
      "pt results:      F1: 0.357     balanced accuracy: 0.333\n",
      "Testing on sw dev + test set\n",
      "sw results:      F1: 0.433     balanced accuracy: 0.333\n",
      "Testing on yo dev + test set\n",
      "yo results:      F1: 0.204     balanced accuracy: 0.333\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-XLMR.py --lang_code ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  ma\n",
      "['negative', 'neutral', 'positive']\n",
      "metrics on dev set\n",
      "eval_loss : 0.6284969449043274\n",
      "eval_accuracy : 0.7474626865671642\n",
      "eval_f1 : 0.7454551510691894\n",
      "eval_bal_acc : 0.7364083562301759\n",
      "eval_runtime : 6.6995\n",
      "eval_samples_per_second : 250.018\n",
      "eval_steps_per_second : 31.346\n",
      "epoch : 5.0\n",
      "eval_samples : 1675\n",
      "Testing on am dev + test set\n",
      "am results:      F1: 0.426     balanced accuracy: 0.523\n",
      "Testing on dz dev + test set\n",
      "dz results:      F1: 0.56     balanced accuracy: 0.549\n",
      "Testing on ha dev + test set\n",
      "ha results:      F1: 0.466     balanced accuracy: 0.503\n",
      "Testing on ig dev + test set\n",
      "ig results:      F1: 0.381     balanced accuracy: 0.386\n",
      "Testing on ma dev + test set\n",
      "ma results:      F1: 0.745     balanced accuracy: 0.736\n",
      "Testing on pcm dev + test set\n",
      "pcm results:      F1: 0.535     balanced accuracy: 0.432\n",
      "Testing on pt dev + test set\n",
      "pt results:      F1: 0.541     balanced accuracy: 0.548\n",
      "Testing on sw dev + test set\n",
      "sw results:      F1: 0.488     balanced accuracy: 0.487\n",
      "Testing on yo dev + test set\n",
      "yo results:      F1: 0.36     balanced accuracy: 0.372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  25%|##5       | 1/4 [00:00<00:02,  1.05ba/s]\n",
      "Running tokenizer on train dataset:  50%|#####     | 2/4 [00:01<00:01,  1.09ba/s]\n",
      "Running tokenizer on train dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.09ba/s]\n",
      "Running tokenizer on train dataset:  75%|#######5  | 3/4 [00:03<00:01,  1.19s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.48s/ba]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, tokenized. If text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3908\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 610\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, length, tokenized. If __index_level_0__, text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.06ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.70s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, length, tokenized. If __index_level_0__, text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, length, tokenized. If __index_level_0__, text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 496\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  20%|##        | 1/5 [00:00<00:03,  1.06ba/s]\n",
      "Running tokenizer on validation dataset:  40%|####      | 2/5 [00:01<00:02,  1.07ba/s]\n",
      "Running tokenizer on validation dataset:  60%|######    | 3/5 [00:02<00:01,  1.07ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.05ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:04<00:01,  1.01s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, length, tokenized. If __index_level_0__, text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4252\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 1/4 [00:00<00:02,  1.05ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 2/4 [00:01<00:01,  1.05ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.04ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.02ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, length, tokenized. If __index_level_0__, text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3058\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.07ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.56s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, length, tokenized. If __index_level_0__, text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.07ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.43s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, length, tokenized. If __index_level_0__, text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1537\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, length, tokenized. If __index_level_0__, text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 919\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, length, tokenized. If __index_level_0__, text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 543\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 1/3 [00:00<00:01,  1.05ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:01<00:00,  1.05ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:02<00:01,  1.23s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, length, tokenized. If __index_level_0__, text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2557\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-XLMR.py --lang_code ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  pcm\n",
      "['negative', 'positive', 'neutral']\n",
      "metrics on dev set\n",
      "eval_loss : 0.7390938997268677\n",
      "eval_accuracy : 0.623942745608328\n",
      "eval_f1 : 0.5416997442143713\n",
      "eval_bal_acc : 0.34913217623497994\n",
      "eval_runtime : 6.1323\n",
      "eval_samples_per_second : 250.639\n",
      "eval_steps_per_second : 31.473\n",
      "epoch : 5.0\n",
      "eval_samples : 1537\n",
      "Testing on am dev + test set\n",
      "am results:      F1: 0.138     balanced accuracy: 0.358\n",
      "Testing on dz dev + test set\n",
      "dz results:      F1: 0.432     balanced accuracy: 0.38\n",
      "Testing on ha dev + test set\n",
      "ha results:      F1: 0.228     balanced accuracy: 0.361\n",
      "Testing on ig dev + test set\n",
      "ig results:      F1: 0.201     balanced accuracy: 0.36\n",
      "Testing on ma dev + test set\n",
      "ma results:      F1: 0.215     balanced accuracy: 0.38\n",
      "Testing on pcm dev + test set\n",
      "pcm results:      F1: 0.542     balanced accuracy: 0.349\n",
      "Testing on pt dev + test set\n",
      "pt results:      F1: 0.173     balanced accuracy: 0.386\n",
      "Testing on sw dev + test set\n",
      "sw results:      F1: 0.0168     balanced accuracy: 0.333\n",
      "Testing on yo dev + test set\n",
      "yo results:      F1: 0.103     balanced accuracy: 0.341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  25%|##5       | 1/4 [00:00<00:02,  1.02ba/s]\n",
      "Running tokenizer on train dataset:  50%|#####     | 2/4 [00:01<00:01,  1.10ba/s]\n",
      "Running tokenizer on train dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.13ba/s]\n",
      "Running tokenizer on train dataset:  75%|#######5  | 3/4 [00:03<00:01,  1.06s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.17ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.32s/ba]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3584\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 560\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, __index_level_0__, length. If text, tokenized, __index_level_0__, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1537\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.04ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.69s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, __index_level_0__, length. If text, tokenized, __index_level_0__, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, __index_level_0__, length. If text, tokenized, __index_level_0__, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 496\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  20%|##        | 1/5 [00:00<00:03,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  40%|####      | 2/5 [00:01<00:02,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  60%|######    | 3/5 [00:02<00:01,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.04ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, __index_level_0__, length. If text, tokenized, __index_level_0__, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4252\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 1/4 [00:00<00:02,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 2/4 [00:01<00:01,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.08ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, __index_level_0__, length. If text, tokenized, __index_level_0__, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3058\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.51s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, __index_level_0__, length. If text, tokenized, __index_level_0__, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.38s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, __index_level_0__, length. If text, tokenized, __index_level_0__, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1537\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, __index_level_0__, length. If text, tokenized, __index_level_0__, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 919\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, __index_level_0__, length. If text, tokenized, __index_level_0__, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 543\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 1/3 [00:00<00:01,  1.08ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:01<00:00,  1.08ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:02<00:01,  1.19s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, __index_level_0__, length. If text, tokenized, __index_level_0__, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2557\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-XLMR.py --lang_code pcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  pt\n",
      "['neutral', 'positive', 'negative']\n",
      "metrics on dev set\n",
      "eval_loss : 1.0183545351028442\n",
      "eval_accuracy : 0.5212187159956474\n",
      "eval_f1 : 0.3571727681858585\n",
      "eval_bal_acc : 0.3333333333333333\n",
      "eval_runtime : 3.6687\n",
      "eval_samples_per_second : 250.501\n",
      "eval_steps_per_second : 31.347\n",
      "epoch : 5.0\n",
      "eval_samples : 919\n",
      "Testing on am dev + test set\n",
      "am results:      F1: 0.364     balanced accuracy: 0.333\n",
      "Testing on dz dev + test set\n",
      "dz results:      F1: 0.0714     balanced accuracy: 0.333\n",
      "Testing on ha dev + test set\n",
      "ha results:      F1: 0.176     balanced accuracy: 0.333\n",
      "Testing on ig dev + test set\n",
      "ig results:      F1: 0.266     balanced accuracy: 0.333\n",
      "Testing on ma dev + test set\n",
      "ma results:      F1: 0.228     balanced accuracy: 0.333\n",
      "Testing on pcm dev + test set\n",
      "pcm results:      F1: 0.000652     balanced accuracy: 0.333\n",
      "Testing on pt dev + test set\n",
      "pt results:      F1: 0.357     balanced accuracy: 0.333\n",
      "Testing on sw dev + test set\n",
      "sw results:      F1: 0.433     balanced accuracy: 0.333\n",
      "Testing on yo dev + test set\n",
      "yo results:      F1: 0.204     balanced accuracy: 0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  33%|###3      | 1/3 [00:00<00:01,  1.10ba/s]\n",
      "Running tokenizer on train dataset:  67%|######6   | 2/3 [00:01<00:00,  1.14ba/s]\n",
      "Running tokenizer on train dataset:  67%|######6   | 2/3 [00:01<00:00,  1.06ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, length, text. If tokenized, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2144\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 335\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, length, __index_level_0__, text. If tokenized, length, __index_level_0__, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 919\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.02ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.70s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, length, __index_level_0__, text. If tokenized, length, __index_level_0__, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, length, __index_level_0__, text. If tokenized, length, __index_level_0__, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 496\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  20%|##        | 1/5 [00:00<00:03,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  40%|####      | 2/5 [00:01<00:02,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  60%|######    | 3/5 [00:02<00:01,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.03ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, length, __index_level_0__, text. If tokenized, length, __index_level_0__, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4252\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 1/4 [00:00<00:02,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 2/4 [00:01<00:01,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.08ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, length, __index_level_0__, text. If tokenized, length, __index_level_0__, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3058\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.51s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, length, __index_level_0__, text. If tokenized, length, __index_level_0__, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.38s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, length, __index_level_0__, text. If tokenized, length, __index_level_0__, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1537\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, length, __index_level_0__, text. If tokenized, length, __index_level_0__, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 919\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, length, __index_level_0__, text. If tokenized, length, __index_level_0__, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 543\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 1/3 [00:00<00:01,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:01<00:00,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:02<00:01,  1.18s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, length, __index_level_0__, text. If tokenized, length, __index_level_0__, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2557\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-XLMR.py --lang_code pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  sw\n",
      "['positive', 'neutral', 'negative']\n",
      "metrics on dev set\n",
      "eval_loss : 0.8970898985862732\n",
      "eval_accuracy : 0.585635359116022\n",
      "eval_f1 : 0.43259476004389086\n",
      "eval_bal_acc : 0.3333333333333333\n",
      "eval_runtime : 2.1655\n",
      "eval_samples_per_second : 250.752\n",
      "eval_steps_per_second : 31.402\n",
      "epoch : 5.0\n",
      "eval_samples : 543\n",
      "Testing on am dev + test set\n",
      "am results:      F1: 0.364     balanced accuracy: 0.333\n",
      "Testing on dz dev + test set\n",
      "dz results:      F1: 0.0714     balanced accuracy: 0.333\n",
      "Testing on ha dev + test set\n",
      "ha results:      F1: 0.176     balanced accuracy: 0.333\n",
      "Testing on ig dev + test set\n",
      "ig results:      F1: 0.266     balanced accuracy: 0.333\n",
      "Testing on ma dev + test set\n",
      "ma results:      F1: 0.228     balanced accuracy: 0.333\n",
      "Testing on pcm dev + test set\n",
      "pcm results:      F1: 0.000652     balanced accuracy: 0.333\n",
      "Testing on pt dev + test set\n",
      "pt results:      F1: 0.357     balanced accuracy: 0.333\n",
      "Testing on sw dev + test set\n",
      "sw results:      F1: 0.433     balanced accuracy: 0.333\n",
      "Testing on yo dev + test set\n",
      "yo results:      F1: 0.204     balanced accuracy: 0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  50%|#####     | 1/2 [00:00<00:00,  1.02ba/s]\n",
      "Running tokenizer on train dataset:  50%|#####     | 1/2 [00:01<00:01,  1.21s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, tokenized. If text, length, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1267\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 200\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 543\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.64s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 496\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  20%|##        | 1/5 [00:00<00:03,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  40%|####      | 2/5 [00:01<00:02,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  60%|######    | 3/5 [00:02<00:01,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.03ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4252\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 1/4 [00:00<00:02,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 2/4 [00:01<00:01,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.09ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3058\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.51s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.40s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1537\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 919\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 543\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 1/3 [00:00<00:01,  1.08ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:01<00:00,  1.08ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:02<00:01,  1.19s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2557\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-XLMR.py --lang_code sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  yo\n",
      "['negative', 'positive', 'neutral']\n",
      "metrics on dev set\n",
      "eval_loss : 0.807750940322876\n",
      "eval_accuracy : 0.657411028549081\n",
      "eval_f1 : 0.6512205969575711\n",
      "eval_bal_acc : 0.6203111118995269\n",
      "eval_runtime : 10.3173\n",
      "eval_samples_per_second : 247.835\n",
      "eval_steps_per_second : 31.016\n",
      "epoch : 5.0\n",
      "eval_samples : 2557\n",
      "Testing on am dev + test set\n",
      "am results:      F1: 0.261     balanced accuracy: 0.386\n",
      "Testing on dz dev + test set\n",
      "dz results:      F1: 0.399     balanced accuracy: 0.501\n",
      "Testing on ha dev + test set\n",
      "ha results:      F1: 0.462     balanced accuracy: 0.488\n",
      "Testing on ig dev + test set\n",
      "ig results:      F1: 0.355     balanced accuracy: 0.378\n",
      "Testing on ma dev + test set\n",
      "ma results:      F1: 0.295     balanced accuracy: 0.36\n",
      "Testing on pcm dev + test set\n",
      "pcm results:      F1: 0.376     balanced accuracy: 0.353\n",
      "Testing on pt dev + test set\n",
      "pt results:      F1: 0.298     balanced accuracy: 0.392\n",
      "Testing on sw dev + test set\n",
      "sw results:      F1: 0.225     balanced accuracy: 0.4\n",
      "Testing on yo dev + test set\n",
      "yo results:      F1: 0.651     balanced accuracy: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/6 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:  17%|#6        | 1/6 [00:00<00:04,  1.03ba/s]\n",
      "Running tokenizer on train dataset:  33%|###3      | 2/6 [00:01<00:03,  1.07ba/s]\n",
      "Running tokenizer on train dataset:  50%|#####     | 3/6 [00:02<00:02,  1.08ba/s]\n",
      "Running tokenizer on train dataset:  67%|######6   | 4/6 [00:03<00:01,  1.09ba/s]\n",
      "Running tokenizer on train dataset:  83%|########3 | 5/6 [00:04<00:00,  1.11ba/s]\n",
      "Running tokenizer on train dataset:  83%|########3 | 5/6 [00:05<00:01,  1.09s/ba]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 1/3 [00:00<00:01,  1.15ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:01<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:02<00:01,  1.15s/ba]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, text, length. If tokenized, text, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5965\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 930\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, text, length, __index_level_0__. If tokenized, text, length, __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2557\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.09ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.64s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, text, length, __index_level_0__. If tokenized, text, length, __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, text, length, __index_level_0__. If tokenized, text, length, __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 496\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  20%|##        | 1/5 [00:00<00:03,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  40%|####      | 2/5 [00:01<00:02,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  60%|######    | 3/5 [00:02<00:01,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.04ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, text, length, __index_level_0__. If tokenized, text, length, __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4252\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 1/4 [00:00<00:02,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 2/4 [00:01<00:01,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.09ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, text, length, __index_level_0__. If tokenized, text, length, __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3058\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.49s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, text, length, __index_level_0__. If tokenized, text, length, __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.39s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, text, length, __index_level_0__. If tokenized, text, length, __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1537\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, text, length, __index_level_0__. If tokenized, text, length, __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 919\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, text, length, __index_level_0__. If tokenized, text, length, __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 543\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 1/3 [00:00<00:01,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:01<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:02<00:01,  1.16s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, text, length, __index_level_0__. If tokenized, text, length, __index_level_0__ are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2557\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-XLMR.py --lang_code yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same, but now for all data (task B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['negative', 'neutral', 'positive']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/40 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:   2%|2         | 1/40 [00:00<00:21,  1.84ba/s]\n",
      "Running tokenizer on train dataset:   5%|5         | 2/40 [00:01<00:20,  1.87ba/s]\n",
      "Running tokenizer on train dataset:   8%|7         | 3/40 [00:01<00:19,  1.90ba/s]\n",
      "Running tokenizer on train dataset:  10%|#         | 4/40 [00:02<00:18,  1.91ba/s]\n",
      "Running tokenizer on train dataset:  12%|#2        | 5/40 [00:02<00:17,  1.95ba/s]\n",
      "Running tokenizer on train dataset:  15%|#5        | 6/40 [00:03<00:17,  1.97ba/s]\n",
      "Running tokenizer on train dataset:  18%|#7        | 7/40 [00:03<00:17,  1.89ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 8/40 [00:04<00:16,  1.90ba/s]\n",
      "Running tokenizer on train dataset:  22%|##2       | 9/40 [00:04<00:16,  1.91ba/s]\n",
      "Running tokenizer on train dataset:  25%|##5       | 10/40 [00:05<00:15,  1.91ba/s]\n",
      "Running tokenizer on train dataset:  28%|##7       | 11/40 [00:05<00:15,  1.93ba/s]\n",
      "Running tokenizer on train dataset:  30%|###       | 12/40 [00:06<00:14,  1.94ba/s]\n",
      "Running tokenizer on train dataset:  32%|###2      | 13/40 [00:06<00:14,  1.93ba/s]\n",
      "Running tokenizer on train dataset:  35%|###5      | 14/40 [00:07<00:13,  1.96ba/s]\n",
      "Running tokenizer on train dataset:  38%|###7      | 15/40 [00:07<00:12,  1.97ba/s]\n",
      "Running tokenizer on train dataset:  40%|####      | 16/40 [00:08<00:12,  1.98ba/s]\n",
      "Running tokenizer on train dataset:  42%|####2     | 17/40 [00:08<00:11,  1.98ba/s]\n",
      "Running tokenizer on train dataset:  45%|####5     | 18/40 [00:09<00:11,  1.99ba/s]\n",
      "Running tokenizer on train dataset:  48%|####7     | 19/40 [00:09<00:10,  1.99ba/s]\n",
      "Running tokenizer on train dataset:  50%|#####     | 20/40 [00:10<00:09,  2.01ba/s]\n",
      "Running tokenizer on train dataset:  52%|#####2    | 21/40 [00:10<00:09,  1.93ba/s]\n",
      "Running tokenizer on train dataset:  55%|#####5    | 22/40 [00:11<00:09,  1.96ba/s]\n",
      "Running tokenizer on train dataset:  57%|#####7    | 23/40 [00:11<00:08,  1.97ba/s]\n",
      "Running tokenizer on train dataset:  60%|######    | 24/40 [00:12<00:08,  1.98ba/s]\n",
      "Running tokenizer on train dataset:  62%|######2   | 25/40 [00:12<00:07,  1.98ba/s]\n",
      "Running tokenizer on train dataset:  65%|######5   | 26/40 [00:13<00:07,  1.98ba/s]\n",
      "Running tokenizer on train dataset:  68%|######7   | 27/40 [00:13<00:06,  1.99ba/s]\n",
      "Running tokenizer on train dataset:  70%|#######   | 28/40 [00:14<00:06,  1.99ba/s]\n",
      "Running tokenizer on train dataset:  72%|#######2  | 29/40 [00:14<00:05,  2.01ba/s]\n",
      "Running tokenizer on train dataset:  75%|#######5  | 30/40 [00:15<00:04,  2.00ba/s]\n",
      "Running tokenizer on train dataset:  78%|#######7  | 31/40 [00:15<00:04,  2.00ba/s]\n",
      "Running tokenizer on train dataset:  80%|########  | 32/40 [00:16<00:04,  2.00ba/s]\n",
      "Running tokenizer on train dataset:  82%|########2 | 33/40 [00:16<00:03,  2.01ba/s]\n",
      "Running tokenizer on train dataset:  85%|########5 | 34/40 [00:17<00:03,  1.94ba/s]\n",
      "Running tokenizer on train dataset:  88%|########7 | 35/40 [00:17<00:02,  1.97ba/s]\n",
      "Running tokenizer on train dataset:  90%|######### | 36/40 [00:18<00:02,  1.97ba/s]\n",
      "Running tokenizer on train dataset:  92%|#########2| 37/40 [00:18<00:01,  1.98ba/s]\n",
      "Running tokenizer on train dataset:  95%|#########5| 38/40 [00:19<00:01,  1.98ba/s]\n",
      "Running tokenizer on train dataset:  98%|#########7| 39/40 [00:19<00:00,  2.00ba/s]\n",
      "Running tokenizer on train dataset:  98%|#########7| 39/40 [00:19<00:00,  1.95ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/6 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  17%|#6        | 1/6 [00:00<00:02,  2.02ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 2/6 [00:00<00:01,  2.01ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 3/6 [00:01<00:01,  2.02ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 4/6 [00:01<00:00,  2.02ba/s]\n",
      "Running tokenizer on validation dataset:  83%|########3 | 5/6 [00:02<00:00,  2.01ba/s]\n",
      "Running tokenizer on validation dataset:  83%|########3 | 5/6 [00:02<00:00,  1.79ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/12 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   8%|8         | 1/12 [00:00<00:06,  1.81ba/s]\n",
      "Running tokenizer on validation dataset:  17%|#6        | 2/12 [00:01<00:05,  1.93ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 3/12 [00:01<00:04,  1.97ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 4/12 [00:02<00:04,  1.99ba/s]\n",
      "Running tokenizer on validation dataset:  42%|####1     | 5/12 [00:02<00:03,  2.00ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 6/12 [00:03<00:02,  2.01ba/s]\n",
      "Running tokenizer on validation dataset:  58%|#####8    | 7/12 [00:03<00:02,  2.02ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 8/12 [00:04<00:01,  2.01ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 9/12 [00:04<00:01,  2.01ba/s]\n",
      "Running tokenizer on validation dataset:  83%|########3 | 10/12 [00:05<00:00,  2.01ba/s]\n",
      "Running tokenizer on validation dataset:  92%|#########1| 11/12 [00:05<00:00,  2.02ba/s]\n",
      "Running tokenizer on validation dataset:  92%|#########1| 11/12 [00:05<00:00,  1.96ba/s]\n",
      "C:\\Users\\Thomas\\Downloads\\afrisent\\afrisenti-10701\\baselines\\Baseline_B-LSTM.py:310: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_ids = torch.LongTensor(np.array(train_dataset['input_ids']))\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Thomas\\Downloads\\afrisent\\afrisenti-10701\\baselines\\Baseline_B-LSTM.py\", line 310, in <module>\n",
      "    train_ids = torch.LongTensor(np.array(train_dataset['input_ids']))\n",
      "TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_B-LSTM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'neutral', 'positive']\n",
      "eval_loss : 1.0241329669952393\n",
      "eval_accuracy : 0.4676767676767677\n",
      "eval_runtime : 66.9962\n",
      "eval_samples_per_second : 251.208\n",
      "eval_steps_per_second : 31.405\n",
      "epoch : 5.0\n",
      "eval_samples : 16830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/40 [00:00<?, ?ba/s]\n",
      "Running tokenizer on train dataset:   2%|2         | 1/40 [00:00<00:35,  1.10ba/s]\n",
      "Running tokenizer on train dataset:   5%|5         | 2/40 [00:01<00:33,  1.14ba/s]\n",
      "Running tokenizer on train dataset:   8%|7         | 3/40 [00:02<00:32,  1.14ba/s]\n",
      "Running tokenizer on train dataset:  10%|#         | 4/40 [00:03<00:31,  1.15ba/s]\n",
      "Running tokenizer on train dataset:  12%|#2        | 5/40 [00:04<00:30,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  15%|#5        | 6/40 [00:05<00:29,  1.14ba/s]\n",
      "Running tokenizer on train dataset:  18%|#7        | 7/40 [00:06<00:28,  1.15ba/s]\n",
      "Running tokenizer on train dataset:  20%|##        | 8/40 [00:06<00:27,  1.15ba/s]\n",
      "Running tokenizer on train dataset:  22%|##2       | 9/40 [00:07<00:26,  1.16ba/s]\n",
      "Running tokenizer on train dataset:  25%|##5       | 10/40 [00:08<00:25,  1.16ba/s]\n",
      "Running tokenizer on train dataset:  28%|##7       | 11/40 [00:09<00:24,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  30%|###       | 12/40 [00:10<00:23,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  32%|###2      | 13/40 [00:11<00:23,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  35%|###5      | 14/40 [00:12<00:22,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  38%|###7      | 15/40 [00:12<00:21,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  40%|####      | 16/40 [00:13<00:20,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  42%|####2     | 17/40 [00:14<00:19,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  45%|####5     | 18/40 [00:15<00:18,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  48%|####7     | 19/40 [00:16<00:17,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  50%|#####     | 20/40 [00:17<00:17,  1.18ba/s]\n",
      "Running tokenizer on train dataset:  52%|#####2    | 21/40 [00:18<00:16,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  55%|#####5    | 22/40 [00:18<00:15,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  57%|#####7    | 23/40 [00:19<00:14,  1.18ba/s]\n",
      "Running tokenizer on train dataset:  60%|######    | 24/40 [00:20<00:13,  1.18ba/s]\n",
      "Running tokenizer on train dataset:  62%|######2   | 25/40 [00:21<00:12,  1.18ba/s]\n",
      "Running tokenizer on train dataset:  65%|######5   | 26/40 [00:22<00:11,  1.18ba/s]\n",
      "Running tokenizer on train dataset:  68%|######7   | 27/40 [00:23<00:11,  1.15ba/s]\n",
      "Running tokenizer on train dataset:  70%|#######   | 28/40 [00:24<00:10,  1.16ba/s]\n",
      "Running tokenizer on train dataset:  72%|#######2  | 29/40 [00:24<00:09,  1.16ba/s]\n",
      "Running tokenizer on train dataset:  75%|#######5  | 30/40 [00:25<00:08,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  78%|#######7  | 31/40 [00:26<00:07,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  80%|########  | 32/40 [00:27<00:06,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  82%|########2 | 33/40 [00:28<00:05,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  85%|########5 | 34/40 [00:29<00:05,  1.18ba/s]\n",
      "Running tokenizer on train dataset:  88%|########7 | 35/40 [00:30<00:04,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  90%|######### | 36/40 [00:30<00:03,  1.17ba/s]\n",
      "Running tokenizer on train dataset:  92%|#########2| 37/40 [00:31<00:02,  1.18ba/s]\n",
      "Running tokenizer on train dataset:  95%|#########5| 38/40 [00:32<00:01,  1.18ba/s]\n",
      "Running tokenizer on train dataset:  98%|#########7| 39/40 [00:33<00:00,  1.18ba/s]\n",
      "Running tokenizer on train dataset:  98%|#########7| 39/40 [00:33<00:00,  1.16ba/s]\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/17 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   6%|5         | 1/17 [00:00<00:13,  1.19ba/s]\n",
      "Running tokenizer on validation dataset:  12%|#1        | 2/17 [00:01<00:12,  1.19ba/s]\n",
      "Running tokenizer on validation dataset:  18%|#7        | 3/17 [00:02<00:11,  1.19ba/s]\n",
      "Running tokenizer on validation dataset:  24%|##3       | 4/17 [00:03<00:10,  1.19ba/s]\n",
      "Running tokenizer on validation dataset:  29%|##9       | 5/17 [00:04<00:10,  1.18ba/s]\n",
      "Running tokenizer on validation dataset:  35%|###5      | 6/17 [00:05<00:09,  1.18ba/s]\n",
      "Running tokenizer on validation dataset:  41%|####1     | 7/17 [00:05<00:08,  1.18ba/s]\n",
      "Running tokenizer on validation dataset:  47%|####7     | 8/17 [00:06<00:07,  1.18ba/s]\n",
      "Running tokenizer on validation dataset:  53%|#####2    | 9/17 [00:07<00:06,  1.18ba/s]\n",
      "Running tokenizer on validation dataset:  59%|#####8    | 10/17 [00:08<00:05,  1.18ba/s]\n",
      "Running tokenizer on validation dataset:  65%|######4   | 11/17 [00:09<00:05,  1.18ba/s]\n",
      "Running tokenizer on validation dataset:  71%|#######   | 12/17 [00:10<00:04,  1.18ba/s]\n",
      "Running tokenizer on validation dataset:  76%|#######6  | 13/17 [00:11<00:03,  1.15ba/s]\n",
      "Running tokenizer on validation dataset:  82%|########2 | 14/17 [00:11<00:02,  1.16ba/s]\n",
      "Running tokenizer on validation dataset:  88%|########8 | 15/17 [00:12<00:01,  1.16ba/s]\n",
      "Running tokenizer on validation dataset:  94%|#########4| 16/17 [00:13<00:00,  1.17ba/s]\n",
      "Running tokenizer on validation dataset:  94%|#########4| 16/17 [00:14<00:00,  1.12ba/s]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 39268\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 6135\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: __index_level_0__, text, tokenized, length. If __index_level_0__, text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16830\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, tokenized, length. If text, tokenized, length are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 599\n",
      "  Batch size = 8\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Thomas\\Downloads\\afrisent\\afrisenti-10701\\baselines\\Baseline_B-XLMR.py\", line 358, in <module>\n",
      "    gc.collect()\n",
      "NameError: name 'gc' is not defined\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_B-XLMR.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for task C, with 2 flavors: training on other african languages or on english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  am\n",
      "dev data: Dataset({\n",
      "    features: ['text', 'label', '__index_level_0__'],\n",
      "    num_rows: 1796\n",
      "})\n",
      "['neutral', 'positive', 'negative']\n",
      "Adding dz to the training set\n",
      "Adding ha to the training set\n",
      "Adding ig to the training set\n",
      "Adding ma to the training set\n",
      "Adding pcm to the training set\n",
      "Adding pt to the training set\n",
      "Adding sw to the training set\n",
      "Adding yo to the training set\n",
      "train data: Dataset({\n",
      "    features: ['text', 'label', '__index_level_0__'],\n",
      "    num_rows: 35077\n",
      "})\n",
      "metrics on dev set\n",
      "eval_loss : 1.0797785520553589\n",
      "eval_accuracy : 0.52728285077951\n",
      "eval_f1 : 0.36408083097936267\n",
      "eval_bal_acc : 0.3333333333333333\n",
      "eval_runtime : 7.1686\n",
      "eval_samples_per_second : 250.536\n",
      "eval_steps_per_second : 31.387\n",
      "epoch : 5.0\n",
      "eval_samples : 1796\n",
      "Testing on am dev + test set\n",
      "am results:      F1: 0.364     balanced accuracy: 0.333\n",
      "Testing on dz dev + test set\n",
      "dz results:      F1: 0.0714     balanced accuracy: 0.333\n",
      "Testing on ha dev + test set\n",
      "ha results:      F1: 0.176     balanced accuracy: 0.333\n",
      "Testing on ig dev + test set\n",
      "ig results:      F1: 0.266     balanced accuracy: 0.333\n",
      "Testing on ma dev + test set\n",
      "ma results:      F1: 0.228     balanced accuracy: 0.333\n",
      "Testing on pcm dev + test set\n",
      "pcm results:      F1: 0.000652     balanced accuracy: 0.333\n",
      "Testing on pt dev + test set\n",
      "pt results:      F1: 0.357     balanced accuracy: 0.333\n",
      "Testing on sw dev + test set\n",
      "sw results:      F1: 0.433     balanced accuracy: 0.333\n",
      "Testing on yo dev + test set\n",
      "yo results:      F1: 0.204     balanced accuracy: 0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/36 [00:00<?, ?ba/s]\n",
      "  3%|2         | 1/36 [00:00<00:34,  1.01ba/s]\n",
      "  6%|5         | 2/36 [00:01<00:32,  1.06ba/s]\n",
      "  8%|8         | 3/36 [00:02<00:30,  1.08ba/s]\n",
      " 11%|#1        | 4/36 [00:03<00:29,  1.09ba/s]\n",
      " 14%|#3        | 5/36 [00:04<00:27,  1.11ba/s]\n",
      " 17%|#6        | 6/36 [00:05<00:26,  1.13ba/s]\n",
      " 19%|#9        | 7/36 [00:06<00:26,  1.11ba/s]\n",
      " 22%|##2       | 8/36 [00:07<00:24,  1.13ba/s]\n",
      " 25%|##5       | 9/36 [00:08<00:24,  1.12ba/s]\n",
      " 28%|##7       | 10/36 [00:09<00:23,  1.13ba/s]\n",
      " 31%|###       | 11/36 [00:09<00:21,  1.14ba/s]\n",
      " 33%|###3      | 12/36 [00:10<00:21,  1.14ba/s]\n",
      " 36%|###6      | 13/36 [00:11<00:20,  1.13ba/s]\n",
      " 39%|###8      | 14/36 [00:12<00:19,  1.14ba/s]\n",
      " 42%|####1     | 15/36 [00:13<00:18,  1.15ba/s]\n",
      " 44%|####4     | 16/36 [00:14<00:17,  1.16ba/s]\n",
      " 47%|####7     | 17/36 [00:15<00:16,  1.16ba/s]\n",
      " 50%|#####     | 18/36 [00:15<00:15,  1.17ba/s]\n",
      " 53%|#####2    | 19/36 [00:16<00:14,  1.16ba/s]\n",
      " 56%|#####5    | 20/36 [00:17<00:13,  1.16ba/s]\n",
      " 58%|#####8    | 21/36 [00:18<00:12,  1.16ba/s]\n",
      " 61%|######1   | 22/36 [00:19<00:11,  1.17ba/s]\n",
      " 64%|######3   | 23/36 [00:20<00:11,  1.16ba/s]\n",
      " 67%|######6   | 24/36 [00:21<00:10,  1.15ba/s]\n",
      " 69%|######9   | 25/36 [00:21<00:09,  1.14ba/s]\n",
      " 72%|#######2  | 26/36 [00:22<00:08,  1.13ba/s]\n",
      " 75%|#######5  | 27/36 [00:23<00:07,  1.14ba/s]\n",
      " 78%|#######7  | 28/36 [00:24<00:06,  1.15ba/s]\n",
      " 81%|########  | 29/36 [00:25<00:06,  1.15ba/s]\n",
      " 83%|########3 | 30/36 [00:26<00:05,  1.14ba/s]\n",
      " 86%|########6 | 31/36 [00:27<00:04,  1.15ba/s]\n",
      " 89%|########8 | 32/36 [00:28<00:03,  1.14ba/s]\n",
      " 92%|#########1| 33/36 [00:28<00:02,  1.15ba/s]\n",
      " 94%|#########4| 34/36 [00:29<00:01,  1.15ba/s]\n",
      " 97%|#########7| 35/36 [00:30<00:00,  1.15ba/s]\n",
      " 97%|#########7| 35/36 [00:30<00:00,  1.14ba/s]\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?ba/s]\n",
      " 50%|#####     | 1/2 [00:00<00:00,  1.18ba/s]\n",
      " 50%|#####     | 1/2 [00:01<00:01,  1.53s/ba]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, __index_level_0__, length, text. If tokenized, __index_level_0__, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 35077\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 5480\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, __index_level_0__, length, text. If tokenized, __index_level_0__, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.62s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, __index_level_0__, length, text. If tokenized, __index_level_0__, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1796\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, __index_level_0__, length, text. If tokenized, __index_level_0__, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 496\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/5 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  20%|##        | 1/5 [00:00<00:03,  1.14ba/s]\n",
      "Running tokenizer on validation dataset:  40%|####      | 2/5 [00:01<00:02,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  60%|######    | 3/5 [00:02<00:01,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.11ba/s]\n",
      "Running tokenizer on validation dataset:  80%|########  | 4/5 [00:03<00:00,  1.03ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, __index_level_0__, length, text. If tokenized, __index_level_0__, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4252\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  25%|##5       | 1/4 [00:00<00:02,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 2/4 [00:01<00:01,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  75%|#######5  | 3/4 [00:02<00:00,  1.10ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, __index_level_0__, length, text. If tokenized, __index_level_0__, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3058\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.13ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.48s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, __index_level_0__, length, text. If tokenized, __index_level_0__, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1675\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/2 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:00<00:00,  1.12ba/s]\n",
      "Running tokenizer on validation dataset:  50%|#####     | 1/2 [00:01<00:01,  1.37s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, __index_level_0__, length, text. If tokenized, __index_level_0__, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1537\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, __index_level_0__, length, text. If tokenized, __index_level_0__, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 919\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, __index_level_0__, length, text. If tokenized, __index_level_0__, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 543\n",
      "  Batch size = 8\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/3 [00:00<?, ?ba/s]\n",
      "Running tokenizer on validation dataset:  33%|###3      | 1/3 [00:00<00:01,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:01<00:00,  1.10ba/s]\n",
      "Running tokenizer on validation dataset:  67%|######6   | 2/3 [00:02<00:01,  1.16s/ba]\n",
      "The following columns in the test set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: tokenized, __index_level_0__, length, text. If tokenized, __index_level_0__, length, text are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2557\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_C-XLMR.py --lang_code am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  am"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "  0%|          | 0/9 [00:00<?, ?ba/s]\n",
      " 11%|#1        | 1/9 [00:00<00:07,  1.09ba/s]\n",
      " 22%|##2       | 2/9 [00:01<00:06,  1.13ba/s]\n",
      " 33%|###3      | 3/9 [00:02<00:05,  1.12ba/s]\n",
      " 44%|####4     | 4/9 [00:03<00:04,  1.12ba/s]\n",
      " 56%|#####5    | 5/9 [00:04<00:03,  1.14ba/s]\n",
      " 67%|######6   | 6/9 [00:05<00:02,  1.13ba/s]\n",
      " 78%|#######7  | 7/9 [00:06<00:01,  1.14ba/s]\n",
      " 89%|########8 | 8/9 [00:07<00:00,  1.10ba/s]\n",
      " 89%|########8 | 8/9 [00:07<00:00,  1.01ba/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?ba/s]\n",
      " 25%|##5       | 1/4 [00:00<00:02,  1.12ba/s]\n",
      " 50%|#####     | 2/4 [00:01<00:01,  1.12ba/s]\n",
      " 75%|#######5  | 3/4 [00:02<00:00,  1.13ba/s]\n",
      " 75%|#######5  | 3/4 [00:03<00:01,  1.12s/ba]\n",
      "The following columns in the training set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\Thomas\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 8904\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1390\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: text, length, __index_level_0__, tokenized. If text, length, __index_level_0__, tokenized are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3817\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using EN for zero shot\n",
      "['positive', 'neutral', 'negative']\n",
      "metrics on dev set\n",
      "eval_loss : 0.7768728733062744\n",
      "eval_accuracy : 0.7770500392978779\n",
      "eval_f1 : 0.7768393124394632\n",
      "eval_bal_acc : 0.7605186768250274\n",
      "eval_runtime : 15.2484\n",
      "eval_samples_per_second : 250.321\n",
      "eval_steps_per_second : 31.347\n",
      "epoch : 5.0\n",
      "eval_samples : 3817\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_C-XLMR.py --use_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_C-XLMR.py --lang_code ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python Baseline_C-XLMR.py --lang_code pcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
