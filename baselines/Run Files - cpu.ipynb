{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thomas/afrisenti-10701/baselines\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get the data\n",
    "\n",
    "import os\n",
    "colab = False\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    proj_folder = '/content/drive/MyDrive'\n",
    "else:\n",
    "    proj_folder = os.getcwd()\n",
    "\n",
    "%cd {proj_folder}\n",
    "\n",
    "\n",
    "PROJECT_DIR = f'{proj_folder}/afrisent-semeval-2023'\n",
    "if not os.path.isdir(PROJECT_DIR):\n",
    "  %run Make_Datasets.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments on codes: 'am', 'dz', 'ha', 'ig', 'ma', 'pcm', 'pt', 'sw', 'yo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  am\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "['neutral', 'positive', 'negative']\n",
      "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 3000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 29855    /    29855\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ███░░░░░ 14602    /    29855\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██████░░ 25928    /    29855\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 29855    /    29855\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 3000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 5000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 6068     /     6068\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running tokenizer on train dataset: 100%|█████████| 5/5 [00:10<00:00,  2.14s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 1/1 [00:01<00:00,  1.52s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 2/2 [00:03<00:00,  1.59s/ba]\n",
      "0.5338345864661654 0.3715907415597818 0.3333333333333333\n",
      "0.23642439431913115 0.09041635620582988 0.3333333333333333\n",
      "0.2297410192147034 0.08584073408158077 0.3333333333333333\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "\n",
      "    bigram counts\n",
      "    Train score: 0.778\n",
      "    Eval score: 0.521\n",
      "    Balanced Accuracy: 0.457\n",
      "    Weighted Test F1: 0.514\n",
      "    Params: {'alpha': 0.1}\n",
      "    \n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "\n",
      "    unigram tf-idf\n",
      "    Train score: 0.765\n",
      "    Eval score: 0.55\n",
      "    Balanced Accuracy: 0.444\n",
      "    Weighted Test F1: 0.519\n",
      "    Params: {'C': 1, 'gamma': 'scale', 'kernel': 'linear', 'max_iter': 1000}\n",
      "    \n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.755\n",
      "    Eval score: 0.551\n",
      "    Balanced Accuracy: 0.426\n",
      "    Weighted Test F1: 0.504\n",
      "    Params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "    \n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.515\n",
      "    Eval score: 0.534\n",
      "    Balanced Accuracy: 0.333\n",
      "    Weighted Test F1: 0.372\n",
      "    Params: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 500}\n",
      "    \n",
      "<function AdaDTC at 0x7fe9432b9160>\n",
      "\n",
      "    trigram counts\n",
      "    Train score: 0.912\n",
      "    Eval score: 0.52\n",
      "    Balanced Accuracy: 0.437\n",
      "    Weighted Test F1: 0.502\n",
      "    Params: {'base_estimator__criterion': 'entropy', 'base_estimator__max_depth': 3, 'base_estimator__splitter': 'random', 'learning_rate': 1, 'n_estimators': 500}\n",
      "    \n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "\n",
      "    trigram tf-idf\n",
      "    Train score: 0.778\n",
      "    Eval score: 0.549\n",
      "    Balanced Accuracy: 0.434\n",
      "    Weighted Test F1: 0.51\n",
      "    Params: {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-cpu.py --lang_code am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  dz\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "['negative', 'neutral', 'positive']\n",
      "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 6583     /     6583\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 6583     /     6583\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 1323     /     1323\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running tokenizer on train dataset: 100%|█████████| 2/2 [00:03<00:00,  1.88s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 1/1 [00:00<00:00,  2.03ba/s]\n",
      "Running tokenizer on validation dataset: 100%|████| 1/1 [00:01<00:00,  1.05s/ba]\n",
      "0.5045317220543807 0.3383807131850666 0.3333333333333333\n",
      "0.18731117824773413 0.05910072799674054 0.3333333333333333\n",
      "0.3081570996978852 0.14518255967290666 0.3333333333333333\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "\n",
      "    trigram counts\n",
      "    Train score: 0.817\n",
      "    Eval score: 0.58\n",
      "    Balanced Accuracy: 0.49\n",
      "    Weighted Test F1: 0.562\n",
      "    Params: {'alpha': 0.1}\n",
      "    \n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "\n",
      "    bigram counts\n",
      "    Train score: 0.977\n",
      "    Eval score: 0.538\n",
      "    Balanced Accuracy: 0.463\n",
      "    Weighted Test F1: 0.523\n",
      "    Params: {'C': 10, 'gamma': 'auto', 'kernel': 'linear', 'max_iter': 1000}\n",
      "    \n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.907\n",
      "    Eval score: 0.538\n",
      "    Balanced Accuracy: 0.458\n",
      "    Weighted Test F1: 0.522\n",
      "    Params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "    \n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\n",
      "    trigram counts\n",
      "    Train score: 0.577\n",
      "    Eval score: 0.525\n",
      "    Balanced Accuracy: 0.356\n",
      "    Weighted Test F1: 0.382\n",
      "    Params: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 500}\n",
      "    \n",
      "<function AdaDTC at 0x7fe654d6eee0>\n",
      "\n",
      "    unigram tf-idf\n",
      "    Train score: 0.925\n",
      "    Eval score: 0.532\n",
      "    Balanced Accuracy: 0.431\n",
      "    Weighted Test F1: 0.498\n",
      "    Params: {'base_estimator__criterion': 'entropy', 'base_estimator__max_depth': 2, 'base_estimator__splitter': 'random', 'learning_rate': 1, 'n_estimators': 1000}\n",
      "    \n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.99\n",
      "    Eval score: 0.547\n",
      "    Balanced Accuracy: 0.475\n",
      "    Weighted Test F1: 0.536\n",
      "    Params: {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-cpu.py --lang_code dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  ha\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "['neutral', 'positive', 'negative']\n",
      "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 3000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 6000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 9000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 33470    /    33470\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██░░░░░░ 12358    /    33470\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █████░░░ 22712    /    33470\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 33470    /    33470\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 3000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 5000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 6987     /     6987\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running tokenizer on train dataset: 100%|███████| 10/10 [00:28<00:00,  2.87s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 2/2 [00:03<00:00,  1.98s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 3/3 [00:07<00:00,  2.64s/ba]\n",
      "0.345679012345679 0.17759655680144978 0.3333333333333333\n",
      "0.34074074074074073 0.1731941886638019 0.3333333333333333\n",
      "0.3135802469135803 0.1497168848046041 0.3333333333333333\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.809\n",
      "    Eval score: 0.715\n",
      "    Balanced Accuracy: 0.713\n",
      "    Weighted Test F1: 0.718\n",
      "    Params: {'alpha': 1}\n",
      "    \n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.865\n",
      "    Eval score: 0.688\n",
      "    Balanced Accuracy: 0.688\n",
      "    Weighted Test F1: 0.692\n",
      "    Params: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000}\n",
      "    \n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.85\n",
      "    Eval score: 0.734\n",
      "    Balanced Accuracy: 0.732\n",
      "    Weighted Test F1: 0.736\n",
      "    Params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "    \n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.648\n",
      "    Eval score: 0.636\n",
      "    Balanced Accuracy: 0.626\n",
      "    Weighted Test F1: 0.618\n",
      "    Params: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 500}\n",
      "    \n",
      "<function AdaDTC at 0x7fb54f5ed9d0>\n",
      "\n",
      "    trigram counts\n",
      "    Train score: 0.916\n",
      "    Eval score: 0.679\n",
      "    Balanced Accuracy: 0.673\n",
      "    Weighted Test F1: 0.676\n",
      "    Params: {'base_estimator__criterion': 'gini', 'base_estimator__max_depth': 5, 'base_estimator__splitter': 'best', 'learning_rate': 0.1, 'n_estimators': 500}\n",
      "    \n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "\n",
      "    trigram counts\n",
      "    Train score: 0.899\n",
      "    Eval score: 0.736\n",
      "    Balanced Accuracy: 0.734\n",
      "    Weighted Test F1: 0.737\n",
      "    Params: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-cpu.py --lang_code ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  ig\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "['neutral', 'negative', 'positive']\n",
      "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 3000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 6000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 24871    /    24871\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ███░░░░░ 12152    /    24871\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██████░░ 21328    /    24871\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 24871    /    24871\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 3000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 5000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 5692     /     5692\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running tokenizer on train dataset: 100%|█████████| 8/8 [00:21<00:00,  2.71s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 2/2 [00:02<00:00,  1.49s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 3/3 [00:05<00:00,  1.97s/ba]\n",
      "0.44433545855811674 0.2733907812928039 0.3333333333333333\n",
      "0.24865129965669447 0.09903080041315325 0.3333333333333333\n",
      "0.3070132417851888 0.14423286255724443 0.3333333333333333\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.832\n",
      "    Eval score: 0.77\n",
      "    Balanced Accuracy: 0.741\n",
      "    Weighted Test F1: 0.767\n",
      "    Params: {'alpha': 0.1}\n",
      "    \n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.989\n",
      "    Eval score: 0.77\n",
      "    Balanced Accuracy: 0.745\n",
      "    Weighted Test F1: 0.767\n",
      "    Params: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000}\n",
      "    \n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.852\n",
      "    Eval score: 0.771\n",
      "    Balanced Accuracy: 0.744\n",
      "    Weighted Test F1: 0.768\n",
      "    Params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "    \n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\n",
      "    bigram counts\n",
      "    Train score: 0.512\n",
      "    Eval score: 0.513\n",
      "    Balanced Accuracy: 0.409\n",
      "    Weighted Test F1: 0.402\n",
      "    Params: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 500}\n",
      "    \n",
      "<function AdaDTC at 0x7fd20bfc84c0>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.945\n",
      "    Eval score: 0.712\n",
      "    Balanced Accuracy: 0.676\n",
      "    Weighted Test F1: 0.704\n",
      "    Params: {'base_estimator__criterion': 'gini', 'base_estimator__max_depth': 5, 'base_estimator__splitter': 'best', 'learning_rate': 0.1, 'n_estimators': 500}\n",
      "    \n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "\n",
      "    trigram tf-idf\n",
      "    Train score: 0.839\n",
      "    Eval score: 0.769\n",
      "    Balanced Accuracy: 0.741\n",
      "    Weighted Test F1: 0.766\n",
      "    Params: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-cpu.py --lang_code ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  ma\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "['negative', 'positive', 'neutral']\n",
      "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 3000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 26166    /    26166\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████░░░░ 13311    /    26166\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ███████░ 25839    /    26166\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 26166    /    26166\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 3000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 5245     /     5245\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running tokenizer on train dataset: 100%|█████████| 4/4 [00:11<00:00,  2.88s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 1/1 [00:01<00:00,  1.60s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 2/2 [00:03<00:00,  1.57s/ba]\n",
      "0.2846911369740376 0.12617669903518325 0.3333333333333333\n",
      "0.3187108325872874 0.15405438750994474 0.3333333333333333\n",
      "0.39659803043867503 0.2252473429286321 0.3333333333333333\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "\n",
      "    trigram tf-idf\n",
      "    Train score: 0.795\n",
      "    Eval score: 0.702\n",
      "    Balanced Accuracy: 0.681\n",
      "    Weighted Test F1: 0.694\n",
      "    Params: {'alpha': 0.1}\n",
      "    \n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "\n",
      "    unigram tf-idf\n",
      "    Train score: 0.967\n",
      "    Eval score: 0.706\n",
      "    Balanced Accuracy: 0.684\n",
      "    Weighted Test F1: 0.698\n",
      "    Params: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000}\n",
      "    \n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\n",
      "    bigram counts\n",
      "    Train score: 0.871\n",
      "    Eval score: 0.66\n",
      "    Balanced Accuracy: 0.644\n",
      "    Weighted Test F1: 0.655\n",
      "    Params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "    \n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\n",
      "    unigram tf-idf\n",
      "    Train score: 0.536\n",
      "    Eval score: 0.511\n",
      "    Balanced Accuracy: 0.46\n",
      "    Weighted Test F1: 0.439\n",
      "    Params: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 500}\n",
      "    \n",
      "<function AdaDTC at 0x7fd0ae6f80d0>\n",
      "\n",
      "    trigram counts\n",
      "    Train score: 0.994\n",
      "    Eval score: 0.61\n",
      "    Balanced Accuracy: 0.59\n",
      "    Weighted Test F1: 0.603\n",
      "    Params: {'base_estimator__criterion': 'gini', 'base_estimator__max_depth': 5, 'base_estimator__splitter': 'best', 'learning_rate': 0.1, 'n_estimators': 500}\n",
      "    \n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.886\n",
      "    Eval score: 0.689\n",
      "    Balanced Accuracy: 0.673\n",
      "    Weighted Test F1: 0.685\n",
      "    Params: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-cpu.py --lang_code ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  pcm\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "['negative', 'positive', 'neutral']\n",
      "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 2000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 4000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 9377     /     9377\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 9377     /     9377\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 3621     /     3621\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running tokenizer on train dataset: 100%|█████████| 4/4 [00:10<00:00,  2.63s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 1/1 [00:01<00:00,  1.45s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 2/2 [00:02<00:00,  1.44s/ba]\n",
      "0.6253658536585366 0.48122390419582467 0.3333333333333333\n",
      "0.3551219512195122 0.18612583188466872 0.3333333333333333\n",
      "0.01951219512195122 0.000746878282179951 0.3333333333333333\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "\n",
      "    trigram counts\n",
      "    Train score: 0.829\n",
      "    Eval score: 0.692\n",
      "    Balanced Accuracy: 0.435\n",
      "    Weighted Test F1: 0.673\n",
      "    Params: {'alpha': 1}\n",
      "    \n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "\n",
      "    trigram counts\n",
      "    Train score: 0.959\n",
      "    Eval score: 0.692\n",
      "    Balanced Accuracy: 0.453\n",
      "    Weighted Test F1: 0.684\n",
      "    Params: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000}\n",
      "    \n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\n",
      "    bigram counts\n",
      "    Train score: 0.871\n",
      "    Eval score: 0.681\n",
      "    Balanced Accuracy: 0.421\n",
      "    Weighted Test F1: 0.656\n",
      "    Params: {'C': 0.1, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "    \n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.638\n",
      "    Eval score: 0.627\n",
      "    Balanced Accuracy: 0.335\n",
      "    Weighted Test F1: 0.486\n",
      "    Params: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 500}\n",
      "    \n",
      "<function AdaDTC at 0x7fca446adb80>\n",
      "\n",
      "    bigram counts\n",
      "    Train score: 0.765\n",
      "    Eval score: 0.643\n",
      "    Balanced Accuracy: 0.39\n",
      "    Weighted Test F1: 0.582\n",
      "    Params: {'base_estimator__criterion': 'gini', 'base_estimator__max_depth': 5, 'base_estimator__splitter': 'best', 'learning_rate': 0.1, 'n_estimators': 500}\n",
      "    \n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.987\n",
      "    Eval score: 0.704\n",
      "    Balanced Accuracy: 0.448\n",
      "    Weighted Test F1: 0.689\n",
      "    Params: {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-cpu.py --lang_code pcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  pt\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "['neutral', 'negative', 'positive']\n",
      "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 2000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 12675    /    12675\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██░░░░░░ 3276     /    12675\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████░░░░ 7812     /    12675\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 12675    /    12675\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 2000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 3000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 3570     /     3570\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running tokenizer on train dataset: 100%|█████████| 3/3 [00:11<00:00,  3.71s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 1/1 [00:01<00:00,  1.39s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 1/1 [00:02<00:00,  2.71s/ba]\n",
      "0.5220228384991843 0.3580864058300943 0.3333333333333333\n",
      "0.23491027732463296 0.08937141330184185 0.3333333333333333\n",
      "0.24306688417618272 0.09505765286680111 0.3333333333333333\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "\n",
      "    trigram counts\n",
      "    Train score: 0.755\n",
      "    Eval score: 0.506\n",
      "    Balanced Accuracy: 0.475\n",
      "    Weighted Test F1: 0.508\n",
      "    Params: {'alpha': 1}\n",
      "    \n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.783\n",
      "    Eval score: 0.55\n",
      "    Balanced Accuracy: 0.448\n",
      "    Weighted Test F1: 0.516\n",
      "    Params: {'C': 1, 'gamma': 'scale', 'kernel': 'linear', 'max_iter': 1000}\n",
      "    \n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.769\n",
      "    Eval score: 0.542\n",
      "    Balanced Accuracy: 0.427\n",
      "    Weighted Test F1: 0.497\n",
      "    Params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "    \n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.524\n",
      "    Eval score: 0.522\n",
      "    Balanced Accuracy: 0.333\n",
      "    Weighted Test F1: 0.358\n",
      "    Params: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 500}\n",
      "    \n",
      "<function AdaDTC at 0x7fb7f188e700>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.995\n",
      "    Eval score: 0.523\n",
      "    Balanced Accuracy: 0.429\n",
      "    Weighted Test F1: 0.493\n",
      "    Params: {'base_estimator__criterion': 'gini', 'base_estimator__max_depth': 5, 'base_estimator__splitter': 'best', 'learning_rate': 0.1, 'n_estimators': 500}\n",
      "    \n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "\n",
      "    unigram tf-idf\n",
      "    Train score: 0.823\n",
      "    Eval score: 0.537\n",
      "    Balanced Accuracy: 0.454\n",
      "    Weighted Test F1: 0.515\n",
      "    Params: {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-cpu.py --lang_code pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  sw\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "['positive', 'neutral', 'negative']\n",
      "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 9111     /     9111\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █████░░░ 6008     /     9111\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 9111     /     9111\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 2000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 2358     /     2358\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running tokenizer on train dataset: 100%|█████████| 2/2 [00:06<00:00,  3.23s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 1/1 [00:00<00:00,  1.13ba/s]\n",
      "Running tokenizer on validation dataset: 100%|████| 1/1 [00:01<00:00,  1.73s/ba]\n",
      "0.3453038674033149 0.17726071221936088 0.3333333333333333\n",
      "0.56353591160221 0.4062237666673174 0.3333333333333333\n",
      "0.09116022099447514 0.015231834394013568 0.3333333333333333\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "\n",
      "    unigram tf-idf\n",
      "    Train score: 0.767\n",
      "    Eval score: 0.575\n",
      "    Balanced Accuracy: 0.362\n",
      "    Weighted Test F1: 0.489\n",
      "    Params: {'alpha': 0.001}\n",
      "    \n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "\n",
      "    unigram tf-idf\n",
      "    Train score: 0.758\n",
      "    Eval score: 0.564\n",
      "    Balanced Accuracy: 0.36\n",
      "    Weighted Test F1: 0.493\n",
      "    Params: {'C': 1, 'gamma': 'scale', 'kernel': 'linear', 'max_iter': 1000}\n",
      "    \n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\n",
      "    trigram tf-idf\n",
      "    Train score: 0.754\n",
      "    Eval score: 0.552\n",
      "    Balanced Accuracy: 0.352\n",
      "    Weighted Test F1: 0.48\n",
      "    Params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "    \n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.595\n",
      "    Eval score: 0.564\n",
      "    Balanced Accuracy: 0.333\n",
      "    Weighted Test F1: 0.406\n",
      "    Params: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 500}\n",
      "    \n",
      "<function AdaDTC at 0x7f97a763e790>\n",
      "\n",
      "    trigram tf-idf\n",
      "    Train score: 0.896\n",
      "    Eval score: 0.524\n",
      "    Balanced Accuracy: 0.346\n",
      "    Weighted Test F1: 0.474\n",
      "    Params: {'base_estimator__criterion': 'entropy', 'base_estimator__max_depth': 2, 'base_estimator__splitter': 'random', 'learning_rate': 1, 'n_estimators': 1000}\n",
      "    \n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "\n",
      "    unigram tf-idf\n",
      "    Train score: 0.847\n",
      "    Eval score: 0.54\n",
      "    Balanced Accuracy: 0.362\n",
      "    Weighted Test F1: 0.499\n",
      "    Params: {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-cpu.py --lang_code sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Code:  yo\n",
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "['positive', 'neutral', 'negative']\n",
      "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 2000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 4000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 34044    /    34044\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ███░░░░░ 14280    /    34044\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██████░░ 25840    /    34044\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████████ 34044    /    34044\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 1000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 3000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ░░░░░░░░ 6000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Compute merges                           ████████ 8632     /     8632\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running tokenizer on train dataset: 100%|█████████| 6/6 [00:15<00:00,  2.51s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 1/1 [00:02<00:00,  2.04s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 2/2 [00:04<00:00,  2.10s/ba]\n",
      "0.4146627565982405 0.2430900239759171 0.3333333333333333\n",
      "0.37771260997067446 0.20710678656544432 0.3333333333333333\n",
      "0.20762463343108503 0.0713930259685324 0.3333333333333333\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "\n",
      "    bigram counts\n",
      "    Train score: 0.789\n",
      "    Eval score: 0.66\n",
      "    Balanced Accuracy: 0.639\n",
      "    Weighted Test F1: 0.66\n",
      "    Params: {'alpha': 0.1}\n",
      "    \n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.995\n",
      "    Eval score: 0.672\n",
      "    Balanced Accuracy: 0.64\n",
      "    Weighted Test F1: 0.668\n",
      "    Params: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf', 'max_iter': 1000}\n",
      "    \n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\n",
      "    trigram tf-idf\n",
      "    Train score: 0.977\n",
      "    Eval score: 0.673\n",
      "    Balanced Accuracy: 0.646\n",
      "    Weighted Test F1: 0.672\n",
      "    Params: {'C': 10, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "    \n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\n",
      "    unigram tf-idf\n",
      "    Train score: 0.56\n",
      "    Eval score: 0.538\n",
      "    Balanced Accuracy: 0.444\n",
      "    Weighted Test F1: 0.459\n",
      "    Params: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 500}\n",
      "    \n",
      "<function AdaDTC at 0x7feaf75ed4c0>\n",
      "\n",
      "    unigram counts\n",
      "    Train score: 0.985\n",
      "    Eval score: 0.596\n",
      "    Balanced Accuracy: 0.562\n",
      "    Weighted Test F1: 0.593\n",
      "    Params: {'base_estimator__criterion': 'gini', 'base_estimator__max_depth': 5, 'base_estimator__splitter': 'best', 'learning_rate': 0.1, 'n_estimators': 500}\n",
      "    \n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "\n",
      "    trigram counts\n",
      "    Train score: 0.99\n",
      "    Eval score: 0.668\n",
      "    Balanced Accuracy: 0.64\n",
      "    Weighted Test F1: 0.666\n",
      "    Params: {'activation': 'relu', 'alpha': 1, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python Baseline_A-cpu.py --lang_code yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "['positive', 'neutral', 'negative']\n",
      "[00:00:00] Pre-processing sequences                 ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 3000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 6000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 9000     /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 12000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 15000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 18000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 21000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 24000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 27000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 29000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Pre-processing sequences                 ████████ 32000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Pre-processing sequences                 ████████ 35000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Pre-processing sequences                 ████████ 38000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Pre-processing sequences                 ████████ 41000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Pre-processing sequences                 ████████ 44000    /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Pre-processing sequences                 ████████ 0        /        0\n",
      "[00:00:00] Tokenize words                           ████████ 0        /        0\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           █░░░░░░░ 39537    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ███░░░░░ 77355    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           █████░░░ 118611   /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ███████░ 158148   /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Tokenize words                           ████████ 171967   /   171967\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 12033    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ░░░░░░░░ 20628    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 30942    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █░░░░░░░ 41256    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██░░░░░░ 49851    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ██░░░░░░ 60168    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ███░░░░░ 70481    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ███░░░░░ 79074    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████░░░░ 87669    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████░░░░ 96264    /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              ████░░░░ 104859   /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █████░░░ 115175   /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:00] Count pairs                              █████░░░ 123768   /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ██████░░ 135802   /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ██████░░ 146115   /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███████░ 156429   /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ███████░ 168462   /   171967\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Count pairs                              ████████ 171967   /   171967\n",
      "\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ░░░░░░░░ 1000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ░░░░░░░░ 2000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ░░░░░░░░ 3000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ░░░░░░░░ 4000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ░░░░░░░░ 5000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:01] Compute merges                           ░░░░░░░░ 6000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           ░░░░░░░░ 7000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           ░░░░░░░░ 8000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           ░░░░░░░░ 9000     /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           ░░░░░░░░ 10000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           ░░░░░░░░ 12000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           █░░░░░░░ 14000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           █░░░░░░░ 16000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           █░░░░░░░ 18000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           █░░░░░░░ 20000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           █░░░░░░░ 22000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:02] Compute merges                           █░░░░░░░ 24000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Compute merges                           ██░░░░░░ 26000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Compute merges                           ██░░░░░░ 28000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Compute merges                           ██░░░░░░ 30000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Compute merges                           ██░░░░░░ 32000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Compute merges                           ██░░░░░░ 34000    /   100000\n",
      "\u001b[2K\u001b[1B\u001b[1A[00:00:03] Compute merges                           ████████ 36614    /    36614\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Running tokenizer on train dataset: 100%|███████| 40/40 [01:47<00:00,  2.69s/ba]\n",
      "Running tokenizer on validation dataset: 100%|████| 6/6 [00:14<00:00,  2.48s/ba]\n",
      "Running tokenizer on validation dataset: 100%|██| 12/12 [00:29<00:00,  2.48s/ba]\n",
      "0.31327985739750447 0.14946436359039406 0.3333333333333333\n",
      "0.3767379679144385 0.20618519976362162 0.3333333333333333\n",
      "0.30998217468805705 0.14670268112192983 0.3333333333333333\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.738\n",
      "    Eval score: 0.646\n",
      "    Balanced Accuracy: 0.638\n",
      "    Weighted Test F1: 0.644\n",
      "    Params: {'alpha': 0.001}\n",
      "    \n",
      "<class 'sklearn.svm._classes.SVC'>\n",
      "\n",
      "    trigram tf-idf\n",
      "    Train score: 0.545\n",
      "    Eval score: 0.491\n",
      "    Balanced Accuracy: 0.487\n",
      "    Weighted Test F1: 0.488\n",
      "    Params: {'C': 1, 'gamma': 'scale', 'kernel': 'linear', 'max_iter': 1000}\n",
      "    \n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.818\n",
      "    Eval score: 0.673\n",
      "    Balanced Accuracy: 0.666\n",
      "    Weighted Test F1: 0.672\n",
      "    Params: {'C': 1, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "    \n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "\n",
      "    bigram tf-idf\n",
      "    Train score: 0.464\n",
      "    Eval score: 0.467\n",
      "    Balanced Accuracy: 0.431\n",
      "    Weighted Test F1: 0.38\n",
      "    Params: {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 500}\n",
      "    \n",
      "<function AdaDTC at 0x7fbfbf64e790>\n",
      "\n",
      "    bigram counts\n",
      "    Train score: 0.739\n",
      "    Eval score: 0.627\n",
      "    Balanced Accuracy: 0.62\n",
      "    Weighted Test F1: 0.625\n",
      "    Params: {'base_estimator__criterion': 'entropy', 'base_estimator__max_depth': 2, 'base_estimator__splitter': 'random', 'learning_rate': 1, 'n_estimators': 1000}\n",
      "    \n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "\n",
      "    bigram counts\n",
      "    Train score: 0.822\n",
      "    Eval score: 0.665\n",
      "    Balanced Accuracy: 0.658\n",
      "    Weighted Test F1: 0.663\n",
      "    Params: {'activation': 'tanh', 'alpha': 1, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!python Baseline_B-cpu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
